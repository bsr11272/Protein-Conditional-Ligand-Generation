{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c894b4ff-94ac-4f89-95c6-7d2c9abf8e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/work/SensingandInnovationLab/boda.s/.dep_genai/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a6ca54b-524b-4780-ab6e-e1b56ef1656a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e59c1cbc-6241-4abb-a138-1551f2959aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import multiprocessing\n",
    "import re\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "import networkx as nx\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "import esm\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, Descriptors, rdMolDescriptors, Lipinski, AllChem, Draw\n",
    "from rdkit.Chem.Scaffolds import MurckoScaffold\n",
    "from rdkit.Chem import BRICS, FragmentMatcher\n",
    "from rdkit.Chem.QED import qed\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric.nn import GCNConv, GATv2Conv, GraphConv, MessagePassing\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "import torch_scatter\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8c37d6e-0bdc-4c85-9de5-5b6365625cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef44999-260b-4313-bf73-0040292d380f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4665f53c-0bbe-4af8-947e-d73bfff71dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/SensingandInnovationLab/boda.s/.dep_genai/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset from Hugging Face...\n",
      "Downloaded 20000 samples\n",
      "Saved dataset to data/binding_data_200k.parquet\n"
     ]
    }
   ],
   "source": [
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "print(\"Downloading dataset from Hugging Face...\")\n",
    "dataset = load_dataset(\"jglaser/binding_affinity\", split=\"train[:200000]\")\n",
    "\n",
    "df = pd.DataFrame(dataset)\n",
    "print(f\"Downloaded {len(df)} samples\")\n",
    "\n",
    "df.to_parquet('data/binding_data_200k.parquet')\n",
    "print(\"Saved dataset to data/binding_data_200k.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6f2d0b-5fde-4dfc-b64e-4bdc18e24fd3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Protein Preprocessing using ESM-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "606ebf90-d16e-41f3-8e23-5e4d7120fcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_protein_embeddings(sequences, output_file, batch_size=4, max_seq_len=1022, chunk_overlap=128):\n",
    "    \"\"\"\n",
    "    Generate and store FULL sequence embeddings (not just CLS token) for each protein\n",
    "    \n",
    "    For proteins longer than max_seq_len, use chunking with overlap and store the full sequence.\n",
    "    \"\"\"\n",
    "    print(\"Loading ESM-2 model from Hugging Face...\")\n",
    "    from transformers import AutoTokenizer, AutoModel\n",
    "    \n",
    "    # Load model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t33_650M_UR50D\")\n",
    "    model = AutoModel.from_pretrained(\"facebook/esm2_t33_650M_UR50D\")\n",
    "    \n",
    "    # Use GPU if available for speed\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device).eval()\n",
    "    \n",
    "    all_embeddings = {}  # Dictionary to store embeddings with sequence_id as key\n",
    "    sequence_ids = []\n",
    "    \n",
    "    # Calculate effective sequence length\n",
    "    effective_len = max_seq_len - 2  # Subtract 2 for special tokens\n",
    "    chunk_stride = effective_len - chunk_overlap\n",
    "    \n",
    "    print(f\"Generating full sequence embeddings for {len(sequences)} proteins...\")\n",
    "    # Process sequences in batches\n",
    "    for i in tqdm(range(0, len(sequences), batch_size)):\n",
    "        batch_seqs = sequences[i:i+batch_size]\n",
    "        batch_ids = list(range(i, min(i+batch_size, len(sequences))))\n",
    "        \n",
    "        for j, (seq_id, sequence) in enumerate(zip(batch_ids, batch_seqs)):\n",
    "            try:\n",
    "                # Check if sequence needs chunking\n",
    "                if len(sequence) > effective_len:\n",
    "                    print(f\"Sequence {seq_id} is long ({len(sequence)} > {effective_len}). Using chunking.\")\n",
    "                    \n",
    "                    # Calculate number of chunks\n",
    "                    num_chunks = max(1, int(np.ceil((len(sequence) - chunk_overlap) / chunk_stride)))\n",
    "                    \n",
    "                    # Store embeddings for each position in the sequence\n",
    "                    full_sequence_embeddings = np.zeros((len(sequence), model.config.hidden_size))\n",
    "                    \n",
    "                    # Track which positions have been filled\n",
    "                    position_counts = np.zeros(len(sequence))\n",
    "                    \n",
    "                    # Process each chunk\n",
    "                    for chunk_idx in range(num_chunks):\n",
    "                        # Calculate chunk start and end positions\n",
    "                        start_pos = chunk_idx * chunk_stride\n",
    "                        end_pos = min(start_pos + effective_len, len(sequence))\n",
    "                        \n",
    "                        # Get chunk sequence\n",
    "                        chunk_seq = sequence[start_pos:end_pos]\n",
    "                        \n",
    "                        # Skip tiny chunks\n",
    "                        if len(chunk_seq) < 8:\n",
    "                            continue\n",
    "                        \n",
    "                        # Process the chunk using transformers\n",
    "                        inputs = tokenizer(chunk_seq, return_tensors=\"pt\").to(device)\n",
    "                        \n",
    "                        with torch.no_grad():\n",
    "                            outputs = model(**inputs, output_hidden_states=True)\n",
    "                        \n",
    "                        # Get the last layer's hidden states, excluding special tokens\n",
    "                        # [0] for batch index, [1:-1] to exclude [CLS] and [SEP] tokens\n",
    "                        last_hidden_states = outputs.hidden_states[-1][0, 1:len(chunk_seq)+1, :].cpu().numpy()\n",
    "                        \n",
    "                        # Add to the full sequence embeddings\n",
    "                        full_sequence_embeddings[start_pos:end_pos, :] += last_hidden_states\n",
    "                        position_counts[start_pos:end_pos] += 1\n",
    "                    \n",
    "                    # Average embeddings where chunks overlapped\n",
    "                    valid_positions = position_counts > 0\n",
    "                    full_sequence_embeddings[valid_positions] /= position_counts[valid_positions, np.newaxis]\n",
    "                    \n",
    "                    # Store embeddings\n",
    "                    all_embeddings[seq_id] = full_sequence_embeddings\n",
    "                    sequence_ids.append(seq_id)\n",
    "                \n",
    "                else:\n",
    "                    # For shorter sequences, process normally\n",
    "                    inputs = tokenizer(sequence, return_tensors=\"pt\").to(device)\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(**inputs, output_hidden_states=True)\n",
    "                    \n",
    "                    # Extract all token embeddings (excluding special tokens)\n",
    "                    last_hidden_states = outputs.hidden_states[-1][0, 1:len(sequence)+1, :].cpu().numpy()\n",
    "                    \n",
    "                    # Store embeddings\n",
    "                    all_embeddings[seq_id] = last_hidden_states\n",
    "                    sequence_ids.append(seq_id)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing sequence {seq_id}: {e}\")\n",
    "    \n",
    "    # Save to disk - using dictionary format to handle variable-length sequences\n",
    "    print(f\"Saving full sequence embeddings for {len(all_embeddings)} proteins to {output_file}\")\n",
    "    np.savez_compressed(output_file, embeddings=all_embeddings, ids=np.array(sequence_ids))\n",
    "    \n",
    "    return all_embeddings, sequence_ids\n",
    "\n",
    "\n",
    "def process_in_chunks(parquet_file, chunk_size=5000, seq_chunk_overlap=128):\n",
    "    \"\"\"\n",
    "    Process the dataset in chunks to avoid memory issues\n",
    "    Stores full sequence embeddings for each protein\n",
    "    \"\"\"\n",
    "    # Load the data\n",
    "    df = pd.read_parquet(parquet_file)\n",
    "    \n",
    "    # Get unique protein sequences to avoid duplicate processing\n",
    "    unique_proteins = df['seq'].unique()\n",
    "    print(f\"Total unique proteins: {len(unique_proteins)}\")\n",
    "    \n",
    "    # Create processed_data directory if it doesn't exist\n",
    "    os.makedirs('processed_data', exist_ok=True)\n",
    "    \n",
    "    # Process in chunks\n",
    "    for i in range(0, len(unique_proteins), chunk_size):\n",
    "        chunk_proteins = unique_proteins[i:i+chunk_size]\n",
    "        output_file = f'processed_data/protein_full_embeddings_chunk_{i//chunk_size}.npz'\n",
    "        \n",
    "        # Skip if already processed\n",
    "        if os.path.exists(output_file):\n",
    "            print(f\"Chunk {i//chunk_size} already processed, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Generate embeddings for this chunk\n",
    "        generate_protein_embeddings(chunk_proteins, output_file, chunk_overlap=seq_chunk_overlap)\n",
    "        \n",
    "        # Clear memory\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "    \n",
    "    # Create a mapping from protein sequence to its id\n",
    "    protein_to_id_map = {}\n",
    "    all_embeddings = {}\n",
    "    processed_ids = []\n",
    "    \n",
    "    # Load all chunks\n",
    "    for i in range(0, len(unique_proteins), chunk_size):\n",
    "        chunk_file = f'processed_data/protein_full_embeddings_chunk_{i//chunk_size}.npz'\n",
    "        if os.path.exists(chunk_file):\n",
    "            chunk_data = np.load(chunk_file, allow_pickle=True)\n",
    "            chunk_embeddings = chunk_data['embeddings'].item()  # Load the dictionary\n",
    "            chunk_ids = chunk_data['ids']\n",
    "            \n",
    "            # Add embeddings to the combined dictionary\n",
    "            for j, seq_id in enumerate(chunk_ids):\n",
    "                all_embeddings[seq_id] = chunk_embeddings[seq_id]\n",
    "                protein_to_id_map[unique_proteins[i + j]] = seq_id\n",
    "                processed_ids.append(seq_id)\n",
    "    \n",
    "    # Save the combined data\n",
    "    print(f\"Saving combined embeddings for {len(all_embeddings)} proteins\")\n",
    "    np.savez_compressed('processed_data/protein_full_embeddings_all.npz', \n",
    "                         embeddings=all_embeddings, \n",
    "                         ids=np.array(processed_ids))\n",
    "    \n",
    "    # Save the mapping\n",
    "    import pickle\n",
    "    with open('processed_data/protein_to_fullembedding_map.pkl', 'wb') as f:\n",
    "        pickle.dump(protein_to_id_map, f)\n",
    "    \n",
    "    return all_embeddings, protein_to_id_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdae13a1-f211-4231-bc8b-4cd8f7fe1bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique proteins: 11795\n",
      "Loading ESM-2 model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating full sequence embeddings for 5000 proteins...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 260/1250 [00:57<03:27,  4.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1041 is long (1054 > 1020). Using chunking.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▎       | 295/1250 [01:04<03:42,  4.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1180 is long (1073 > 1020). Using chunking.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 346/1250 [01:16<04:01,  3.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1387 is long (1042 > 1020). Using chunking.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 372/1250 [01:22<03:44,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1490 is long (1342 > 1020). Using chunking.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 382/1250 [01:24<03:19,  4.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1531 is long (1706 > 1020). Using chunking.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 515/1250 [01:55<02:26,  5.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 2062 is long (1032 > 1020). Using chunking.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 587/1250 [02:11<02:21,  4.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 2348 is long (1025 > 1020). Using chunking.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 647/1250 [02:24<02:11,  4.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 2590 is long (1718 > 1020). Using chunking.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 727/1250 [02:43<02:02,  4.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 2909 is long (1340 > 1020). Using chunking.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▊    | 732/1250 [02:45<01:58,  4.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 2928 is long (1053 > 1020). Using chunking.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 748/1250 [02:48<01:45,  4.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 2992 is long (1205 > 1020). Using chunking.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 758/1250 [02:51<01:49,  4.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 3032 is long (1175 > 1020). Using chunking.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 873/1250 [03:17<01:13,  5.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 3494 is long (2149 > 1020). Using chunking.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 896/1250 [03:23<01:20,  4.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 3585 is long (1072 > 1020). Using chunking.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 1105/1250 [04:10<00:36,  3.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 4421 is long (1287 > 1020). Using chunking.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 1111/1250 [04:11<00:30,  4.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 4445 is long (1182 > 1020). Using chunking.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 1191/1250 [04:29<00:12,  4.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 4764 is long (1707 > 1020). Using chunking.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [04:43<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving full sequence embeddings for 5000 proteins to processed_data/protein_full_embeddings_chunk_0.npz\n",
      "Loading ESM-2 model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating full sequence embeddings for 5000 proteins...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 126/1250 [00:27<04:09,  4.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 504 is long (1283 > 1020). Using chunking.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 255/1250 [00:56<03:57,  4.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1021 is long (1244 > 1020). Using chunking.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 381/1250 [01:24<02:52,  5.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1524 is long (1091 > 1020). Using chunking.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 692/1250 [02:32<01:52,  4.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 2771 is long (1332 > 1020). Using chunking.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 719/1250 [02:39<01:58,  4.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 2877 is long (1118 > 1020). Using chunking.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 937/1250 [03:26<00:57,  5.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 3749 is long (1179 > 1020). Using chunking.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 1053/1250 [03:52<00:43,  4.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 4214 is long (1089 > 1020). Using chunking.\n",
      "Sequence 4215 is long (1340 > 1020). Using chunking.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 1071/1250 [03:56<00:45,  3.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 4285 is long (1023 > 1020). Using chunking.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 1139/1250 [04:11<00:23,  4.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 4556 is long (1029 > 1020). Using chunking.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [04:34<00:00,  4.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving full sequence embeddings for 5000 proteins to processed_data/protein_full_embeddings_chunk_1.npz\n",
      "Loading ESM-2 model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating full sequence embeddings for 1795 proteins...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 140/449 [00:28<01:02,  4.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 562 is long (1073 > 1020). Using chunking.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 449/449 [01:33<00:00,  4.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving full sequence embeddings for 1795 proteins to processed_data/protein_full_embeddings_chunk_2.npz\n",
      "Saving combined embeddings for 5000 proteins\n"
     ]
    }
   ],
   "source": [
    "os.makedirs('processed_data', exist_ok=True)\n",
    "parquet_file = 'data/binding_data_200k.parquet'\n",
    "embeddings, protein_map = process_in_chunks(parquet_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c75ded1-498e-4782-8fcd-5a63d6e45d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 11795 protein mappings\n",
      "Loaded 5000 full sequence embeddings\n",
      "ID 5 corresponds to protein: GSFVEMVDNLRGKSGQGYYVEMTVGSPPQTLNILVDTGSSNFAVGAAPHP...\n",
      "Embedding shape: (496, 1280) (sequence length × embedding dimension)\n",
      "\n",
      "Found full embedding for sample protein with shape: (284, 1280)\n",
      "This means we have embeddings for all 284 amino acids in the protein\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAysAAAIjCAYAAAADc4SFAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAApp1JREFUeJzs3Xd8U1X/B/BPkrZJV7oXtFB2QaZFoMiSVQRBFGWIMl0ooNQFPooDFQdLEUFRwAfFhYIKyhAEHrGsAsreo4zuPdMm9/cHv0ZDR/INKQ30835eeT1y+8095+ae3OTk3HO+KkVRFBARERERETkZdU1XgIiIiIiIqCLsrBARERERkVNiZ4WIiIiIiJwSOytEREREROSU2FkhIiIiIiKnxM4KERERERE5JXZWiIiIiIjIKbGzQkRERERETomdFSIiIiIickrsrNyAli1bBpVKhbNnzzpdPXr06IEePXpc97rUVLkSycnJuO+++xAQEACVSoV58+ZVGpuXl4eHH34YoaGhUKlUePrpp3H27FmoVCosW7bsutX5Zvfqq69CpVLVdDWuma3tf8uWLVCpVNiyZUu116m2i4yMxF133VXt5UiuC2PGjEFkZKTFNpVKhVdffbVa6maPitpoRfW+EfH9R2QfdlacwKBBg+Dh4YHc3NxKY0aOHAk3Nzekp6dfx5o5l8OHD+PVV1+t8U6avaZMmYL169dj2rRpWL58Ofr161dp7FtvvYVly5ZhwoQJWL58OR566KFqqdNbb72F1atX2xyvUqkqfLz99tvlYi9evIihQ4fC19cXer0ed999N06fPu3A2tPNouyHj4oeSUlJ5eJ/+ukn3HrrrdDpdKhXrx5eeeUVlJaW1kDNiYiournUdAXoSkfk559/xqpVqzBq1Khyfy8oKMCPP/6Ifv36ISAgAA899BCGDx8OrVZbA7Wt2oYNG6pt34cPH8Zrr72GHj16lPuVrTrLdZTNmzfj7rvvxrPPPmtTbKdOnfDKK6+YtymKgsLCQri6ujqsTm+99Rbuu+8+DB482Obn9OnTp1w7bdeuncW/8/LycMcddyA7OxsvvvgiXF1dMXfuXHTv3h379+9HQECAI6p/zV566SVMnTq1pqtxzW6E9m+L119/HQ0aNLDY5uvra/HvX3/9FYMHD0aPHj0wf/58HDhwAG+88QZSUlKwcOHC61jbm0NhYSFcXJz7q8DixYthMplquhrXrFu3bigsLISbm1tNV4XohuLcV6haYtCgQfD29saKFSsq7Kz8+OOPyM/Px8iRIwEAGo0GGo3melfTJjV1Eb4RLv4pKSnlvnhVFduiRQuLbSqVCjqdzupz8/Pz4enpaU8VbdK0aVM8+OCDVcZ89NFHOHHiBHbt2oXbbrsNAHDnnXeiZcuWmD17Nt56661qq5+Ei4uL039Rs0VNtf89e/bAxcUFbdu2Lfe30tJSfPHFFxgzZozN+7vzzjvRvn37KmOeffZZtG7dGhs2bDCfO71ej7feegtPPfUUoqKiJIdQ69lyTalpjvyBpiap1eob4vUmcja8DcwJuLu7495778WmTZuQkpJS7u8rVqyAt7c3Bg0aBKDiuSJ79uxBbGwsAgMD4e7ujgYNGmDcuHHmv1d2r2xF9zv//fffGDNmDBo2bAidTofQ0FCMGzfOplvQrr53PjIystLbO8rqcu7cOTzxxBNo1qwZ3N3dERAQgPvvv9/i+JYtW4b7778fAHDHHXeU20dF9+ynpKRg/PjxCAkJgU6nQ5s2bfD5559XePyzZs3CJ598gkaNGkGr1eK2227D7t27rR4vAJw+fRr3338//P394eHhgU6dOmHt2rUWdVepVFAUBQsWLDDXvSJl5+nMmTNYu3atOfbs2bMVnqsxY8bAy8sLp06dQv/+/eHt7W3u1J44cQJDhgxBaGgodDodwsPDMXz4cGRnZwO40vnJz8/H559/bi7H1i+WhYWFKCoqqvTvK1euxG233WbuqABAVFQUevXqhW+//dbq/pcuXYqePXsiODgYWq0WLVq0qPBX87J5AVu2bEH79u3h7u6OVq1amdvFDz/8gFatWkGn0yE6Ohr79u2zeH5Fc1ZUKhUmTpyI1atXo2XLltBqtbjllluwbt26cuXv27cPd955J/R6Pby8vNCrVy/s2LHD6vEBwKxZs9C5c2cEBATA3d0d0dHRWLlyZYWxX3zxBTp06AAPDw/4+fmhW7duFqMpFbX/CxcuYPDgwfD09ERwcDCmTJmC4uLicvu21k6qMm3aNMTGxuLEiRMW2xVFwfjx4zF+/HgcOHDAhlfjH7m5uTAajRX+7fDhwzh8+DAeffRRi07mE088AUVRKn39/i0rKwtPP/00IiIioNVq0bhxY7zzzjsWv9z/+7qwYMECNGzYEB4eHujbty8SExOhKApmzJiB8PBwuLu74+6770ZGRkaF5W3YsAFt27aFTqdDixYt8MMPP9hVp7K4MWPGwMfHB76+vhg9ejSysrIqLLes/ep0OrRs2RKrVq2qMO7qOStl74mTJ09izJgx8PX1hY+PD8aOHYuCggKL5xYWFmLy5MkIDAw0f0ZdvHjR5nkwtrbRq+esOOr8/Prrr+jatSs8PT3h7e2NAQMG4NChQ+XK9vLywsWLFzF48GB4eXkhKCgIzz77bLl2+vXXXyM6Ohre3t7Q6/Vo1aoV3n//ffPfK/sc/u677xAdHQ13d3cEBgbiwQcfxMWLF6utHkQ3mhv/J8WbxMiRI/H555/j22+/xcSJE83bMzIysH79eowYMQLu7u4VPjclJQV9+/ZFUFAQpk6dCl9fX5w9e7bCD0VbbNy4EadPn8bYsWMRGhqKQ4cO4ZNPPsGhQ4ewY8cO0YTkefPmIS8vz2Lb3LlzLW4F2r17N/78808MHz4c4eHhOHv2LBYuXIgePXrg8OHD8PDwQLdu3TB58mR88MEHePHFF9G8eXMAMP//1QoLC9GjRw+cPHkSEydORIMGDfDdd99hzJgxyMrKwlNPPWURv2LFCuTm5uKxxx6DSqXCu+++i3vvvRenT5+u8le95ORkdO7cGQUFBZg8eTICAgLw+eefY9CgQVi5ciXuuecedOvWzTzvpKJbqP6tefPmWL58OaZMmYLw8HA888wzAICgoCCkpqZW+JzS0lLExsaiS5cumDVrFjw8PGAwGBAbG4vi4mJMmjQJoaGhuHjxItasWYOsrCz4+Phg+fLlePjhh9GhQwc8+uijAIBGjRpVWrcyy5Ytw0cffQRFUdC8eXO89NJLeOCBB8x/N5lM+Pvvvy06y2U6dOiADRs2IDc3F97e3pWWsXDhQtxyyy0YNGgQXFxc8PPPP+OJJ56AyWTCk08+aRF78uRJPPDAA3jsscfw4IMPYtasWRg4cCAWLVqEF198EU888QQAYObMmRg6dCiOHTsGtbrq32n++OMP/PDDD3jiiSfg7e2NDz74AEOGDMH58+fN7fbQoUPo2rUr9Ho9nn/+ebi6uuLjjz9Gjx49sHXrVnTs2LHKMt5//30MGjQII0eOhMFgwNdff437778fa9aswYABA8xxr732Gl599VV07twZr7/+Otzc3LBz505s3rwZffv2rXDfhYWF6NWrF86fP4/JkyejTp06WL58OTZv3mwRZ0s7qcry5cvRpUsX9OnTB3/88QfCw8MBAHFxcfjvf/+LTz75BK1atapyH/92xx13IC8vD25uboiNjcXs2bPRpEkT89/LOptXj77UqVMH4eHh5TqjVysoKED37t1x8eJFPPbYY6hXrx7+/PNPTJs2DZcvXy636MWXX34Jg8GASZMmISMjA++++y6GDh2Knj17YsuWLXjhhRdw8uRJzJ8/H88++yyWLFli8fwTJ05g2LBhePzxxzF69GgsXboU999/P9atW4c+ffqI6qQoCu6++2788ccfePzxx9G8eXOsWrUKo0ePLnecGzZswJAhQ9CiRQvMnDkT6enpGDt2rPn82GLo0KFo0KABZs6cib179+LTTz9FcHAw3nnnHXPMmDFj8O233+Khhx5Cp06dsHXrVou2WxVb22hVruX8LF++HKNHj0ZsbCzeeecdFBQUYOHChejSpQv27dtn0TkyGo2IjY1Fx44dMWvWLPz222+YPXs2GjVqhAkTJgC48rk5YsQI9OrVy/waHTlyBNu3by/3efNvy5Ytw9ixY3Hbbbdh5syZSE5Oxvvvv4/t27dj3759FqPx1VkPIqemkFMoLS1VwsLClJiYGIvtixYtUgAo69evN29bunSpAkA5c+aMoiiKsmrVKgWAsnv37kr3//vvvysAlN9//91i+5kzZxQAytKlS83bCgoKyj3/q6++UgAo27Ztq7QeiqIo3bt3V7p3715pPb799lsFgPL6669XWV58fLwCQPnvf/9r3vbdd99VeAwVlTtv3jwFgPLFF1+YtxkMBiUmJkbx8vJScnJyLI4/ICBAycjIMMf++OOPCgDl559/rvRYFEVRnn76aQWA8r///c+8LTc3V2nQoIESGRmpGI1G83YAypNPPlnl/srUr19fGTBggMW2is7V6NGjFQDK1KlTLWL37dunAFC+++67Ksvx9PRURo8ebVOdFEVROnfurMybN0/58ccflYULFyotW7ZUACgfffSROSY1NbXcOS6zYMECBYBy9OjRKsupqE3ExsYqDRs2tNhWv359BYDy559/mretX79eAaC4u7sr586dM2//+OOPy7WfV155Rbn6MghAcXNzU06ePGne9tdffykAlPnz55u3DR48WHFzc1NOnTpl3nbp0iXF29tb6datW5XHV9ExGgwGpWXLlkrPnj3N206cOKGo1WrlnnvusWhLiqIoJpPJ/N+Vtf9vv/3WvC0/P19p3LixxWtgazupyqlTp5SwsDClefPmSmpqqjJjxgwFgPL222/bvI9vvvlGGTNmjPL5558rq1atUl566SXFw8NDCQwMVM6fP2+Oe++99xQAFtvK3HbbbUqnTp2qLGfGjBmKp6encvz4cYvtU6dOVTQajXm/Ze+1oKAgJSsryxw3bdo0BYDSpk0bpaSkxLx9xIgRipubm1JUVGTeVtY2v//+e/O27OxsJSwsTGnXrp24TqtXr1YAKO+++645prS0VOnatWu560Lbtm2VsLAwi7pv2LBBAaDUr1/fohwAyiuvvGL+d9l7Yty4cRZx99xzjxIQEGD+d0JCggJAefrppy3ixowZU26fFbG1jSrKlevcv+t9recnNzdX8fX1VR555BGLOiUlJSk+Pj4W28uusVdfz9q1a6dER0eb//3UU08per1eKS0trfSYr/4cNhgMSnBwsNKyZUulsLDQHLdmzRoFgDJ9+vRqqQfRjYa3gTkJjUaD4cOHIz4+3uL2pxUrViAkJAS9evWq9Lllv7ysWbMGJSUl11yXf4/gFBUVIS0tDZ06dQIA7N271+79Hj58GOPGjcPdd9+Nl156qcLySkpKkJ6ejsaNG8PX19fu8n755ReEhoZixIgR5m2urq6YPHky8vLysHXrVov4YcOGwc/Pz/zvrl27AoDV1at++eUXdOjQAV26dDFv8/LywqOPPoqzZ8/i8OHDdtVfquxXtTJlv4ivX7++3K0b16Ls17lBgwbh8ccfR0JCAlq2bIkXX3wRhYWFAGD+/4oWgCi7X7sspjL/bhPZ2dlIS0tD9+7dcfr06XK3J7Vo0QIxMTHmf5eNaPTs2RP16tUrt92WFcl69+5tMcrUunVr6PV683ONRiM2bNiAwYMHo2HDhua4sLAwPPDAA/jjjz+Qk5Nj8zFmZmYiOzsbXbt2tWjzq1evhslkwvTp08uNBlU1wvnLL78gLCwM9913n3mbh4eHeQStjCPaScOGDbF+/XokJSUhOjoaL7/8Mp577jm88MILNu9j6NChWLp0KUaNGoXBgwdjxowZWL9+PdLT0/Hmm2+a46y1LWvt6rvvvkPXrl3h5+eHtLQ086N3794wGo3Ytm2bRfz9999vMbpU1oYefPBBi9vQOnbsCIPBUO7WnTp16uCee+4x/1uv12PUqFHYt2+feZUzW+v0yy+/wMXFxeK9rtFoMGnSJIsyL1++jP3792P06NEWde/Tp0+5eXBVefzxxy3+3bVrV6Snp5vbddltkWUjl2Wurk9lbG2jVbH3/GzcuBFZWVkYMWKExWuu0WjQsWNH/P777+XKquj1+Pe1xNfXF/n5+di4caPN9d+zZw9SUlLwxBNPWMxlGTBgAKKioixuJ67OehA5O3ZWnEjZXIMVK1YAuHI/7//+9z8MHz68ygn13bt3x5AhQ/Daa68hMDAQd999N5YuXVrhvb+2yMjIwFNPPYWQkBC4u7sjKCjIvEKPLfexVyQnJwf33nsv6tati//+978WX7QKCwsxffp08/3agYGBCAoKQlZWlt3lnTt3Dk2aNCn3Ba/strFz585ZbP/3l1oA5o5LZmam1XKaNWtWbntl5VQHFxeXcrd3NGjQAHFxcfj0008RGBiI2NhYLFiwwO7XszJubm6YOHEisrKykJCQAOCfL+EVtb+yeS6V3dJYZvv27ejduzc8PT3h6+uLoKAgvPjiiwDKt8Grz13Zl5eIiIgKt1s7pxXtE7jSJsqem5qaioKCgkrPvclkQmJiYpVlrFmzBp06dYJOp4O/vz+CgoKwcOFCi+M7deoU1Gq16EsmcKXdNW7cuFyH5ur6OqqdtGrVCpMmTcL58+cRGhqK1157TfT8inTp0gUdO3bEb7/9Zt5mrW1Za1cnTpzAunXrEBQUZPHo3bs3AJSbM3itbauic9C0aVMAMP8oZWudzp07h7CwMHh5eVns7+pzWnbN+fftc5XFVsXaNfHcuXNQq9XlVm9r3LixTfu3tY1K6mjr+SmbY9WzZ89yr/uGDRvKtQOdToegoCCLbf++HgBXOm1NmzbFnXfeifDwcIwbN67CeW7/VnauKjrmqKiocp8f1VUPImfHOStOJDo6GlFRUfjqq6/w4osv4quvvoKiKOZOTGVUKhVWrlyJHTt24Oeff8b69esxbtw4zJ49Gzt27ICXl1elv8JWNJF16NCh+PPPP/Hcc8+hbdu28PLygslkQr9+/exePnLMmDG4dOkSdu3aBb1eb/G3SZMmYenSpXj66acRExMDHx8fqFQqDB8+/LotV1lZZ1BRlOtS/rXQarUVzsGYPXs2xowZgx9//BEbNmzA5MmTMXPmTOzYsUN077o1ZV8Myiaw+vv7Q6vV4vLly+Viy7bVqVOn0v2dOnUKvXr1QlRUFObMmYOIiAi4ubnhl19+wdy5c8u1icrO3bWc0+puD//73/8waNAgdOvWDR999BHCwsLg6uqKpUuXmn+suF4c0U5+/vlnvPXWW+jRowfi4+MxdOhQrFq16ppXWouIiMCxY8fM/w4LCwNwpR1d/YX08uXL6NChQ5X7M5lM6NOnD55//vkK/17WkShTHW3rWut0vdwI10R7z0/ZNWT58uUIDQ0tF3d1u7Vl9c3g4GDs378f69evx6+//opff/3VPFp49cIu9nKWehBdb+ysOJmRI0fi5Zdfxt9//40VK1agSZMmFisqVaVTp07o1KkT3nzzTaxYsQIjR47E119/jYcfftj8q9jVK8dc/ctNZmYmNm3ahNdeew3Tp083b796tR+Jt99+G6tXr8YPP/xQ4bKiK1euxOjRozF79mzztqKionJ1lUzsr1+/Pv7++2+YTCaLL/JHjx41/90R6tevb/FlqrrKsVerVq3QqlUrvPTSS/jzzz9x++23Y9GiRXjjjTcAyF7TypTdglD2i59arUarVq2wZ8+ecrE7d+5Ew4YNq5xc//PPP6O4uBg//fSTxS+nFd2aUVOCgoLg4eFR6blXq9Xlvkz/2/fffw+dTof169db3NK0dOlSi7hGjRrBZDLh8OHDFS4PXJn69evj4MGDUBTF4hxXVF/AejupytatWzF06FD07t0bP/30E37++WcMHToUY8aMwfLly6+pjZ0+fdril+Sy12DPnj0WHZNLly7hwoULVm8hatSoEfLy8syjFtXt5MmT5c7B8ePHAcA8gdvWOtWvXx+bNm1CXl6exejK1ee07JpT0TW7svNvj/r168NkMuHMmTMWozgnT560+fmSNupIZbd4BgcHO7QtuLm5YeDAgRg4cCBMJhOeeOIJfPzxx3j55ZcrHHEqO1fHjh1Dz549Lf527Ngxuz8/pPUgcna8DczJlI2iTJ8+Hfv377c6qgJc6WBc/WtX2Yd62e0S9evXh0ajKXdP9kcffWTx77Jfbq7e39Wr5Njqt99+w0svvYT//Oc/lSYe1Gg05cqbP39+uVGfstwhlS3V+W/9+/dHUlISvvnmG/O20tJSzJ8/H15eXujevbvsQKooZ9euXYiPjzdvy8/PxyeffILIyEjx7TuOkpOTUy6jd6tWraBWqy1uofH09LTp9QRQ4Wpkubm5mDdvHgIDAxEdHW3eft9992H37t0WHZZjx45h8+bN5iWoK1NRG8zOzi73Rb4maTQa9O3bFz/++KPFHLPk5GSsWLECXbp0KTeCePXzVSqVRRs/e/YsVq9ebRE3ePBgqNVqvP766+VGlKr6hbt///64dOmSxVK+BQUF+OSTTyzibG0nldm7dy8GDRqE6OhofP/993B1dcW9996LTz75BF9++SUmT55sdR9AxW3rl19+QUJCAvr162fedssttyAqKgqffPKJxWu3cOFCqFQqi/kPFRk6dCji4+Oxfv36cn/Lysoq91pcq0uXLlksGZyTk4P//ve/aNu2rfkXfVvr1L9/f5SWllos4W00GjF//nyL54SFhaFt27b4/PPPLW7n27hxo0Pn0MXGxgIo/xlydX0qY2sbrQ6xsbHm3DwVzfOsbOXFqly9tL9arUbr1q0BVHzbInBlVbvg4GAsWrTIIubXX3/FkSNHbF5Z7VrrQeTsOLLiZBo0aIDOnTvjxx9/BACbOiuff/45PvroI9xzzz1o1KgRcnNzsXjxYuj1evTv3x/AlXt277//fsyfPx8qlQqNGjXCmjVryt2bq9fr0a1bN7z77rsoKSlB3bp1sWHDBpw5c8au4xkxYgSCgoLQpEkTfPHFFxZ/69OnD0JCQnDXXXdh+fLl8PHxQYsWLRAfH4/ffvutXJbztm3bQqPR4J133kF2dja0Wq05F8fVHn30UXz88ccYM2YMEhISEBkZiZUrV2L79u2YN29elb/sS0ydOhVfffUV7rzzTkyePBn+/v74/PPPcebMGXz//fdWl8itLps3b8bEiRNx//33o2nTpigtLcXy5cuh0WgwZMgQc1x0dDR+++03zJkzB3Xq1EGDBg0qXXJ3wYIFWL16NQYOHIh69erh8uXLWLJkCc6fP4/ly5dbJCZ84oknsHjxYgwYMADPPvssXF1dMWfOHISEhJiXY65M3759zb8MPvbYY8jLy8PixYsRHBxc4a1lNeWNN97Axo0b0aVLFzzxxBNwcXHBxx9/jOLiYrz77rtVPnfAgAGYM2cO+vXrhwceeAApKSlYsGABGjdujL///tsc17hxY/znP//BjBkz0LVrV9x7773QarXYvXs36tSpg5kzZ1a4/0ceeQQffvghRo0ahYSEBISFhWH58uXw8PCwiLO1nVTmhRdeQMOGDbF27VqLfY8bNw6ZmZl44YUX8Oijj1pdvrhz585o164d2rdvDx8fH+zduxdLlixBRESEea5Smffeew+DBg1C3759MXz4cBw8eBAffvghHn744UqXMi/z3HPP4aeffsJdd92FMWPGIDo6Gvn5+Thw4ABWrlyJs2fPIjAw0Opx26pp06YYP348du/ejZCQECxZsgTJyckWHW9b6zRw4EDcfvvtmDp1Ks6ePWvO2VLR/KKZM2diwIAB6NKlC8aNG4eMjAzMnz8ft9xyS7ml5O0VHR2NIUOGYN68eUhPTzcvXVw2cmRtRM3WNlod9Ho9Fi5ciIceegi33norhg8fjqCgIJw/fx5r167F7bffjg8//FC0z4cffhgZGRno2bMnwsPDce7cOcyfPx9t27attF26urrinXfewdixY9G9e3eMGDHCvHRxZGQkpkyZIj42e+pB5PRqYAUysqJsedcOHTpU+Perlwzeu3evMmLECKVevXqKVqtVgoODlbvuukvZs2ePxfNSU1OVIUOGKB4eHoqfn5/y2GOPKQcPHiy37OWFCxeUe+65R/H19VV8fHyU+++/X7l06VK55ShtWboYQKWPsuUbMzMzlbFjxyqBgYGKl5eXEhsbqxw9elSpX79+uWV1Fy9erDRs2FDRaDQW+6hoyeTk5GTzft3c3JRWrVpZHKei/LME5nvvvVfudb76eCtz6tQp5b777lN8fX0VnU6ndOjQQVmzZk2F+6uOpYs9PT3LPf/06dPKuHHjlEaNGik6nU7x9/dX7rjjDuW3336ziDt69KjSrVs3xd3dXQFQ5TLGGzZsUPr06aOEhoYqrq6uiq+vr9K3b19l06ZNFcYnJiYq9913n6LX6xUvLy/lrrvuUk6cOGHT8f/0009K69atFZ1Op0RGRirvvPOOsmTJknLtraLXSVEqfq0rOteVLV1c0XmqqD3u3btXiY2NVby8vBQPDw/ljjvusFhGuSqfffaZ0qRJE0Wr1SpRUVHK0qVLK6yPoijKkiVLlHbt2ilarVbx8/NTunfvrmzcuNH894ra/7lz55RBgwaZlwB+6qmnlHXr1lm8b2xtJ5W5dOmSkpycXOnf9+3bZ9N+/vOf/yht27ZVfHx8FFdXV6VevXrKhAkTlKSkpArjV61apbRt21bRarVKeHi48tJLLykGg8GmsnJzc5Vp06YpjRs3Vtzc3JTAwEClc+fOyqxZs8z7qOy6ULb07NVLPZddC/+9fHxZ21y/fr3SunVr83muaJloW+qkKIqSnp6uPPTQQ4per1d8fHyUhx56yLz89NXXtu+//15p3ry5otVqlRYtWig//PBDuSWAFaXypYtTU1MrPMZ/v//y8/OVJ598UvH391e8vLyUwYMHK8eOHbN56Wpb2qiiVL508bWcn7L42NhYxcfHR9HpdEqjRo2UMWPGWHx2VnaNvfq9unLlSqVv375KcHCw4ubmptSrV0957LHHlMuXL5er39XL73/zzTfm97e/v78ycuRI5cKFCxYxjqwH0Y1GpShONFuOiIiIblj79+9Hu3bt8MUXX9h0ZwARkTWcs0JERERiFeW1mTdvHtRqNbp161YDNSKimxHnrBAREZHYu+++i4SEBNxxxx1wcXExL5X76KOPVrkaHhGRBG8DIyIiIrGNGzfitddew+HDh5GXl4d69erhoYcewn/+859rzrFDRFSGnRUiIiIiInJKnLNCREREREROiZ0VIiIiIiJySjf9TaUmkwmXLl2Ct7e31SRVRERERHT9KYqC3Nxc1KlTp8YSKlelqKgIBoOhWvbt5uYGnU5XLfu+GdwQnZUFCxbgvffeQ1JSEtq0aYP58+ejQ4cONj330qVLXJWEiIiI6AaQmJiI8PDwmq6GhaKiIjSo74WkFGO17D80NBRnzpxhh6USTt9Z+eabbxAXF4dFixahY8eOmDdvHmJjY3Hs2DEEBwdbfb63tzcAoO6b/4Haxkbgv0/eoy/2lY3a5EXJe+d+e11F8ZntSkXxnZqfEsUDwP4NUaJ4RSMuAiW+JlF8nS2y+Mhnj4niAeBoZogoPjvHXVxGaansxWoXmSiK33ssUhQPAP57ZJeMgmD5aGZRw2LZEwplr5PaIH9/m9xkbco7LE9cRvFBH1G8a57stXW7PV0UDwA5ubJ26+Iq/yB3FT6n6ITsddKlytvgwBF/iOI3z+4sLiM1VtbOXc7LvsR4nReFAwAyomWfGS458q8QKlkRaNLpnCi+6HXr3wuudnqomyjeRS///C7NlpWhTZW9ttpMUTgAwCQ8faXyjzGoBJdOY3ERTn/4uvl7mzMxGAxISjHiXEIk9N6OHfXJyTWhfvRZGAwGdlYq4fSdlTlz5uCRRx7B2LFjAQCLFi3C2rVrsWTJEkydOtXq88tu/VLrdFC729YING7yhqjRyj4Q1e52lOEm66yo3WWfCq6esospAGi0sjeWYkeLM+pkXxRdXGXxbl52HHexVhSvLpVfgNQlsi/h0vNn6/vh3zRushMofV8AgNpd+hxhZ8We2wu0sjal8SgRF6ERfkhpSmSvk8ZD1mYBQG0U1smOzorGVXadsvVHJ/P+7WiDWi/ZtdbF1Y73t4fwM0N63PLLmvgzQ22o/s6K9LpW6mLHuXAXXjs95NcQtUFWhkYnvdaKwgEAKuHpU+wpQ3bpvPIcJ75l38tbBS9vx9bPBOc9Xmfh1J0Vg8GAhIQETJs2zbxNrVajd+/eiI+Pr/A5xcXFKC7+5xernJycaq8nEREREd3cjIoJRgcn/DAqdvToahnnm8H0L2lpaTAajQgJsbzlJiQkBElJSRU+Z+bMmfDx8TE/OF+FiIiIiOjG5NSdFXtMmzYN2dnZ5kdiouw+fiIiIiKiq5mgVMuDqubUt4EFBgZCo9EgOTnZYntycjJCQ0MrfI5Wq4VWa8eNlURERERE5FScemTFzc0N0dHR2LRpk3mbyWTCpk2bEBMTU4M1IyIiIqLaxFRN/6OqOfXICgDExcVh9OjRaN++PTp06IB58+YhPz/fvDqYrRSPUig2rnSS3Uy+jEpovGyJkxJveRklnrL4ge32i+J/+b29rAAAOuGiRwZPO4Y7hU85P0AWn766lewJAHret1sUv1cjnzt1KcVXFJ9Z7CGKj1grX4Hk0u2yeJcCcRFQFQhXwcmT/eYyuM8OUTwA/HCorSi+gV+GuIwL53xF8aXC60Hx9kDZEwB4CFdozW0iXOoJgOqi7HyXRsguOrl+onAAwLrZ3UTxaTF2XNekbz9hvD3LxLufl62CVhgpX/VOVSh7vx78u74oPjTcjpWVhCvSef9PvoZvwR2y5cyLPWXnwi1LvgpaXqRs9b6gPfLft1N72b5Et6mwSLx/qh2cvrMybNgwpKamYvr06UhKSkLbtm2xbt26cpPuiYiIiIiqi1FRYFQcO8fE0fu7GTl9ZwUAJk6ciIkTJ9Z0NYiIiIiI6Dq6ITorREREREQ1qTpW7+JqYNaxs0JEREREZIUJCozsrFx3Tr0aGBERERER1V4cWSEiIiIisoK3gdUMjqwQEREREZFT4sgKEREREZEVXLq4ZnBkhYiIiIiInFLtGVkxqq48bFDqLu/lprYTZmH2kJdh1MrifznWUrb/QGG6agCmLFmlVCZxEQhomi6KTz8RIIq351zEJzUQxaeflqfR1qbJUlCfc/UXxZf2lGd6dpElYUZxuLxN2fo+LePSsEAUv/58lCgeAHx8ZGX8dUiWdRsA3MNkx10cKHszKS7ydu6/X/Z7lku2PG16Ybgse7iqVFYnXZK8TkWySwj8DoiLQHq0LEN5qafs/BUGy3+LlLYp11T5V4iAA7LjSOopy7JudJXXyTVVdi5y5W9v4LSXKNzvFtnnnkb4WQwA6mOy90apTn4NUWW42R5bZMcXhOvM9P8PR++TqsaRFSIiIiIickq1Z2SFiIiIiMhOxmrIs+Lo/d2M2FkhIiIiIrLCqFx5OHqfVDXeBkZERERERE6JIytERERERFZwgn3N4MgKERERERE5JY6sEBERERFZYYIKRsiX/be2T6oaR1aIiIiIiMgp1ZqRFU2WK9RFtiV+MnrJklABgHuyrGdc6i7vSZfIckpB5SI7Ds897rICAJTqhE+w4+bMgmLbk0rZU4bJjneBm4sskZ1vZJa4jEwvb1G8S6KHrAB3+clQhPn13C7Lkq0BQKmXbGmUolJZuy3Syo97YJu/RPG/Hm0vLqMoRPZ+1SXLTkZho2JRPADkNJK999TySye8j8vegCphGbmN5JXSpcl+x8u8Rb6cj9ogK0NtkH1maOzIx2oSvjdc8u1IuOkvO26VVnYgeeHyaw6ES8caPeXXEJc82XFnH5VlJnUPlH+nyG9dJIpvuFRcBLKbCpJC2nH9uN5MypWHo/dJVePIChEREREROaVaM7JCRERERGQvYzXMWXH0/m5G7KwQEREREVnBzkrN4G1gREREREQ3iAULFiAyMhI6nQ4dO3bErl27qozPysrCk08+ibCwMGi1WjRt2hS//PLLdarttePIChERERGRFSZFBZPi4KWLhfv75ptvEBcXh0WLFqFjx46YN28eYmNjcezYMQQHB5eLNxgM6NOnD4KDg7Fy5UrUrVsX586dg6+vr4OOoPqxs0JEREREdAOYM2cOHnnkEYwdOxYAsGjRIqxduxZLlizB1KlTy8UvWbIEGRkZ+PPPP+HqemW1vMjIyOtZ5WvG28CIiIiIiKwom7Pi6AcA5OTkWDyKi8svN28wGJCQkIDevXubt6nVavTu3Rvx8fEV1vmnn35CTEwMnnzySYSEhKBly5Z46623YDTeAGtF/z92VoiIiIiIalBERAR8fHzMj5kzZ5aLSUtLg9FoREhIiMX2kJAQJCUlVbjf06dPY+XKlTAajfjll1/w8ssvY/bs2XjjjTeq5TiqQ625DUxToILGZNt9gSqTPNFVbn1ZvGueuAhxQj71IVkWSW3vVFkBAFTrgkTxJjf5vZ49Ik6K4n+53FYUrwiTrQHye0xzj/iLy9AIf0owhsiS/ikF8re/MUSWoE2dKEzoCaBxqwui+FP7w0XxJg/5r0m/Hr9FFO+SJ29TSrjs/BWZZBlZdV7yLIFuKVpRvN/REnEZaW1kSfzcsmUZ1LzPyK/n0kSxXufl57ugQ4HsCYmy5Kelt+XK9g9Ad0iWiFYly40LADDoZfGuibI2qMuQZ9jTZsnis5vKz7f3GVl8YZCsDO9EeaJKo052DSkIkZehKbL9OFSC2JpihBpGB//OX/aJlJiYCL3+nzeIVitr+5UxmUwIDg7GJ598Ao1Gg+joaFy8eBHvvfceXnnlFYeUUd1qTWeFiIiIiMgZ6fV6i85KRQIDA6HRaJCcnGyxPTk5GaGhoRU+JywsDK6urtBo/vnhpnnz5khKSoLBYICbm/xHxeuNt4EREREREVmh/P9qYI58KII7Ndzc3BAdHY1NmzaZt5lMJmzatAkxMTEVPuf222/HyZMnYTL9MzJ2/PhxhIWF3RAdFYCdFSIiIiIiq6pzgr2t4uLisHjxYnz++ec4cuQIJkyYgPz8fPPqYKNGjcK0adPM8RMmTEBGRgaeeuopHD9+HGvXrsVbb72FJ5980qGvTXXibWBERERERDeAYcOGITU1FdOnT0dSUhLatm2LdevWmSfdnz9/Hmr1P2MRERERWL9+PaZMmYLWrVujbt26eOqpp/DCCy/U1CGIsbNCRERERGSFUVHDqDh4gr18TQhMnDgREydOrPBvW7ZsKbctJiYGO3bskBfkJHgbGBEREREROSWOrBARERERWWGCCiYH/85vgh1DK7UMR1aIiIiIiMgp1ZqRFXUJoLaxa2ayIw9PSbAsGVqd7fJ+YmJv2XPck2TxOXtkCR4BAIGycPe2GeIi1ia0EcW75Apfp1R5Iqr0LFnCTbdseRkletmvLUZhkke1lzyBn6lU9tqqbEzE+m9nUmUJNNXSpJ5psiSEAODWoEgUX+om/6VMc0aW9K9BJ1nyzBKjPDliTpGsnSffJn9tTa6y16rUQ7Z/v6PyRHZpbWRtyvOivJ2X5siWDFVrZa+T6Zjs3AGAyV1Whj3XtcI6wqSswmtIqYf8s1VTJHxtfeTZMA2+svNdFCJ7nVTHReEAgOIA2XvDcFl+vl1zbX+OsfhGSAopX73Lln1S1TiyQkRERERETqnWjKwQEREREdmrelYD45wVa9hZISIiIiKy4soEe8fetuXo/d2MeBsYERERERE5JY6sEBERERFZYYIaRi5dfN1xZIWIiIiIiJwSR1aIiIiIiKzgBPuawZEVIiIiIiJyShxZISIiIiKywgQ1TJyzct3Vms5KSbNCGD1saxDe22WZpAEgO1TW2C72kDd2XYrsOUVBsuy0ulR5naQZcLNS5VmVxZnWc2SZugvqyDNcGwtlmbpLpVmbAdTdLIu/0EcWb8qXZxuHVnYcGlnidwBAcYEs0/OdvfeJ4n9fc6soHgA8dQZRfGqIPMO1PihPFJ+Y7iuKN56xI6N5U9l1LWy7/L3kPuGSKP7M3nBRfFIP+XvPNV320WiUNVkAgM8hWRm5HQtF8S7J8s+xgqbFonhNsfzANYWyzxnvqAxRfOm5AFE8AGS2krURjxPy43ZPkb2X8mXNHJd7yOIBQCPILg8AgftzxGVc7uJje7Cs+VEtUms6K0RERERE9jIqKhgVx+ZFcfT+bkbsrBARERERWWGshqWLjbwNzCpOsCciIiIiIqfEkRUiIiIiIitMihomBy9dbOLSxVZxZIWIiIiIiJwSR1aIiIiIiKzgnJWawZEVIiIiIiJyShxZISIiIiKywgTHLzUsz05V+9SazoqSroWSr7UptrBHrnz/Obbtu4zKKG/sJT6yoUKTu+wtUFhHPhTpcUGWgNGUJU+m5XFZVq/gB8+J4o+cqSOKBwBVoey4gxumi8tIvxwsivc6I9u/wVt+vgNvSxPFp10MEZchHRFPKvQWxReFyRM2FqXoRfH6Q/KEm7m3ypL4tY1MFMX/XVpXFA8AfptkdUrqJC4CQZ/J6qWLkF078/3tuK5dlpWR3VaWNBQA1Nmyj18lXfYZU9BEXicYZDdbeFyWF2Fylb22WaGeonhNsPx8B++QXc+zhMlSASCth+x8eB6Wne9iX3mdXPNk5yKnsexaCwA+Z22/3paWyK/NVDvUms4KEREREZG9TFDD5OAZFI7e382InRUiIiIiIiuMihpGBy9d7Oj93Yz4ChERERERkVPiyAoRERERkRUmqGCCoyfYO3Z/NyOOrBARERERkVPiyAoRERERkRWcs1Iz+AoREREREZFT4sgKEREREZEVRqhhdPDv/I7e382o1nRWTG4mQGtbkkTNfnniI80thbL6uMhfepOPMGGSSTZpq0GTJNn+ASRdDhfFFwUbxWWUeMveyFnCZIoeJ+SJKgvDZMeRuTdIXIZXqizJl+7eZFF8zqZQUTwAXD4uO47AE+IikBoia7cnf2wiile1LhLFA4CLm+x8+56SJZkDgJzmsuf8vauRrAA75nBqc2SJZT0uyY9bm1Uiik+6Q3YuXD3lyRENPsJrgjCZIgB4XJQ9Jz9Sdty6c/LrWlGE7LUq8ZI3qvx6suNQp8iOw56pyqm3ydq5Z6K8nQeskyWKzW4g239JkDyholtUgSg+ubm8TdVdKU+QS3S1WtNZISIiIiKyl0lRwaQ4eDUwB+/vZsSxJyIiIiIickocWSEiIiIissJUDXNWTBw3sIqdFSIiIiIiK0yKGiYHLzXs6P3djPgKERERERGRU+LIChERERGRFUaoYLRrzbmq90lV48gKERERERE5JY6sEBERERFZwTkrNaPWdFbUniVQe9iWyMk9WZ7wSdNBlmiuJMddXAYiikXhKpUsqWDGD7IEjwDgkyFLpqUplr+2RmEeqmK17LUtCpEdAwD0j9kvil+b0EZcRkaALF51TJYM09VH1j4AQBMoa4M+p8RFoM0TskySuS20ovgDSWGieHt4P5Mifo7qnKxe7idkydYMdpzvzKbCD9F2OeIydHdlyuK31RfFF9lxukvqChPsucqvIQZf2flQdLJkigY/+a0lfsG5sjJOCi9SAFRGWb2UENk1pzRTnrhQyr17qvg5SWf9RfH1f5YlS00MsON7y2EfUbxedioAABlRtscai12AtfIy6OZXazorRERERET2MsLxc0xkP0PUThx7IiIiIiIip8SRFSIiIiIiKzhnpWaws0JEREREZIVRUcPo4M6Fo/d3M+IrRERERERETokjK0REREREVihQweTgCfYKk0JaxZEVIiIiIiJyShxZISIiIiKygnNWagZfISIiIiIickq1Z2QlUwsU2pbpOruJfPdKvk72BC95xmPXA96ieJUwCXN2c3lqotJzsqy5de48Ly7jxNG6oni1r0EUrwgzKgPA7+cbi+IjGyWLy8gqcBfFG3bIMiQXBcnbIJJk7fz0vfIiTu1pKX+SRKn8fKuEb40TR/XiMrRN80TxhbfKzp+pVP7blMdO2fk2JMiP+1gDWTtXBcpOhs9BV1E8AJR6yOI1ReIiYPCVxXsflmVmL5F9XFx5zh+yjPSelxVxGfkNZOfPb7usDWZHyesklfV3oPg5GuFlJ6e+9P0qP+5S4dcWQ4i8jICDtj/HaLDjM+k6MykqmBTHzjFx9P5uRjU6srJt2zYMHDgQderUgUqlwurVqy3+rigKpk+fjrCwMLi7u6N37944ceJEzVSWiIiIiIiuqxrtrOTn56NNmzZYsGBBhX9/99138cEHH2DRokXYuXMnPD09ERsbi6IiO37GIiIiIiKykxHqanlQ1Wr0NrA777wTd955Z4V/UxQF8+bNw0svvYS7774bAPDf//4XISEhWL16NYYPH349q0pEREREtRhvA6sZTtudO3PmDJKSktC7d2/zNh8fH3Ts2BHx8fGVPq+4uBg5OTkWDyIiIiIiuvE4bWclKSkJABASEmKxPSQkxPy3isycORM+Pj7mR0RERLXWk4iIiIhufiaoq+VBVbvpXqFp06YhOzvb/EhMTKzpKhEREREROcSCBQsQGRkJnU6Hjh07YteuXTY97+uvv4ZKpcLgwYOrt4IO5rSdldDQUABAcrLlkq/Jycnmv1VEq9VCr9dbPIiIiIiIroVRUVXLQ+Kbb75BXFwcXnnlFezduxdt2rRBbGwsUlJSqnze2bNn8eyzz6Jr167X8hLUCKftrDRo0AChoaHYtGmTeVtOTg527tyJmJiYGqwZEREREdH1N2fOHDzyyCMYO3YsWrRogUWLFsHDwwNLliyp9DlGoxEjR47Ea6+9hoYNG17H2jpGja4GlpeXh5MnT5r/febMGezfvx/+/v6oV68enn76abzxxhto0qQJGjRogJdffhl16tSxa/jKr2EmNB62JYXMTZAnfOre5Kgo/s997cRl5DWWZXn0OCc7vSo7kiO65smSRCXl2pGpzFN23MZcWRI47xPyt0GdAbIkj+fSZQkbAaD4kqco3sVHmLAruFgWD8BULEsC6psgS2QHAHmRsuNo3vGMKP7o9gaieAAo8RdmhSyUvU4A0CQ4VRR/eE+kKF5bL18UDwB59W27ZpbxaJolLkMjTKhr0squUzntS0TxAKBSC9tgROXzKCtz+FA9UbzBR3bckWvlx51TT/Z+zWkg/8wI2SZ7b2Q3lpURskOeuDC5gyxemy4/bqMwAWNmC1mCRF1dWVJZADCUyj6PdXYcd3IP29uhqbAU+EZcxHVVnauBXb0glFarhVZreQ02GAxISEjAtGnTzNvUajV69+5d5eJTr7/+OoKDgzF+/Hj873//c2Dtr48a7azs2bMHd9xxh/nfcXFxAIDRo0dj2bJleP7555Gfn49HH30UWVlZ6NKlC9atWwedTviuJyIiIiJyUlcvCPXKK6/g1VdftdiWlpYGo9FY4eJTR49W/KP5H3/8gc8++wz79+93ZHWvqxrtrPTo0QOKUvmvICqVCq+//jpef/3161grIiIiIiJLiqKGSXHsDArl//eXmJhoMc/66lEVe+Tm5uKhhx7C4sWLERgov2vIWdRoZ4WIiIiI6EZghApGOPY2sLL92bIoVGBgIDQajc2LT506dQpnz57FwIEDzdtMpiu3GLq4uODYsWNo1KjRtR5CtXPaCfZERERERHSFm5sboqOjLRafMplM2LRpU4WLT0VFReHAgQPYv3+/+TFo0CDccccd2L9//w2Ti5AjK0REREREVpgUVMMEe1l8XFwcRo8ejfbt26NDhw6YN28e8vPzMXbsWADAqFGjULduXcycORM6nQ4tW7a0eL6vry8AlNvuzNhZISIiIiK6AQwbNgypqamYPn06kpKS0LZtW6xbt8486f78+fNQq2+uG6fYWSEiIiIissJUDRPs7dnfxIkTMXHixAr/tmXLliqfu2zZMnF5Ne3m6noREREREdFNo9aMrBRtD4BGa1t+FsVbnlRqT7JskpLRjhXpXLNkybQK6wgT2ckPG57JsiddOuMjLkOXIetTF/vLkml5XZTFA0B6gSxhY3Gyh7gMxUN2/kylsvah2JEEVOstSySpNsoSdAKAUSc7HwcP1JcVUMcgiwegOyN7w+pk+R0BAAeOya4hTdteEMUH6ORJIXefjhLF556Tv799jsne31m3ys6fJkv+MWf0lLXBo3uEbRBAYPN0UXzuriBR/KXb7fiQEV4SVLJ8vQCAzOayQgyBskLS3OUJWd2yZHVyKRAXgZIOuaJ49XnZZ4zr/6peRaoiRU2E3xHS5a+t/27bPwOMBiNkV7XrzwQVTA5eDczR+7sZcWSFiIiIiIicUq0ZWSEiIiIispdRUcHo4NXAHL2/mxE7K0REREREVjjLBPvahq8QERERERE5JY6sEBERERFZYYLK8UkhOcHeKo6sEBERERGRU+LIChERERGRFUo1LF2scGTFKo6sEBERERGRU6o1Iyv5jUqgtjFZlGua/GXJPu4vivfOFBcBkzDHV1hbWWa6gmI3WQEAivz8RPG6ejniMsJbZ4viEzfXE8VrioWJsQBcTpYl4PKskycuw9ejUBSf/FeIKF4plLdz14O2JVYtk9VcnnDTPUmWeEzVXtY+3N1KRPEAUHBc9uYzyl4mAIBviCxp3Nk02TXnyVs3i+IBYG+DcFG8+oSXuIxSWe47eAXIMvKVXPSVFQAgpGWKKD4tQfbeAwB8GygK1wlf2hJvWTwAFETIroWh2+S/CCd3ksW7CRMRagrldSrRy5Ibl7aTXXMAIOxjWUNP7CPbf/CgRNkTAHiVyJL2XlZkiUkBwO8321/b0hL5Z/H1ZlKqYc4Kly62iiMrRERERETklGrNyAoRERERkb2YZ6VmsLNCRERERGQFbwOrGezOERERERGRU+LIChERERGRFaZqWLqYSSGt48gKERERERE5JY6sEBERERFZwTkrNYMjK0RERERE5JQ4skJEREREZAVHVmpGremsqPM0UJfalgm3NMwg3n+jCFnG48R8WWZoAICwQV+6KMtw7Zoiy2YLAF7CpPfFRcInQJ6pWy08DKNWfqHwC5JlG8/JcxeXkZHgI4ovDS8VxatK5Mdt8JFleoY8gT3Ce50XxfvpZBnN98Q3FcUDgJssiTaKgoSvE4DS87LzrSmUDYw/lTVSFA9AfM3xviVLXIQp3k/2hB2+onCDMCs7IP/yYLLjkzR/UI4o3nBSL4p3y5a/v71Oyxp6nh0fY6pS2XtD+tqWNC2SPQGAx2GdKD4vVZaNHgDy7pMdd+gm2f7PhAXIngBASZIdtzZHfjNOegvbY43FLsBacRFUC9SazgoRERERkb04slIz2FkhIiIiIrKCnZWawQn2RERERETklDiyQkRERERkhQLHJ3GUz3CsfTiyQkRERERETokjK0REREREVnDOSs3gyAoRERERETkljqwQEREREVnBkZWaUWs6K/qTKmjcbGsQxX5a8f5PZ9YVxeuK5I2z1FM2DUuTJTu9KjsS+KmEM8OMBvlgnilTlkhSny7bf1pr+bnoFHRZFL/fJGsfAFDkKUskqfYqEcWb8uRJQI3Ct4ZLofy1PX6ijijer062KN6zsSweAHLPyxLyqQ3y4w5okCmKz98ZKIovDZBP49S4y9qU4S9hgkfIP4SkiUk1efJrTnKKMEGnHTNkDadkbUo6p7dAmCQWANTFstdKf9KO67mH7IPGJVtWhjpVnnhYER6GPjhPXEZOspcoPqOF7IQbs+THfVuHE6L4v7bKE+pKkg9LzwPVHrWms0JEREREZC+OrNQMdlaIiIiIiKxgZ6VmcNCNiIiIiIicEkdWiIiIiIisUBQVFAePhDh6fzcjjqwQEREREZFT4sgKEREREZEVJqhgki7NZ8M+qWocWSEiIiIiIqfEkRUiIiIiIiu4GljNqDWdldyGCtQ62zJ3GT2N4v27+BhE8Uq6h7gMaaI5Q4DsOEzCeADIdhMmojLakSwvUpYsT9kfIIr3OSkKBwBoe8leq3AfeSLC9KO+onjXW2VlXCySvU4A4H9IFp/WTp4tT5uiEcVne8neSyo7Phdc8mWD0NoMeSFpvrIkgS5estc2MCRHFA8Aht9kiSfVshySAICCMNlxmIS5TEN3yrPdek5IEcWfUELEZaiyhAciPAz3y/KP96JQ2XUtO0r+mQFPWbJKU6HsdTIFyj6LAaBII/wcy9OJy3DNlJ2PoL9kJ/xib1E4AODAb81E8YYQeaJR73Tbj1tVLN491RK1prNCRERERGQvrgZWMzhnhYiIiIiInBJHVoiIiIiIrOCclZrBzgoRERERkRW8Daxm8DYwIiIiIiJyShxZISIiIiKyQqmG28A4smIdR1aIiIiIiMgpcWSFiIiIiMgKBYAiTx9mdZ9UtVrTWVHUVx620KbIX5ZSYVLIwnB5ciVNnmwgTF0gi1fsSNioESZxUqcKE6EBSIMwWV5T2f5L9fLEZhcLfETxRw9EiMvQ1Zedj9JMb1G8nx1JAgtCZYkkXfLFRaCkQZEoXq2WXeqNaVpRPAAY/WRtxKiTD1r7/C1LTJfTRFan9AwvUTwAeAhfqsIQ+cduqbcs+Z3neVnS0NQ28nNRkO8pitedkrepUnfhayW8PBc1lr2PAADZsutz8E75Z4bJRdbO8wbmiuL9vApE8QCQdSxUFF+gk3+OKV6ydp5bV9bO7crIeovstfX5n+wzBgCK/WyPNcpfVqolak1nhYiIiIjIXiaooJL+cmDDPqlqnLNCREREREROiSMrRERERERWMM9KzWBnhYiIiIjICpOigooZ7K873gZGREREREROiSMrRERERERWKEo1LF3MtYut4sgKERERERE5JXZWiIiIiIisKJtg7+iH1IIFCxAZGQmdToeOHTti165dlcYuXrwYXbt2hZ+fH/z8/NC7d+8q452RXbeBZWVlYdeuXUhJSYHJZJnoaNSoUQ6pGBERERER/eObb75BXFwcFi1ahI4dO2LevHmIjY3FsWPHEBwcXC5+y5YtGDFiBDp37gydTod33nkHffv2xaFDh1C3bt0aOAI5cWfl559/xsiRI5GXlwe9Xg+V6p8eoUqlctrOiqZYBbXKtt6rNkO+/9DALFF89vY64jIMskTuKKwry3Dtc0SYMRdAkSyhOYoCS8VluKTJ0toGtkkRxef/FiKKB4DTyZGieKWuQVyGUSsb+DRdchfFl1yWZekGgIIIWRZmXZp88PaRdttE8R//2lcU71Y/TxQPAIYiWRtUSuWpmHMjZa+tqkT2a5z6sjzLulu27GbqojaF4jL0OzxE8fl1ZHUK2SN7XQEgt5Ws3arlb2+URMiuha5pso9rl0vy860SvlRGeRFI6yzLtK5KEl6nGssz2LtIm628SUHxkH0eF9SRtcHm006J4gHg+Px6onifdPmBqwXXKaPB+SdvOMPSxXPmzMEjjzyCsWPHAgAWLVqEtWvXYsmSJZg6dWq5+C+//NLi359++im+//57bNq0yWm/s19N/E3imWeewbhx45CXl4esrCxkZmaaHxkZdnzLJyIiIiKqxXJyciwexcXF5WIMBgMSEhLQu3dv8za1Wo3evXsjPj7epnIKCgpQUlICf39/h9W9uok7KxcvXsTkyZPh4SH7NYyIiIiI6EZlUlTV8gCAiIgI+Pj4mB8zZ84sV35aWhqMRiNCQizvCAkJCUFSUpJNx/DCCy+gTp06Fh0eZye+DSw2NhZ79uxBw4YNq6M+REREREROpzqXLk5MTIRe/8/9/lqtHfdZWvH222/j66+/xpYtW6DT6Ry+/+oi7qwMGDAAzz33HA4fPoxWrVrB1dXy3uxBgwY5rHJERERERDc7vV5v0VmpSGBgIDQaDZKTky22JycnIzQ0tMrnzpo1C2+//TZ+++03tG7d+prrez2JOyuPPPIIAOD1118v9zeVSgWjUTaJjIiIiIjI2V0ZWXH0BHvbY93c3BAdHY1NmzZh8ODBAACTyYRNmzZh4sSJlT7v3XffxZtvvon169ejffv211jj6088Z8VkMlX6YEeFiIiIiKh6xMXFYfHixfj8889x5MgRTJgwAfn5+ebVwUaNGoVp06aZ49955x28/PLLWLJkCSIjI5GUlISkpCTk5clXxrTVqVOn8NJLL2HEiBFISbmyQuuvv/6KQ4cO2bU/JoUkIiIiIrLCGZJCDhs2DLNmzcL06dPRtm1b7N+/H+vWrTNPuj9//jwuX75sjl+4cCEMBgPuu+8+hIWFmR+zZs1y6GtTZuvWrWjVqhV27tyJH374wdwp+uuvv/DKK6/YtU+7kkJu3boVs2bNwpEjRwAALVq0wHPPPYeuXbvaVQkiIiIiIrJu4sSJld72tWXLFot/nz17tvor9C9Tp07FG2+8gbi4OHh7e5u39+zZEx9++KFd+xR3Vr744guMHTsW9957LyZPngwA2L59O3r16oVly5bhgQcesKsi1a5+PmBrUqYkL/HuMwpkCfn0SfJb5rJayZagCP5TluQxtXeRKB4AtKdlq0moi+SDea6Nc0XxmbmyZbWDT8kTVSYOEibwK5An3NRmyn5tyfOWtY+8prLkbADgeUqW7NAjWb5sykfbe4riQ25JFcU39k0TxQPAvjUtRPHaDPlxG3Wy851/myyTncmOFWyMl2XXNbdD8iXtg/6SHUd2C1kbLPSXv/fuiDghit+Q0EFcBjSyEyJN2GgIkH/GuGTLXquiAPm9+26XZefPNU9WRoq/MHsyABdv6zH/5n5Z/juvNlMWL50Wkdu9iewJAAJ900XxxT52pKwQfOQrGsfOBakOyv8/HL3Pm8mBAwewYsWKctuDg4ORlib//AXs6Ky8+eabePfddzFlyhTztsmTJ2POnDmYMWOG83ZWiIiIiIio2vj6+uLy5cto0KCBxfZ9+/ahbt26du1T/DP36dOnMXDgwHLbBw0ahDNnzthVCSIiIiIiZ+YMc1ac3fDhw/HCCy8gKSkJKpUKJpMJ27dvx7PPPotRo0bZtU9xZyUiIgKbNm0qt/23335DRESEXZUgIiIiInJqSjU9biJvvfUWoqKiEBERgby8PLRo0QLdunVD586d8dJLL9m1T/FtYM888wwmT56M/fv3o3PnzgCuzFlZtmwZ3n//fbsqQURERERENzY3NzcsXrwYL7/8Mg4ePIi8vDy0a9cOTZrI51WVEXdWJkyYgNDQUMyePRvffvstAKB58+b45ptvcPfdd9tdESIiIiIip1Udt23dZLeBlalXrx7q1avnkH3ZtXTxPffcg3vuucchFSAiIiIiohvfuHHjqvz7kiVLxPu0q7NCRERERFSbKMqVh6P3eTPJzLRcp7ukpAQHDx5EVlYWevaUpSYoY1Nnxd/fH8ePH0dgYCD8/PygUlU+ZJWRkWFXRYiIiIiI6Ma1atWqcttMJhMmTJiARo0a2bVPmzorc+fONWehnDt3bpWdFWelUl152EJjkHdzA7zzRPG5Oh9xGSqdLHlhXl3ZwJk6WSuKB4DiBrJEkp4HZEkkASDf300U73pedhyZTeTt2csvSxRfkCVPVFYUKGuHik6WBK5xoyRRPACc9QoQxXtskCUVBAAYZecjPUuWxDUlRf7ec3WXnQsXYYJHAAj9U5b8NKmH7Hpg+tNPFA8AOc1liUO9j8sS/gGAS5bsGuLqK2vn/ofliWj/TGpgPehfioLsSMCYJnutSuoVi+Ldj8uvtYYWBaJ4U7Y8SaAizNGZ30CYtLfAjhtGPIXv70L5+zsvXFaGa76sDJP8co7MfUGieHWgvAyvC7Yft8qO717XW3UsNXyzLV1cEbVajbi4OPTo0QPPP/+8+Pk2vatHjx5t/u8xY8aIC6nMzJkz8cMPP+Do0aNwd3dH586d8c4776BZs2bmmKKiIjzzzDP4+uuvUVxcjNjYWHz00UcICQlxWD2IiIiIiKh6nDp1CqWlwh8f/p/4JwiNRoPLly8jODjYYnt6ejqCg4NhNNr+C9PWrVvx5JNP4rbbbkNpaSlefPFF9O3bF4cPH4anpycAYMqUKVi7di2+++47+Pj4YOLEibj33nuxfft2adWJiIiIiOyjqBy/etdNNrISFxdn8W9FUXD58mWsXbvWYvBDQtxZUSqZCVRcXAw3N9ntOuvWrbP497JlyxAcHIyEhAR069YN2dnZ+Oyzz7BixQrzpJylS5eiefPm2LFjBzp16iStPhERERGRGCfYW7dv3z6Lf6vVagQFBWH27NlWVwqrjM2dlQ8++AAAoFKp8Omnn8LL65/7xI1GI7Zt24aoqCi7KlEmOzsbwJUJ/QCQkJCAkpIS9O7d2xwTFRWFevXqIT4+vsLOSnFxMYqL/7m3Nycn55rqRERERERE1v3+++8O36fNnZW5c+cCuDKysmjRImg0/8ySc3NzQ2RkJBYtWmR3RUwmE55++mncfvvtaNmyJQAgKSkJbm5u8PX1tYgNCQlBUlLFs8lmzpyJ1157ze56EBERERGVo/z/w9H7pCrZ3Fk5c+YMAOCOO+7ADz/8AD8/+coyVXnyySdx8OBB/PHHH9e0n2nTplncL5eTk4OIiIhrrR4REREREV2lXbt2Nq8UvHfvXvH+xXNWqmN4Z+LEiVizZg22bduG8PBw8/bQ0FAYDAZkZWVZjK4kJycjNDS0wn1ptVpotfIleImIiIiIKsOliys2ePDgat2/TZ2VuLg4zJgxA56enuVm+V9tzpw5NheuKAomTZqEVatWYcuWLWjQwHJt++joaLi6umLTpk0YMmQIAODYsWM4f/48YmJibC6HiIiIiIgc75VXXqnW/dvUWdm3bx9KSkrM/10ZabLIJ598EitWrMCPP/4Ib29v8zwUHx8fuLu7w8fHB+PHj0dcXBz8/f2h1+sxadIkxMTEiFcCK0n2gNrdtiRZBh95L/eSNOlfPXmiMpdLstXWFGER0iRUAGBIl9UJ8sOGOlVWRml9WZK5sK/kScT8B6aK4g8dliciNAQJ1yN3ESYdU8sT2ZmSZYnmigLs+MVIa5LFn5clnnQrltdJbZDFF9txl2xRiOw4vHVpovgMOwacVUWyDH4lsvycV8pIShfFmy40FMVnN5af76ca/yyKf/e3YeIyjMJLp1cr2WIx+d6y6yAAGJNkJ9A9RX6zfUF92XXn9tbHRfHxp2UJPQGg1E/WRrSZ8uSnvh1TRPHFPwZbD/qXEg/5h6v0s9LthDzJr1ue7W2ktOQGmbxxg1TzZmLTt7R/3/rlyNvAFi5cCADo0aOHxfalS5eak0/OnTsXarUaQ4YMsUgKSUREREREzsNoNGLu3Ln49ttvcf78eRgMlr/2ZWRkiPdpx+/clnJycrB69WocPXpU/FxFUSp8lHVUAECn02HBggXIyMhAfn4+fvjhh0rnqxARERERVYeyOSuOftxMXnvtNcyZMwfDhg1DdnY24uLicO+990KtVuPVV1+1a5/izsrQoUPx4YcfAgAKCwvRvn17DB06FK1atcL3339vVyWIiIiIiJyaUk2Pm8iXX36JxYsX45lnnoGLiwtGjBiBTz/9FNOnT8eOHTvs2qe4s7Jt2zZ07doVALBq1SooioKsrCx88MEHeOONN+yqBBERERER3diSkpLQqlUrAICXl5c54ftdd92FtWvX2rVPcWclOzvbnGF+3bp1GDJkCDw8PDBgwACcOHHCrkoQERERETk3VTU9bh7h4eG4fPkyAKBRo0bYsGEDAGD37t12pxYRd1YiIiIQHx+P/Px8rFu3Dn379gUAZGZmQqeTrRREREREREQ3h3vuuQebNm0CAEyaNAkvv/wymjRpglGjRmHcuHF27VO8ZuvTTz+NkSNHwsvLC/Xr1zev5LVt2zbzsA8RERER0U2lOuaY3CRzVj788EM8+OCDePvtt83bhg0bhnr16iE+Ph5NmjTBwIED7dq3uLPyxBNPoEOHDkhMTESfPn2gVl8ZnGnYsCHnrBARERER1TL/+c9/8Pzzz+Oee+7B+PHj0bNnTwBATEzMNSdyt2vp4vbt2+Oee+6Bp6cnFOVKl3DAgAG4/fbbr6kyREREREROiauBVSopKQmLFi3CpUuX0KdPHzRo0AAzZsxAYmLiNe9bnrobwH//+1+899575gn1TZs2xXPPPYeHHnromitUXdSBRVB72Bab7yl/WW4LSRLFn5RmvAfgLktYjczuhbInpMonPulPyfq7mmJ73pWyMvLdhKmhIcyYDkCnkWWXnzNsqbiMKd+NFcWX1i0WxZ/+X31RPADoU2QTAUN25orLuODiLYovblMgi8+Wtg+gTgPZm8/1gwBxGaltZVmxX2ywVRT/xg55lnWfyCxRvG6Hr7iMkqi6onjPxtmieMN5P1E8AJwrDhQ/R8pV+NZ4KHKnKH7u/l6yAgB4BOeL4ov9fcRluF+Ufb7+aWgmitemaUTxAFAUJrue5zcsEZdRuitEFK+ShaNVn2OyJwDIPB8he4IdH9/6w1k2x5YaZZ9h5Fzc3d0xatQojBo1CqdPn8ayZcvw2Wef4bXXXkPv3r0xfvx4DB48GK6uss86wI6RlTlz5mDChAno378/vv32W3z77bfo168fHn/8ccydO1dcASIiIiIip6eoqudxk2nYsCFef/11nDlzBr/++isCAgIwZswY1K0r+3GqjHgIYf78+Vi4cCFGjRpl3jZo0CDccsstePXVVzFlyhS7KkJERERE5KwU5crD0fu8WalUKri4uEClUkFRFJSUyEclATtGVi5fvozOnTuX2965c2fzuspERERERFT7JCYm4vXXX0fDhg3Rp08fXLp0CYsXL7a7nyDurDRu3Bjffvttue3ffPMNmjRpYlcliIiIiIicGifYV8pgMODrr79G37590aBBAyxevBgPPPAAjh8/js2bN2PkyJF252MU3wb22muvYdiwYdi2bZt59a/t27dj06ZNFXZiiIiIiIjo5hUaGoqCggLcdddd+PnnnxEbG2tOb3KtxJ2VIUOGYNeuXZgzZw5Wr14NAGjevDl27dqFdu3aOaRSREREREROpTomxN8kE+xfeuklPPTQQwgKCnL4vkWdlZycHOzcuRMGgwFz586tlgoREREREdGNIy4urtr2bXNnZf/+/ejfvz+Sk5OhKAq8vb3x7bffIjY2ttoqR0RERETkDFTKlYej90lVs7mz8sILL6BBgwb4/vvvodPpMGPGDEycONGcGNLZuR3xgEZr28SegibyxET7L8jWjtZ5yIf9cpvJlnzTnXQXxRsaC5NIAsgRDq7VXya/fzEpRJasUhMgO38FwbLXCQB6+MsScG3PbSouw6iVXcHCgrNE8cX/E2YdA6ApktWpxFuegNEkfErzurKErKe08oSNabtlr1UYZEnmAMCok722M/7qL4pv2P2sKB4A/LWyhJsnXOUJGIv9ZAnC8s4Irwdh8m8CLwbK3t9feMkTMNbfnCmK/+Fe2W3WQX7yhKzJJ2TJMCOOyNt5WmvZ3edRrWTZr8/8HimKBwCvM7I6GbzlbcpF+PFq8JGVceByHVkBAEqLZQk0tXbkbCysb3uS39ISV+CwvAy6+dn8Dk1ISMCGDRtw6623AgCWLFkCf39/5OTkQK+XZ2MnIiIiIrphVMfqXRxZscrmn7kzMjIQHh5u/revry88PT2Rnp5eLRUjIiIiInIazGBfI0Rjn4cPH0ZS0j+3XCiKgiNHjiA395/h5tatWzuudkREREREdEOobKK9SqWCTqdD48aNcffdd8Pf39/mfYo6K7169YKiWI5X3XXXXVCpVFAUBSqVCkajUbJLIiIiIiLnx9vArNq3bx/27t0Lo9GIZs2aAQCOHz8OjUaDqKgofPTRR3jmmWfwxx9/oEWLFjbt0+bOypkzZ+yrNRERERER3fTKRk2WLl1qntOenZ2Nhx9+GF26dMEjjzyCBx54AFOmTMH69ett2qfNnZX69evbV2siIiIiohsdR1aseu+997Bx40aLxbd8fHzw6quvom/fvnjqqacwffp09O3b1+Z9yteRJSIiIiIiukp2djZSUlLKbU9NTUVOTg6AK4t0GQwGm/fJzgoRERERkTVKNT1uInfffTfGjRuHVatW4cKFC7hw4QJWrVqF8ePHY/DgwQCAXbt2oWlT2/PPyTIh3cBMrXOh8rAtqaLmrJd8/3VlCwuUyIvA+E7/E8X/+kYPUfxlP1myNQBo0vKCKP5Su3riMkpb5onijemyJI8mWU46AMDbvw0UxT/WY7O4jNAdsvjsSGES0Iay/QNAgx/zRfGZzTzFZTTsJZsfd/Cg7BZVxY50wYHCKXvnBouLgKrIJIp/rLmsgXyxvI8oHgDSUmSvVaZtcyUt+B2VJaYzecoSETb7SL68ftqDsnZeWN/2XwjLJMf4iuJz9sniNUWicACAylN2vgsD5L93et6eKoo/ejhCFO8lO3UAgKL2sicZk+WJhD0uy5anVZXK4k05tidfLNOwl+zz+3yqLPk1ALjkC74blXKBppvBxx9/jClTpmD48OEoLb1yvXZxccHo0aMxd+5cAEBUVBQ+/fRTm/dZazorRERERER2q468KDdZnhUvLy8sXrwYc+fOxenTpwEADRs2hJfXP7/St23bVrRPdlaIiIiIiMhhvLy8HJZ70abOSrt27aBS2dbz27t37zVViIiIiIjI2aiUKw9H7/Nmkp+fj7fffhubNm1CSkoKTCbLW5zLRlskbOqslE2IAYCioiJ89NFHaNGiBWJiYgAAO3bswKFDh/DEE0+IK0BERERE5PS4dLFVDz/8MLZu3YqHHnoIYWFhNg92VMWmzsorr7xiUYnJkydjxowZ5WISExOvuUJERERERHTj+fXXX7F27VrcfvvtDtuneCmP7777DqNGjSq3/cEHH8T333/vkEoREREREdGNxc/PD/7+/g7dp7iz4u7uju3bt5fbvn37duh0OodUioiIiIiIbiwzZszA9OnTUVBQ4LB9ilcDe/rppzFhwgTs3bsXHTp0AADs3LkTS5Yswcsvv+ywihEREREROQsVqmGCvWN3V+Nmz56NU6dOISQkBJGRkXB1tUxmZ89CXOLOytSpU9GwYUO8//77+OKLLwAAzZs3x9KlSzF06FBxBa4XjUaBRmNb0jWlUL5/P79cUXxBloe4jEAXWRkZzWUDZ4rOtqSZ/5b+tSxhV14HeRltwpJF8YVBsiyPmvflyTCzGweL4sNcM8VlGDxll7DCAjdRvP6i/BKZ1FGWzdQkf2lhEq4537LlOVG8TiNvgwcvN5OVcUGeabTUS/YJuDlFVqeCUFnSSQAIOChL0pZ3l/ziWXpOlsxOHyxLElsarBfFA0CuSXYuVIWyxJYAUBQga+fesmYOyE83TG6yOvmekGdgzFgVKIr36J8jijde8BHFA0BJoez9GnBQfu3MaSxrU/qTsv0XBsnrdPpkqChe5SlvVBnNbf8QMBoUQJb7mpzQvxflchS78qwMHTrUqTsmREREREQO5SRJIRcsWID33nsPSUlJaNOmDebPn2++26ki3333HV5++WWcPXsWTZo0wTvvvIP+/ftfS60r9e9FuRxFPGeFiIiIiIiuv2+++QZxcXF45ZVXsHfvXrRp0waxsbFISUmpMP7PP//EiBEjMH78eOzbtw+DBw/G4MGDcfDgwetcc/vZ1Fnx9/dHWloagH9m+Vf2ICIiIiK66SjV9BCYM2cOHnnkEYwdOxYtWrTAokWL4OHhgSVLllQY//7776Nfv3547rnn0Lx5c8yYMQO33norPvzwQ1nBVajufoJNt4HNnTsX3t7e5v92RIIXIiIiIqIbRjUmhczJsZyfpdVqodVazvkxGAxISEjAtGnTzNvUajV69+6N+Pj4CncfHx+PuLg4i22xsbFYvXr1tdf9//27nzBv3jyH7beMTZ2V0aNHm/97zJgxDq8EEREREVFtFRFhuWDRK6+8gldffdViW1paGoxGI0JCQiy2h4SE4OjRoxXuNykpqcL4pKSka6/0//t3P+Hf/+0o4gn2v/zyCzQaDWJjYy22b9iwAUajEXfeeafDKkdERERE5AxUSjUsXfz/+0tMTIRe/8/qhVePqjizq0eFqvLvY7SVXUsXv/322+W2m0wmTJ06lZ0VIiIiIiIBvV5v9Yt8YGAgNBoNkpMtUzokJycjNLTipahDQ0NF8fbw9fW1eYqI0ShbEh+wYzWwEydOoEWLFuW2R0VF4eRJ4cLgREREREQ3ghqeYO/m5obo6Ghs2rTJvM1kMmHTpk2IiYmp8DkxMTEW8QCwcePGSuPt8fvvv2Pz5s3YvHkzlixZguDgYDz//PNYtWoVVq1aheeffx4hISGVLgJgjXhkxcfHB6dPn0ZkZKTF9pMnT8LT09OuShARERERUdXi4uIwevRotG/fHh06dMC8efOQn5+PsWPHAgBGjRqFunXrYubMmQCAp556Ct27d8fs2bMxYMAAfP3119izZw8++eQTh9Wpe/fu5v9+/fXXMWfOHIwYMcK8bdCgQWjVqhU++eQTu+a0iDsrd999N55++mmsWrUKjRo1AnClo/LMM89g0KBB4gpcL3r3Qrh42JZ9NUkvy6gMABkJsozm3vnymx7nrZS9vu5psv0XB8jT7hhdZSvDBW+T5yH92zVcFK8UybJJN9MXiOKvFCIL/yW9tbiI9Ftl2YLrh2SI4pMC64riAaAotFQUH/a7vE2lfV5fFJ/eRnYyBnbbI4oHgLOdUkXxpWtkWboBIM9dFp+UK7tOmbzlQ+/pt8gye2t2yuIBIHhPrij+dD1fUbzBV9ZmAeCj9K6i+M7Rx8RlHPyq/B0KVcnvkicr4Iz8x0OPy7Lr+aVu8jIMPsJM7mtl97dntJG3c8/jbqL4QtnHPQDA75DsuAtCZeci8KC8nbv0kk2wPpsUIC5DXaqzOVaRH8L1V42rgdlq2LBhSE1NxfTp05GUlIS2bdti3bp15kn058+fh1r9z2dv586dsWLFCrz00kt48cUX0aRJE6xevRotW7Z05FGYxcfHY9GiReW2t2/fHg8//LBd+xR/c3z33XfRr18/REVFITz8ypfICxcuoGvXrnjvvffsqgQREREREVk3ceJETJw4scK/bdmypdy2+++/H/fff3811+qKiIgILF68GO+++67F9k8//bTcime2sus2sD///BMbN27EX3/9BXd3d7Ru3RrdunWzqwJERERERM6uOlcDu1nMnTsXQ4YMwa+//oqOHTsCAHbt2oUTJ07g+++/t2uf8ntyAKhUKvTt2xd9+/YFACiKgl9//RWfffYZVq5caVdFiIiIiIiclqK68nD0Pm8i/fv3x/Hjx7Fw4UJz7peBAwfi8ccfv34jK/925swZLFmyBMuWLUNqaip69+59LbsjIiIiIqIbWEREBN566y2H7U/cWSkuLsbKlSvx2Wef4Y8//oDRaMSsWbMwfvx4uxK9EBERERE5PSeYYO+M/v77b5tjW7eWLzhkc2clISEBn332Gb766is0btwYDz30EL766iuEh4cjNjaWHRUiIiIiolqmbdu2UKlUUBTFIjmkolzpif17mz1JIW3urHTs2BGTJk3Cjh070KxZM3FBREREREQ3Kk6wr9iZM2fM/71v3z48++yzeO6558yJJ+Pj4zF79uxyK4TZyubOSq9evfDZZ58hJSUFDz30EGJjYy16SkREREREVLvUr/9PfrT7778fH3zwAfr372/e1rp1a0RERODll1/G4MGDxfu3ubOyfv16JCYmYunSpZgwYQIKCwsxbNgwALghOi3p2Z5Ql9iWnChgn/x4CsKEz1HkXeniCIMoPuCAbEpSVit5ndSlsucUBcpfW/djWlG8oZUsyWNaG3kSUGOo7FzsPNBYXIZLQLEoXi38eabES5Z0EgBUBtn5KwySJ4XMC5cdh6KRxR/IrCOKB4CMbFnyO1NL+WurPypLZmrI95PtX5Z7EQBQ4iWL17TPEpehvHdYFK96Vna/s/vXJaJ4APjp106i+PeGfi4u40RWc1F8bqmsfbhny6+16hLZe6kwWP6Z4RaRL4ovSZZdnzVF8muObw9ZcsSUfSHiMjQG2WuV10h2q0y9lcmieAA4elmW5FF90fYEj2U8k2w/jtIS+e1B1x3nrFh14MABNGjQoNz2Bg0a4PBh2fW+jOhdHRERgenTp+PMmTNYvnw5UlNT4eLigrvvvhsvvvgi9u7da1cliIiIiIjoxta8eXPMnDkTBsM/P+oaDAbMnDkTzZvLfqQpY/fSxX369EGfPn2QmZmJL774AkuWLME777xj18QZIiIiIiKnVg1zVm62kZVFixZh4MCBCA8PN6/89ffff0OlUuHnn3+2a5/XlGcFAPz8/DBp0iRMmjSJIytEREREdHPibWBWdejQAadPn8aXX35pTgo5bNgwPPDAA/D0lN1SXeaaOyv/duuttzpyd0REREREdAPx9PTEo48+6rD9yWeiERERERHVNko1PW4yy5cvR5cuXVCnTh2cO3cOADB37lz8+OOPdu2PnRUiIiIiIrpmCxcuRFxcHO68805kZmaa57L7+flh3rx5du2TnRUiIiIiIivKkkI6+nEzmT9/PhYvXoz//Oc/cHH5Z7ZJ+/btceDAAbv2afecldTUVBw7dgwA0KxZMwQFBdm7KyIiIiIiusGdOXMG7dq1K7ddq9UiP1+WZ6mMuLOSn5+PSZMmYfny5eahHY1Gg1GjRmH+/Pnw8PCwqyLV7ZY6SXD1dLMpNud4XfH+U7vYtu8yBh95P9HjuCw5Ykp7WXf9zuh9ongAOPFFlCj+9L3ypFK+R2XJzQryXUXxRnd58rQlXZeK4sf+Pk5chuqcuyg+8bys3Sp1i0TxAOC7XXb+VCb5T0aeF2XnozhAliwvOUeeBNRPL0s0atoiO3eAHUnjYgpF8SqtPDmiao+PKD7vsjCLJICiuzqI4kuSZe3jcnd5nSYOXiuK/29SZ3EZmbfI4ns2OS6KPxwoT1xYukL2HL8j8mtnvvD9J008qU2X16m4VPZ5bNTJr2t54bIbWTzOy+JzW8vPt6lElrzWFCi/hhjdbL8+G2+ABONkXYMGDbB//36LrPYAsG7dOrvzrIhvA4uLi8PWrVvx008/ISsrC1lZWfjxxx+xdetWPPPMM3ZVgoiIiIiIbmxxcXF48skn8c0330BRFOzatQtvvvkmpk2bhueff96ufYp/3v/++++xcuVK9OjRw7ytf//+cHd3x9ChQ7Fw4UK7KkJERERE5LSYZ8Wqhx9+GO7u7njppZdQUFCABx54AHXq1MH777+P4cOH27VPcWeloKAAISHlhxuDg4NRUCC7VYKIiIiI6EZQHRPib7YJ9gAwcuRIjBw5EgUFBcjLy0NwcPA17U98G1hMTAxeeeUVFBX9c797YWEhXnvtNcTExFxTZYiIiIiI6MaWkpKChIQEHDt2DKmpqde0L/HIyvvvv4/Y2FiEh4ejTZs2AIC//voLOp0O69evv6bKEBERERE5rZtwJMSRcnNz8cQTT+Crr76CyXRlEQeNRoNhw4ZhwYIF8PGRLdwC2DGy0rJlS5w4cQIzZ85E27Zt0bZtW7z99ts4ceIEbrlFuLwJERERERHdFB5++GHs3LkTa9euNS/EtWbNGuzZswePPfaYXfu0K8+Kh4cHHnnkEbsKJCIiIiK64XCCvVVr1qzB+vXr0aVLF/O22NhYLF68GP369bNrn3Z1Vk6cOIHff/8dKSkp5iGeMtOnT7erIkREREREdOMKCAio8FYvHx8f+Pn52bVPcWdl8eLFmDBhAgIDAxEaGgrVv5L4qFQqdlaIiIiI6KbD1cCse+mllxAXF4fly5cjNDQUAJCUlITnnnsOL7/8sl37FHdW3njjDbz55pt44YUX7CqwphxPC4amwLYM8IXD5NmnIcwEq820I/NvPaMoXlUqK+NYtnxpOcVD1oTcssTTpOA6OEX2hAuynrumUH6l+M+JwaJ436A8cRklJ/1F8fmRpaL4wM2ybPQAUBAqa1Mudqxm7pYjzOTeqch60L+0C7kkigeAvFLbrh1lzusCxWWUesjiPXfKnqDumSErAEBWI4Mo3i3JVVxGektZm/I5Ktt/cYAsHgA+OtxNFG884SUuwzVPdtyb41uJ4n0byc93RndZhnKNu+yaAwDGHDdRvFuq7RnQAUBTLAoHAOTHy96vGi/5Z4bPadnnd/Jtss/KtNbyG2W8Dsvi81vJrrUAkNHc9vNtLJada3Ie7dq1sxi8OHHiBOrVq4d69eoBAM6fPw+tVovU1FS75q2IW3dmZibuv/9+cUFERERERDcszlmp0ODBg6t1/+LOyv33348NGzbg8ccfr476EBERERE5Hd4GVrFXXnmlWvcv7qw0btwYL7/8Mnbs2IFWrVrB1dVy2H/y5MkOqxwREREREd148vLyyi3EpdfrxfsRd1Y++eQTeHl5YevWrdi6davF31QqFTsrRERERHTz4W1gVp05cwYTJ07Eli1bUFT0zzwnRVGgUqlgNMrmbwF2dFbOnDkjLoSIiIiIiG5uDz74IBRFwZIlSxASEmIx8d5eduVZISIiIiKqVTiyYtVff/2FhIQENGvWzGH7tKmzEhcXhxkzZsDT0xNxcXFVxs6ZM8chFSMiIiIiohvHbbfdhsTExOvfWdm3bx9KSkrM/10ZRwz1EBERERE5G64GZt2nn36Kxx9/HBcvXkTLli3LLcTVunVr8T5t6qz8/vvvFf73jaSkWAOjxra73lxy5Z0u//bpovjUgiBxGRBWSxUsS+B0+fdwWQEA3BvI3mWu+eIikLFf9lp5t8gSxRt8ZUkkAcDFJEvYFeglP/CzfrJ6ze21QhT/4sVRongAQJscUbhpp3zVj2I/WUP33SJLbrk/uLkoHgAKI2TJ79y9xUVAf7ss+WluoSxRpeGYvJ0HHZLFZ8pfWoSvzxbFX5wuu+YYD/qI4gEAx2RJHj2S5Z8Z7umyRMIFkbJJqcYN8sSkvsIvTQWh8iSg2nzZa6WW5alEXn3Z6woAXudk13OjPJ8uLvaWxQf/KTsZufXkbbA4UFaG2znZNQcAXNpn2hyrKrAjoyc5ndTUVJw6dQpjx441b1OpVNc0wV6eTtyBFi5ciNatW0Ov10Ov1yMmJga//vqr+e9FRUV48sknERAQAC8vLwwZMgTJyck1WGMiIiIiqpWUanrcRMaNG4d27dohPj4ep0+fxpkzZyz+3x7iCfZFRUWYP38+fv/9d6SkpJRbP3nv3r027ys8PBxvv/02mjRpAkVR8Pnnn+Puu+/Gvn37cMstt2DKlClYu3YtvvvuO/j4+GDixIm49957sX37dmm1iYiIiIjsxwn2Vp07dw4//fQTGjdu7LB9ijsr48ePx4YNG3DfffehQ4cO1zRPZeDAgRb/fvPNN7Fw4ULs2LED4eHh+Oyzz7BixQr07NkTALB06VI0b94cO3bsQKdOnewul4iIiIiIHKtnz57466+/arazsmbNGvzyyy+4/fbbHVYJADAajfjuu++Qn5+PmJgYJCQkoKSkBL17/3OjZ1RUFOrVq4f4+PhKOyvFxcUoLv7nvsecHNk99kREREREV+MEe+sGDhyIKVOm4MCBA2jVqlW5CfaDBg0S71PcWalbty68ve2YPVqJAwcOICYmBkVFRfDy8sKqVavQokUL7N+/H25ubvD19bWIDwkJQVJSUqX7mzlzJl577TWH1Y+IiIiIiKx7/PHHAQCvv/56ub9dtwn2s2fPxgsvvIBz586JC6tIs2bNsH//fuzcuRMTJkzA6NGjcfjwYbv3N23aNGRnZ5sfiYmJDqknEREREdVinGBvlclkqvRhT0cFsGNkpX379igqKkLDhg3h4eFRbngnIyNDtD83NzfzfW3R0dHYvXs33n//fQwbNgwGgwFZWVkWoyvJyckIDQ2tdH9arRZarXx5PSIiIiIici7izsqIESNw8eJFvPXWWwgJCXF4IkiTyYTi4mJER0fD1dUVmzZtwpAhQwAAx44dw/nz5xETE+PQMomIiIiIqsI5K5Xr378/vvrqK/j4XMlt9fbbb+Pxxx83Dzikp6eja9eudt09Je6s/Pnnn4iPj0ebNm3EhV1t2rRpuPPOO1GvXj3k5uZixYoV2LJlC9avXw8fHx+MHz8ecXFx8Pf3h16vx6RJkxATE2PXSmBNwlLh6ulmU+zJQw3F+8/ZFiKKNzYyiMtwTZUl4HILk2XTKgiTD88FHpK9y5LracRlaIplHeLcdE9RvI/8VIgTVbbvY/uS3mVOu9YVxU/5fYQoXuMjT56GM7L5aq4e8iJ0t8pGZz10skRi6aftSMiqlb03DH7yFFZpmbLXVnNWlpmuNFiW2BIAdFmy957RU/7jVVGYrJHkX5C9tmG3yXNzXToXIIovjZS/tsXH3UXxrvoCUbzJVZ6wsTBUdj2P2CC/eGZGCZOZCqfIelySv/dKZB8ZMATacSuLm+x6W+Rv2/eVMvYkqjS6yc53SYgwQycAU5Ht7dBUZN8tQuQc1q9fb7HA1VtvvYWhQ4eaOyulpaU4duyYXfsWd1aioqJQWFhoV2FXS0lJwahRo3D58mX4+PigdevWWL9+Pfr06QMAmDt3LtRqNYYMGYLi4mLExsbio48+ckjZREREREQ2Y56VSimKUuW/r4W4s/L222/jmWeewZtvvlnhkmR6vd7mfX322WdV/l2n02HBggVYsGCBtJpERERERI7DzkqNEHdW+vXrBwDo1auXxXZFUexekoyIiIiIiG5MKpWq3Dx2R81rF3dWfv/9d4cUTERERER0o1D9/8PR+7wZKIqCMWPGmFfkLSoqwuOPPw5PzyuTwv49n0VK3Fnp3r17pX87ePCg3RUhIiIiIqIbz+jRoy3+/eCDD5aLGTVqlF37FndWrpabm4uvvvoKn376KRISEngbGBERERHdfDhnpVJLly6ttn3L1/j7f9u2bcPo0aMRFhaGWbNmoWfPntixY4cj60ZERERERLWYaGQlKSkJy5Ytw2effYacnBwMHToUxcXFWL16NVq0aFFddSQiIiIiqlFMClkzbO6sDBw4ENu2bcOAAQMwb9489OvXDxqNBosWLarO+jnM2Q2R0Ghty5pUVFd+K5v7ZVmyQ/czsoRPAFAcIEsqZfzLRxTvdkuuKB4ASjxk2bQCDsjflXnDs0TxxTmyZGslXqJwAIBHkmxK3J+XI8VlBO6TlZHRUnZXZ6le3s7rNEgTxWduDxWXkXfUTxSfq5G1KReTfDqjUfgclR1l6LfKsrpltZAdtzpXnpD1cmfZcYT9T/7+NnjL6qV4yhIwXrrgL4oHAJcs4XvJKD/fpR6y16o0X/aZoZWfbuibp4viz4UJsykCUDJkn2MhO2X7zxki/xwzFMsSaPr+aUe2W2EO3pymsif4HZK3Qfd0WRvMHiVLTAoABSd8bY41FdnRaKlWsPmK/Ouvv2Ly5MmYMGECmjRpUp11IiIiIiJyLpyzUiNsnrPyxx9/IDc3F9HR0ejYsSM+/PBDpKXJfmUlIiIiIrphKQ5+kFU2d1Y6deqExYsX4/Lly3jsscfw9ddfo06dOjCZTNi4cSNyc+VDr0RERERE5HgZGRkYOXIk9Ho9fH19MX78eOTl5VUZP2nSJDRr1gzu7u6oV68eJk+ejOzs7OtY6/LEq4F5enpi3Lhx+OOPP3DgwAE888wzePvttxEcHIxBgwZVRx2JiIiIiGpU2QR7Rz+qy8iRI3Ho0CFs3LgRa9aswbZt2/Doo49WGn/p0iVcunQJs2bNwsGDB7Fs2TKsW7cO48ePr75K2sDupYsBoFmzZnj33Xdx4cIFfPXVV46qExERERER2enIkSNYt24dPv30U3Ts2BFdunTB/Pnz8fXXX+PSpUsVPqdly5b4/vvvMXDgQDRq1Ag9e/bEm2++iZ9//hmlpbLFTRzpmjorZTQaDQYPHoyffvrJEbsjIiIiInIujp6v8q95Kzk5ORaP4uLia6pqfHw8fH190b59e/O23r17Q61WY+dO25fZy87Ohl6vh4vLNeeRt5tDOitERERERGSfiIgI+Pj4mB8zZ868pv0lJSUhODjYYpuLiwv8/f2RlJRk0z7S0tIwY8aMKm8dux5qrptERERERHSDqM6kkImJidDr9ebtWq22wvipU6finXfeqXKfR44cueZ65eTkYMCAAWjRogVeffXVa97ftWBnhYiIiIioBun1eovOSmWeeeYZjBkzpsqYhg0bIjQ0FCkpKRbbS0tLkZGRgdDQqhM25+bmol+/fvD29saqVavg6ipLnOpotaazUuqlwKSzrTusKZTfHWfwkXW1jaEGcRkuSbIMxgZfWQZcU17Fvfiq+P2VKYq/0C9AXIZG+jNGpux18j0lTC0MILWdLFuwl7gEIL21LF4JLRLFex50lxUAIC09RBRf6id/bd2yZe+/4kBZGRPu2CiKB4BP1vYVxWuK5Nmks7oViuJNhbLLt9ZH1j4AQFFkx3HZU96mwn+Tvb/dT8ne366Vr9JZqZyWJbInlMrPd+hOWbu9ECt7Xxj08p9/SxMCRfGuLeQpC7T1Ze0wrchXFO+2x0cUDwBoKXvv5deRv7ZGd9lzPCNlS8UaG8jrlHzIT1bGaflr63NasH/516LrzwmSQgYFBSEoKMhqXExMDLKyspCQkIDo6GgAwObNm2EymdCxY8dKn5eTk4PY2FhotVr89NNP0Ol0sgpWA85ZISIiIiK6iTRv3hz9+vXDI488gl27dmH79u2YOHEihg8fjjp16gAALl68iKioKOzatQvAlY5K3759kZ+fj88++ww5OTlISkpCUlISjEZjjR1LrRlZISIiIiKyV3XOWakOX375JSZOnIhevXpBrVZjyJAh+OCDD8x/LykpwbFjx1BQUAAA2Lt3r3mlsMaNG1vs68yZM4iMjKy+ylaBnRUiIiIiImuc4DYwCX9/f6xYsaLSv0dGRkJR/qlAjx49LP7tLHgbGBEREREROSWOrBARERERWXODjazcLDiyQkRERERETokjK0REREREVtxoE+xvFhxZISIiIiIip1RrRlYMYSVQu2tsCy6RJ/jSeMuSiDUIzhCXcSY/TBSvS7LxeP9fkVreHFTZsoxrBh9/cRnKQVniKkQUi8JL3OXJMKWyTwqPAYDJU7amudt5WeKm/CZ2ZOASJgl0TZO3qdJmBaJ4l7OyRISL18gSPAJAqa/sXKgU2XsPADz+lh2HsUOOKL60RF4nt/2eonhFmBwXAFKECVZL9LJkikYP+fVcl1j92Zov3y47Dun1XCPLcwgAcLs9XRRfsF9+Pc8X5g1V3GRtytBKfuBe8R6ieHt+Cc9uLjvfRYWy5KeaI7L3KgDcc8+fovhVv8aIy8jsYPvnjKnQAHwqLuL64pyVGsGRFSIiIiIickq1ZmSFiIiIiMheKkWBysF5SBy9v5sROytERERERNbwNrAawdvAiIiIiIjIKXFkhYiIiIjICi5dXDM4skJERERERE6JIytERERERNZwzkqN4MgKERERERE5pVozsuJ20RUanW2JvoR57wAApYWyhF2ni4LFZbjkyvqWkT3PiuKP76kvigeAcyNlz9Hcki0uoyBZluzKJUmW5DG7qSgcAKBqmC+K17nIkgoCgG6DXhSf3V2WDE1zSZZEEgAgfG/Y817SnJRljXNrnSWKV22VJ+gEZO9vbZr8wAvbyc6fKU/WzpVi+W9TWlkeO6jkzRzaTFm813lhYtL7UmQFAEi6LGsjrh7yBKumZFkiQk2RbP9eF4UnD0DGYVmSR6WxPAFj3a9kyQ7zwmTvvQy9/GuNQXaptWuOgSJ8UkmW7P1t8pGf7+/+6Ch7Qrgs4TIA+O6y/XPGaFBwQVzC9cU5KzWDIytEREREROSUas3IChERERGR3ThnpUaws0JEREREZAVvA6sZvA2MiIiIiIicEkdWiIiIiIis4W1gNYIjK0RERERE5JQ4skJEREREZAPOMbn+OLJCREREREROqfaMrKhsT1CnbZUl3r0h1Uv2hBJ50jjFRdadP7s5UhQf3vWSKB4AVHOCRPHJrj7iMgI7pYni05JlGb7C18qSjgHAhQBZQsVu7Q6Jy9jSqLUoXqstEcUXeMmSswGAe2CBKF6zU5htDUBxgKyda4UJN9PbyBOb+QXkiuKDp8l/BzrWUpb89KF2O0TxXx7sIIoHgPxWstdWd0KeaNQk/BTKaCtLfue6R56A11+YmS6ne6m4DF2qrI3kR8rKUJfIP949m2eI4pVNsiSSAHCxm+z9rYTKsmG6usozkw4b+qcoftn2LuIyNIWy8x3R+rIo/tyxUFE8ALRudVYU/9fxeuIyslra3m5NhfL30XWnKFcejt4nVYkjK0RERERE5JRqz8gKEREREZGdmGelZrCzQkRERERkDZcurhG8DYyIiIiIiJwSR1aIiIiIiKxQma48HL1PqhpHVoiIiIiIyClxZIWIiIiIyBrOWakRHFkhIiIiIiKnxJEVIiIiIiIruHRxzag9nZUmeVB52JYd1bjTT7x7nTAZuMGn+mdUeXVKFcWfOyvLRg8AdfSy7O9FjeXZwwsv+Iri9cdkzfpid/m5cEuTDUrqXQrFZRh1snoZjcKBUq0807PWVZZhOC9QfhU2hhhE8RlZsszvXr4FongAyEjVi+IN/YQXBABKkey1/erXbqJ4lT3j6MGyc6G7LV1cROGeAFG84iJrUx7JKlE8AGRFyd57Wjd55m3v87LjKGoiK0NRyz/esy7L2rm6gfzaqWhkx92oTpooPmVNhCgeAJZlyTLSu2XIPvcAwBAou96euxgoiu986zFRPADsXdtCFO+XIb+euwgut0aDBhfEJVBtUHs6K0RERERE9lKUKw9H75OqxM4KEREREZEVvA2sZnCCPREREREROSWOrBARERERWcOli2sER1aIiIiIiMgpcWSFiIiIiMgKzlmpGRxZISIiIiIip8SRFSIiIiIia7h0cY2oNZ0VD10JNDrbBpLyPeQNxxAuS56msSMhn+a0uyg+e68sqZSrPM8VSjxk8ep0V3EZ7pdlA4AlXrL9u+bKk8YZ6pSI4jd+2UlchrtOFq+66C2Lb58rKwBAzmlfUbxdQ7fCMXGNi+y9lJspbLQA3PVFsvg0eTt3y5G9AbOiZK+T13n52cgNFsYf9ReXodbKjkMXKEuwWtRdFA4A8Ngpey/lecquzQBQ1EGYePKU7ILgmif/HFMXytqIyUv+OeZ1XPbeOGOoK4pX6tuT5Ff23jMEyZOAQpjM1Nc/TxS//e+mongAcHOX1Smzjfx8Q5AE1FRYCqyQF0E3v1rTWSEiIiIishfnrNQMdlaIiIiIiKzh0sU1ghPsiYiIiIjIKXFkhYiIiIjICt4GVjM4skJERERERE6JIytERERERNaYlCsPR++TqsSRFSIiIiIickocWSEiIiIisoargdWIWtNZcV3lB42bbUm1VI3lSQJVObJEV67pWnEZBj9ZsqvQ5imi+LTdIaJ4APBIlSXHyiqRNznXfNk7uVB4GG5Z8vPdo8UxUfwWNBOX4XnMTRRf2FKWLE9rx6w+k4csKZimSH6+PQ/Kkt+ZOmWL4pXznqJ4AChUZG3EU96koB6ULntCpuw4SlPliQs1SbI26J4qP/BSYY5Og/D8mYJkCXsBoLierJ17BeWLy8hLlmWvLdHLrv9uufIbJ9yyZeev2E+WHBcAIGwiulTZcWi7pMkKAJB5zk8Urz8qT/qa00rWDguKZO89e2ZpS5NbqrTyhJsqjeA5JjuSbVKtUGs6K0RERERE9lKhGlYDc+zubkrsrBARERERWaMoVx6O3idViRPsiYiIiIjIKbGzQkRERERkRVlSSEc/qktGRgZGjhwJvV4PX19fjB8/Hnl5eTY9V1EU3HnnnVCpVFi9enX1VdIG7KwQEREREd1kRo4ciUOHDmHjxo1Ys2YNtm3bhkcffdSm586bNw8qlXPMqOGcFSIiIiIia26gpYuPHDmCdevWYffu3Wjfvj0AYP78+ejfvz9mzZqFOnXqVPrc/fv3Y/bs2dizZw/CwsKqp4ICHFkhIiIiIqpBOTk5Fo/i4uJr2l98fDx8fX3NHRUA6N27N9RqNXbu3Fnp8woKCvDAAw9gwYIFCA0NvaY6OAo7K0REREREVqgUpVoeABAREQEfHx/zY+bMmddU16SkJAQHB1tsc3Fxgb+/P5KSkip93pQpU9C5c2fcfffd11S+I/E2sAoY3eVjcl7nZf2+gtDqX6ouKdVHFO/bVpiUDkB6TqAoXlUiP+6crrJkhyZhwk23HI0oHgC2b2kpivdsmiMuo6CerF5anSyhlusfelE8AJhkudOgseOHobzGskRzMaGXRPEntPJEdtkHA0TxBWHy+3w1v8veSzFDDovij25pLooHAJeOWaL4fI2vuAzFpXqvha7n7UjAK0yWF6rPFZeRs0p2fc5uLNu/wVsWDwCaIlm860Vh4kIAxg6ya2FhmixrqK+rPLFg15gEUfy2Q7eJy4ioK/t8vbxP9ou2qzxfI3yPy+JTb5e/ti0bXLQ5tiTfgPPiEm4eiYmJ0Ov/+VzWaiu+dk2dOhXvvPNOlfs6cuSIXXX46aefsHnzZuzbt8+u51cXpxlZefvtt6FSqfD000+btxUVFeHJJ59EQEAAvLy8MGTIECQnJ9dcJYmIiIiodjJV0wOAXq+3eFTWWXnmmWdw5MiRKh8NGzZEaGgoUlJSLJ5bWlqKjIyMSm/v2rx5M06dOgVfX1+4uLjAxeXKmMaQIUPQo0cPe14xh3CKkZXdu3fj448/RuvWrS22T5kyBWvXrsV3330HHx8fTJw4Effeey+2b99eQzUlIiIiotro37dtOXKfEkFBQQgKCrIaFxMTg6ysLCQkJCA6OhrAlc6IyWRCx44dK3zO1KlT8fDDD1tsa9WqFebOnYuBAweK6ulINT6ykpeXh5EjR2Lx4sXw8/vnHpPs7Gx89tlnmDNnDnr27Ino6GgsXboUf/75J3bs2FGDNSYiIiIicl7NmzdHv3798Mgjj2DXrl3Yvn07Jk6ciOHDh5tXArt48SKioqKwa9cuAEBoaChatmxp8QCAevXqoUGDBjV2LDXeWXnyyScxYMAA9O7d22J7QkICSkpKLLZHRUWhXr16iI+Pr3R/xcXF5VZUICIiIiK6Jko1ParJl19+iaioKPTq1Qv9+/dHly5d8Mknn5j/XlJSgmPHjqGgoKD6KuEANXob2Ndff429e/di9+7d5f6WlJQENzc3+Pr6WmwPCQmpchWDmTNn4rXXXnN0VYmIiIiIbhj+/v5YsWJFpX+PjIyEYuU2NGt/vx5qbGQlMTERTz31FL788kvodDqH7XfatGnIzs42PxITEx22byIiIiKqpRSleh5UpRrrrCQkJCAlJQW33nqrecWBrVu34oMPPoCLiwtCQkJgMBiQlZVl8bzk5OQqk9RotdpyKyoQEREREdGNp8ZuA+vVqxcOHDhgsW3s2LGIiorCCy+8gIiICLi6umLTpk0YMmQIAODYsWM4f/48YmJiaqLKRERERFRLqZQrD0fvk6pWY50Vb29v8yoDZTw9PREQEGDePn78eMTFxcHf3x96vR6TJk1CTEwMOnXqVBNVJiIiIiKi68gp8qxUZu7cuVCr1RgyZAiKi4sRGxuLjz76yK59pbdRoNbZ1n11ybcj+7QwU7drZJ64jOIUWSZf13PCuUC++bJ4AAVhsrS56lBhimQAxmJZJnfXPNndjSZ5EmYYw2XHUZAjn5cVsl3WDpO6uIviSzvIV/8ozRG+WPakVRa+/XbsaSaKb9pKPo+tOErWBtWb/awHXSU/XPbz2p97Zcet3F4iigcAz3jZcXjIE1yj2F8Wb6hjkMW7y84dAKh0RlH82YRwcRm6UFlDDzgoey8ly5Oso8hTdtw+R+RfIQwFwluzI2SNKnedLPM7APzYPED2hNvk76X8PbJ6qYTN1qSV/zyf1l74HDsu5ymLI22ONRrk3w+uu+qYY8I5K1Y5VWdly5YtFv/W6XRYsGABFixYUDMVIiIiIiKiGuNUnRUiIiIiImekMl15OHqfVDV2VoiIiIiIrOFtYDWixjPYExERERERVYQjK0RERERE1ij//3D0PqlKHFkhIiIiIiKnxJEVIiIiIiIrVIoClYPnmDh6fzcjjqwQEREREZFTqjUjK5piFdQq25JwKa7y/efVk/WMTRc9xWWoAmSZJ1XpskSExdsDRfEA4OIpO+56IeniMqQJ10rrCl+nJHnCRiVTlhzRRZioEgAKQmRJ41xyZfGlrvKGrrsku2SYXOW/GHm0zhTFZ5V4i+LP/lFPFA8ApY0LRfHediRHdGuUI4pX9vuI4t2T5W0ws6UsSaB7kjwBozTRnI2X8X/iC+THrcnQiuI9LssTCefXlb03tOmyMow+8kbokiF7f+fVl6+5Km2H6kJZfG5DWZsFAFfhcfseFReB1BhhvdSy9qEqlrfzJs0viuIv/Ca/dqa1tb2NmIoAfCMu4vriamA1giMrRERERETklGrNyAoRERERkd0UiEeDbdonVYmdFSIiIiIiKzjBvmbwNjAiIiIiInJKHFkhIiIiIrJGQTVMsHfs7m5GHFkhIiIiIiKnxJEVIiIiIiJruHRxjeDIChEREREROaVaM7KiyVdBU2pbUi2jh7yXW+ohW8tOI0x0BQClebIkfsVBsiRUxS7y4/bfI2tC59P8xGW4ZcmSoRVrZAkbi/1F4QAAla9BFF+ikSdgVJXKEuwpEbLEhY3tSNCZHOQlis9NkcUDQEmWhyheVSR7nUobFYniAUCnKxHFa3Pka1tmpcoSxeqjZckzM9LsSEQrvCYoqfKkkCV6WRnu3rLzZzorS/AIAPW6nhfFn02TX0Tc9sveG9lNZK+TJlv+8V4aIruuQfjeA4DCUFm8yUP2OSa9HgDyRMLqQ7LPGADw/Vt2PrJuk9UpZIv8uE94h4jilXryRKPugkTCRjsSW153JgDyHLDW90lVugFaBhERERER1Ua1ZmSFiIiIiMhezLNSM9hZISIiIiKyhhPsawRvAyMiIiIiIqfEkRUiIiIiIms4slIjOLJCREREREROiSMrRERERETWcGSlRnBkhYiIiIiInFKtGVkxeipQdLb1Xks95Rl6fI/I+n2ZbWSJrgDA1VeWDM09XpZ0LKeNMCEYABdZHkLguDwxnfq2LFG8MctdFu8hT6blopH9EmLSyduUukT29lS7ytrUyeNhongA8Ptb9lpp5UXALVuYQFP4o1RJuqx9AEBBsCwJXEmUPGuYS5bstTWFyMpQucrbYGBgrig+LVeeHDHsf7LjSInWi+Jd7Ei4diopSBTftdFJcRl7ElqJ4o1hsiSBUORt0FWY/DRwnTzZbWq0rF7B24XXwVL5r9SZg2XHnd1QJy7D77isIeYI36+p7ew434my65ohQP69JfBv2xNJlpaUQv5Ous6YFLJGcGSFiIiIiIicUq0ZWSEiIiIisheTQtYMdlaIiIiIiKzhBPsawdvAiIiIiIjIKXFkhYiIiIjIGpMCqBw8EmLiyIo1HFkhIiIiIiKnxJEVIiIiIiJrOGelRnBkhYiIiIiInBJHVoiIiIiIrKqGkRVpZuNaqPZ0VlSwOeuoulg+4JTTUBbv5ifLRg8AJWnyzNsSKrX8DZNXV5bK1efWVHEZaWneonj9X1pRfGGw/LhL1LLMzR5n5ZmeS3xk9Sq95CmKdymWp+HNqyeLLwmWZYYGgIjbk2VlGGWZ3y/tCRPFA4BHnTxRvPqMj7gMg/B8Fx+TlWHPxb7giCxTtzpInoq50F/WDhUX4etkR52UPNmrte9LWTZ6ADB5yeJVGtlxaxJl10EAcE+SPSezmbgIeJ+WHUdBqKx95DWQZ1n3+112MrKbydtUXl3Z9wqXc7L3nr5NuigeAIq2BYriS/zknxkFQbYft9Egu5ZT7VF7OitERERERPbinJUawc4KEREREZE1JgUOv22LSxdbxQn2RERERETklDiyQkRERERkjWK68nD0PqlKHFkhIiIiIiKnxJEVIiIiIiJrOMG+RnBkhYiIiIiInBJHVoiIiIiIrOFqYDWi1nRWNHkqaEpsS2hUFCpPKuVxUZbMyBAuT66kPyYro7hrriheSbUj6aTwMBr6yhNXZf0tS1yVV082Wc3kIZ/c1q75WVH8Yb9QcRmlRbJEkn5+ssSFOYcDRPEAENA2RVbGthBxGSchf46E/zn5czJCZe8N1yD5h4+xrixRrCpZlsDPFGQQxQNASYDsmuNxxo7kp7KcrzAJk0IqWvn7W+1eKoovDJV/lLrkyS6eplzZa6t4yo+7QJi4sFQv/6yst65QFJ/WTpg90w6FIcJzoZUft8ooe22NOlk7T0vWi+IBwF2YN9R/n/xmnOwmtsea5LmyqZaoNZ0VIiIiIiK7cc5KjWBnhYiIiIjIGgXV0Flx7O5uRpxgT0RERERETokjK0RERERE1vA2sBrBkRUiIiIiInJK7KwQEREREVljMlXPo5pkZGRg5MiR0Ov18PX1xfjx45GXZ33l0Pj4ePTs2ROenp7Q6/Xo1q0bCgtlK/k5EjsrREREREQ3mZEjR+LQoUPYuHEj1qxZg23btuHRRx+t8jnx8fHo168f+vbti127dmH37t2YOHEi1Oqa6zJwzgoRERERkTU30JyVI0eOYN26ddi9ezfat28PAJg/fz769++PWbNmoU6dOhU+b8qUKZg8eTKmTp1q3tasWbNqqaOtak1nxTUP0JTYGHtalggNAHKibNz5/3M76SEuQ9dPlpCvOF+WyE6bKm8O7qmyN9m5D5uKy9A0lSXsKgmSnQuv426ieAA4eUKQ6QqAvmequIys07JkaE/c9oso/kd9W1E8ABiMsvdGth0/xPgFyZKZqlSyNmjwliUZBYCIusJkpj8FictwzZW9WDlTMkTx9iSN0yXKEhEWB8hvZ3AVNhK3bNn1IOAPeSMseFB2u0Oei05chlqYV9A1U/beKwmUJbYEAG24LLGsMVGY0RPAiYmyNuV2Rna+PUPyRfEAoD7hI3uCIv+sLAyWXaekSVx9d8jbYFZraRuRH3dpSLHNsaZCeeLam0lOTo7Fv7VaLbRaYebOf4mPj4evr6+5owIAvXv3hlqtxs6dO3HPPfeUe05KSgp27tyJkSNHonPnzjh16hSioqLw5ptvokuXLnbX5VrxNjAiIiIiImvKRlYc/QAQEREBHx8f82PmzJnXVNWkpCQEBwdbbHNxcYG/vz+SkpIqfM7p06cBAK+++ioeeeQRrFu3Drfeeit69eqFEydOXFN9rkWtGVkhIiIiIrKbSYHDsziaruwvMTERev0/I+CVjapMnToV77zzTpW7PHLkiH1V+f/J/o899hjGjh0LAGjXrh02bdqEJUuWXHMHyl7srBARERER1SC9Xm/RWanMM888gzFjxlQZ07BhQ4SGhiIlxXL6QGlpKTIyMhAaGlrh88LCwgAALVq0sNjevHlznD9/3mrdqgs7K0REREREViiKCYri2KWGpfsLCgpCUJD1eZExMTHIyspCQkICoqOjAQCbN2+GyWRCx44dK3xOZGQk6tSpg2PHjllsP378OO68805RPR2Jc1aIiIiIiG4izZs3R79+/fDII49g165d2L59OyZOnIjhw4ebVwK7ePEioqKisGvXLgCASqXCc889hw8++AArV67EyZMn8fLLL+Po0aMYP358jR0LR1aIiIiIiKxRFPMcE4fus5p8+eWXmDhxInr16gW1Wo0hQ4bggw8+MP+9pKQEx44dQ0FBgXnb008/jaKiIkyZMgUZGRlo06YNNm7ciEaNGlVbPa1hZ4WIiIiI6Cbj7++PFStWVPr3yMhIKBV0lqZOnWqRZ6WmsbNCRERERGSNUg2rgVXjyMrNotZ0VgqiC6H2sK1BuJySJ1dSFcum/xgbyZKOAUDyJV9RvMZdlnXMM0sUDgDIi5DFK80KrAddpSRJlkDT529ZkkejHTmXCkNkF5eCowHiMhSdrIz3j90his9Nkid0012WXTKK6soT0xkLZSekRwPZ2u//08qTQl7eV/HKKZXxDhMXAYzLFoXnHPWX7T9AliwVAMK3yq5TpwfL30w+p2STS41usiSBl3rKvwjcFiBL4rpP5ScuI6+R7HyoSmWfMS5Z8o93o5+sDO/T8mmvuU1kSSFNbsJkignCBI8ASv6vvXsPiuq8+wD+3eW2XJYVSGUXdYGIUWwDVRG6oY6JEiWNGY1ptRMTsSGXVvBSp9Y6rZrGRExqTGubGhIbyHTEGJJo1GpSkxLiNSIXJYrrDUd9BWnGcA3LZc/z/pGX87qiwiPIWdnvZ2Zn2HMezvM757d78OdzznlC5fpoGyB/g3XgeclJPUPk8qdLlZy4FkBgu1wffiXyE8v6Xen+OcHZKnBRugfyBB5TrBARERER3TJFAXS9+zQw9PLTxfojFitERERERF3hZWCa4KOLiYiIiIjILXFkhYiIiIioC0JRIHr5MrDenmSyP+LIChERERERuSWOrBARERERdYX3rGiCIytEREREROSWOLJCRERERNQVRQA6jqz0NY6sEBERERGRW/KYkRX9/xigN3RvZvqWwfIzPXv5yc0Wr7+Fytz3nNzM7M1DW6TaNwyV2wcACLbLzcpbN1huHwBgxNtyM3ufXiI3i7azTb5mFy1y+20c2CjdR9NZuZmYG8/LzS48PblIqj0AfFiUINXeu1buOAFAuyNAqr0SJZc/H/lU4O4Hz0m1/6bYKt1HVW2gVHvvQXKzyzu/lZs5HADOPir3XYr4Qv68VhclObO35CTaeof893ty2DGp9kfa7pHuQx/QLtXecNxfqr1jpNznAwAcDXL59uren1QXg4fVSLWvLjVLtW++u1WqPQDovOWexiQUnXQfjcPk+vC7JPl9PRoq1x6AY5zkOcQsv9/eEl045f9c9D0hAPT2pJAcWekKR1aIiIiIiMgteczIChERERHRrRKKgOjle1YER1a6xGKFiIiIiKgrQkHvXwbGSSG7wsvAiIiIiIjILXFkhYiIiIioC7wMTBscWSEiIiIiIrfEkRUiIiIioq7wnhVN9PtipWN4TXE4uv07SrP8PCs6p+QcJXr5D6dTcm4PpVlunhVds/zHQT4m+Wfgtzvl9kP5Vm5IVWm//fOsOL+V2wdA7jMLAMJLbr9bGuU/50qzXEyKQ/7B+Yoitx+tjXKfKWeL3D4AQFuTXB/tbfJ9yJ53lDa5c47SLD+PEhxy54T2NvnLGWTPIU7JQ6v4yZ9rmxvl5kCR/a4CgPKt3O84W+TmuJDdPgAo7ZK5aJE/tu1NkudzyWN7S3+/+2CeFUj+nXE65L6vzlb57578Z1D+b6VOIt0d52Z3viyqHW1AL4fXDvnPrKfRCXf+VPSCixcvYsiQIVqHQURERERduHDhAgYPHqx1GC4cDgeio6NRXV19W7ZvNptRWVkJQzcnL/c0/b5YURQFly5dgtFohE73//8bUl9fjyFDhuDChQsIDpacEpnuOMy3Z2G+PQvz7VmY7/5JCIGGhgZERERAr3e/W6odDgdaW+WvDukOX19fFio30e8vA9Pr9Tet0IODg3my8yDMt2dhvj0L8+1ZmO/+x2QyaR3CDRkMBhYUGnG/0pWIiIiIiAgsVoiIiIiIyE15bLHi5+eHFStWwM/PT+tQqA8w356F+fYszLdnYb6JPEu/v8GeiIiIiIjuTB47skJERERERO6NxQoREREREbklFitEREREROSWWKwQEREREZFb8thi5fXXX0dUVBQMBgOSkpJw6NAhrUOiXvDFF1/gkUceQUREBHQ6HbZu3eqyXgiB5cuXw2KxwN/fHykpKTh16pQ2wVKPZGVlYezYsTAajRg4cCCmTZsGu93u0sbhcCAjIwNhYWEICgrCY489hsuXL2sUMfXE+vXrERcXp04EaLPZsGvXLnU9c92/rV69GjqdDgsXLlSXMedEnsEji5XNmzdj0aJFWLFiBUpKShAfH4/JkyejpqZG69Coh5qamhAfH4/XX3/9uutfeeUVrFu3Dm+88Qa+/PJLBAYGYvLkyXA4HH0cKfVUYWEhMjIycPDgQezevRttbW2YNGkSmpqa1Da//vWvsX37duTn56OwsBCXLl3C9OnTNYyabtXgwYOxevVqFBcX4/Dhw5gwYQKmTp2KY8eOAWCu+7OioiJkZ2cjLi7OZTlzTuQhhAdKTEwUGRkZ6nun0ykiIiJEVlaWhlFRbwMgtmzZor5XFEWYzWbxpz/9SV1WW1sr/Pz8xKZNmzSIkHpTTU2NACAKCwuFEN/l1sfHR+Tn56ttKioqBABx4MABrcKkXhQSEiI2bNjAXPdjDQ0NYtiwYWL37t1i/PjxYsGCBUIIfr+JPInHjay0traiuLgYKSkp6jK9Xo+UlBQcOHBAw8jodqusrER1dbVL7k0mE5KSkpj7fqCurg4AEBoaCgAoLi5GW1ubS75HjBgBq9XKfN/hnE4n3n33XTQ1NcFmszHX/VhGRgYefvhhl9wC/H4TeRJvrQPoa19//TWcTifCw8NdloeHh+PEiRMaRUV9obq6GgCum/uOdXRnUhQFCxcuRHJyMn7wgx8A+C7fvr6+GDBggEtb5vvOVV5eDpvNBofDgaCgIGzZsgUjR45EWVkZc90PvfvuuygpKUFRUVGndfx+E3kOjytWiKj/ycjIwFdffYW9e/dqHQrdRsOHD0dZWRnq6urw/vvvIy0tDYWFhVqHRbfBhQsXsGDBAuzevRsGg0HrcIhIQx53Gdhdd90FLy+vTk8MuXz5Msxms0ZRUV/oyC9z379kZmZix44dKCgowODBg9XlZrMZra2tqK2tdWnPfN+5fH19ERMTgzFjxiArKwvx8fH4y1/+wlz3Q8XFxaipqcHo0aPh7e0Nb29vFBYWYt26dfD29kZ4eDhzTuQhPK5Y8fX1xZgxY/DZZ5+pyxRFwWeffQabzaZhZHS7RUdHw2w2u+S+vr4eX375JXN/BxJCIDMzE1u2bMF//vMfREdHu6wfM2YMfHx8XPJtt9tx/vx55rufUBQFLS0tzHU/NHHiRJSXl6OsrEx9JSQkYNasWerPzDmRZ/DIy8AWLVqEtLQ0JCQkIDExEX/+85/R1NSEX/ziF1qHRj3U2NiI06dPq+8rKytRVlaG0NBQWK1WLFy4EC+++CKGDRuG6OhoLFu2DBEREZg2bZp2QdMtycjIQF5eHj766CMYjUb1OnWTyQR/f3+YTCakp6dj0aJFCA0NRXBwMObNmwebzYYf/ehHGkdPspYuXYqHHnoIVqsVDQ0NyMvLw+eff45PPvmEue6HjEajev9Zh8DAQISFhanLmXMiD6H148i08te//lVYrVbh6+srEhMTxcGDB7UOiXpBQUGBANDplZaWJoT47vHFy5YtE+Hh4cLPz09MnDhR2O12bYOmW3K9PAMQOTk5apvm5mYxd+5cERISIgICAsSjjz4qqqqqtAuabtlTTz0lIiMjha+vr/je974nJk6cKP7973+r65nr/u/qRxcLwZwTeQqdEEJoVCcRERERERHdkMfds0JERERERHcGFitEREREROSWWKwQEREREZFbYrFCRERERERuicUKERERERG5JRYrRERERETkllisEBERERGRW2KxQkREREREbonFChH1S88//zx++MMf9vp2z507B51Oh7Kyshu2+fzzz6HT6VBbWwsAyM3NxYABA3o9lp64//77sXDhQq3D6JJOp8PWrVu1DoOIiDTCYoWINDVnzhzodLpOr9TUVK1D6zUzZ87EyZMnb3s/ubm56vHz8vJCSEgIkpKS8MILL6Curs6l7YcffoiVK1fe9ph6qqqqCg899JDWYRARkUa8tQ6AiCg1NRU5OTkuy/z8/DSKpvf5+/vD39+/T/oKDg6G3W6HEAK1tbXYv38/srKykJOTg3379iEiIgIAEBoa2ifx9JTZbNY6BCIi0hBHVohIc35+fjCbzS6vkJAQdb1Op0N2djamTJmCgIAAxMbG4sCBAzh9+jTuv/9+BAYG4r777sOZM2c6bTs7OxtDhgxBQEAAZsyY0WmEYcOGDYiNjYXBYMCIESPw97//3WX9oUOHMGrUKBgMBiQkJKC0tLRTHzt37sQ999wDf39/PPDAAzh37pzL+msvA+u4RO2f//wnoqKiYDKZ8POf/xwNDQ1qm4aGBsyaNQuBgYGwWCx47bXXunXplk6ng9lshsViQWxsLNLT07F//340Njbit7/9rdru2m1FRUXhxRdfxOzZsxEUFITIyEhs27YN//3vfzF16lQEBQUhLi4Ohw8fdulv7969GDduHPz9/TFkyBDMnz8fTU1NLttdtWoVnnrqKRiNRlitVrz55pvq+tbWVmRmZsJiscBgMCAyMhJZWVku+3P1ZWDl5eWYMGEC/P39ERYWhmeffRaNjY3q+jlz5mDatGlYs2YNLBYLwsLCkJGRgba2tpseNyIick8sVojojrBy5UrMnj0bZWVlGDFiBB5//HE899xzWLp0KQ4fPgwhBDIzM11+5/Tp03jvvfewfft2fPzxxygtLcXcuXPV9Rs3bsTy5cvx0ksvoaKiAqtWrcKyZcvwzjvvAAAaGxsxZcoUjBw5EsXFxXj++efxm9/8xqWPCxcuYPr06XjkkUdQVlaGp59+Gr/73e+63J8zZ85g69at2LFjB3bs2IHCwkKsXr1aXb9o0SLs27cP27Ztw+7du7Fnzx6UlJTc0rEbOHAgZs2ahW3btsHpdN6w3WuvvYbk5GSUlpbi4YcfxpNPPonZs2fjiSeeQElJCYYOHYrZs2dDCKHuQ2pqKh577DEcPXoUmzdvxt69ezvl4dVXX1ULvblz5+JXv/oV7HY7AGDdunXYtm0b3nvvPdjtdmzcuBFRUVHXja+pqQmTJ09GSEgIioqKkJ+fj08//bRTfwUFBThz5gwKCgrwzjvvIDc3F7m5ubd07IiISGOCiEhDaWlpwsvLSwQGBrq8XnrpJbUNAPGHP/xBfX/gwAEBQPzjH/9Ql23atEkYDAb1/YoVK4SXl5e4ePGiumzXrl1Cr9eLqqoqIYQQQ4cOFXl5eS7xrFy5UthsNiGEENnZ2SIsLEw0Nzer69evXy8AiNLSUiGEEEuXLhUjR4502caSJUsEAPHNN98IIYTIyckRJpPJJbaAgABRX1+vLlu8eLFISkoSQghRX18vfHx8RH5+vrq+trZWBAQEiAULFtzwWF7bz9U64r58+bIQQojx48e7bCsyMlI88cQT6vuqqioBQCxbtkxd1nHcO45fenq6ePbZZ1362bNnj9Dr9eoxu3a7iqKIgQMHivXr1wshhJg3b56YMGGCUBTlunEDEFu2bBFCCPHmm2+KkJAQ0djYqK7/17/+JfR6vaiurhZCfPd5ioyMFO3t7Wqbn/3sZ2LmzJnX3T4REbk33rNCRJp74IEHsH79epdl195TERcXp/4cHh4OALj33ntdljkcDtTX1yM4OBgAYLVaMWjQILWNzWaDoiiw2+0wGo04c+YM0tPT8cwzz6ht2tvbYTKZAAAVFRWIi4uDwWBw2cbVKioqkJSU5LLs2jbXExUVBaPRqL63WCyoqakBAJw9exZtbW1ITExU15tMJgwfPrzL7d6I+L/REJ1Od8M23TnGAFBTUwOz2YwjR47g6NGj2Lhxo0s/iqKgsrISsbGxnbbbcZlax77OmTMHDz74IIYPH47U1FRMmTIFkyZNum58FRUViI+PR2BgoLosOTlZzWlHfN///vfh5eWltrFYLCgvL7/Z4SEiIjfFYoWINBcYGIiYmJibtvHx8VF/7vgH9/WWKYrSrT477nN46623OhUbV/9D93a5Onbgu/i7G/utqKioQHBwMMLCwroVU3eOcWNjI5577jnMnz+/07asVut1t9uxnY5tjB49GpWVldi1axc+/fRTzJgxAykpKXj//fdld7Fb/RER0Z2F96wQUb91/vx5XLp0SX1/8OBB6PV6DB8+HOHh4YiIiMDZs2cRExPj8oqOjgYAxMbG4ujRo3A4HC7buFpsbCwOHTrksuzaNrLuvvtu+Pj4oKioSF1WV1d3y48/rqmpQV5eHqZNmwa9vvdO+6NHj8bx48c7Hb+YmBj4+vp2ezvBwcGYOXMm3nrrLWzevBkffPABrly50qldbGwsjhw54nID/759+9ScEhFR/8NihYg019LSgurqapfX119/3ePtGgwGpKWl4ciRI9izZw/mz5+PGTNmqI/D/eMf/4isrCysW7cOJ0+eRHl5OXJycrB27VoAwOOPPw6dTodnnnkGx48fx86dO7FmzRqXPn75y1/i1KlTWLx4Mex2O/Ly8np8M7fRaERaWhoWL16MgoICHDt2DOnp6dDr9Te9jAv47jKs6upqVFVVoaKiAm+//Tbuu+8+mEwmlxv4e8OSJUuwf/9+ZGZmoqysDKdOncJHH33U6Yb3m1m7di02bdqEEydO4OTJk8jPz4fZbL7uJJqzZs1Sc/rVV1+hoKAA8+bNw5NPPqleAkZERP0LixUi0tzHH38Mi8Xi8vrxj3/c4+3GxMRg+vTp+MlPfoJJkyYhLi7O5dHETz/9NDZs2ICcnBzce++9GD9+PHJzc9WRlaCgIGzfvh3l5eUYNWoUfv/73+Pll1926cNqteKDDz7A1q1bER8fjzfeeAOrVq3qcexr166FzWbDlClTkJKSguTkZPURyzdTX18Pi8WCQYMGwWazITs7G2lpaSgtLYXFYulxXFeLi4tDYWEhTp48iXHjxmHUqFFYvny5OpdLdxiNRrzyyitISEjA2LFjce7cOezcufO6I0ABAQH45JNPcOXKFYwdOxY//elPMXHiRPztb3/rzd0iIiI3ohMdd10SEZHbampqwqBBg/Dqq68iPT1d63CIiIj6BG+wJyJyQ6WlpThx4gQSExNRV1eHF154AQAwdepUjSMjIiLqOyxWiIjc1Jo1a2C32+Hr64sxY8Zgz549uOuuu7QOi4iIqM/wMjAiIiIiInJLvMGeiIiIiIjcEosVIiIiIiJySyxWiIiIiIjILbFYISIiIiIit8RihYiIiIiI3BKLFSIiIiIickssVoiIiIiIyC2xWCEiIiIiIrf0v/NtpPy7KCkTAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean embedding across positions: [ 0.01184019 -0.00657512  0.00154995  0.00653086 -0.07230945]... (showing first 5 values)\n",
      "Std dev across positions: [0.14652799 0.18079527 0.17563711 0.17511298 0.18494053]... (showing first 5 values)\n",
      "\n",
      "Found 18 proteins with sequences longer than 1000 amino acids\n",
      "Min sequence length: 24\n",
      "Max sequence length: 1340\n",
      "Mean sequence length: 289.86\n"
     ]
    }
   ],
   "source": [
    "# Load the protein-to-full-embedding mapping\n",
    "with open('processed_data/protein_to_fullembedding_map.pkl', 'rb') as f:\n",
    "    protein_to_fullembedding_map = pickle.load(f)\n",
    "\n",
    "# Load the full embeddings data\n",
    "full_embeddings_data = np.load('processed_data/protein_full_embeddings_all.npz', allow_pickle=True)\n",
    "all_embeddings = full_embeddings_data['embeddings'].item()  # Convert to dictionary\n",
    "all_ids = full_embeddings_data['ids']\n",
    "\n",
    "print(f\"Loaded {len(protein_to_fullembedding_map)} protein mappings\")\n",
    "print(f\"Loaded {len(all_embeddings)} full sequence embeddings\")\n",
    "\n",
    "# Example: Find which protein corresponds to a specific ID\n",
    "target_id = 5\n",
    "for protein_seq, seq_id in protein_to_fullembedding_map.items():\n",
    "    if seq_id == target_id:\n",
    "        print(f\"ID {target_id} corresponds to protein: {protein_seq[:50]}...\")  # Show first 50 characters\n",
    "        # Show the embedding shape for this protein\n",
    "        embedding = all_embeddings[target_id]\n",
    "        print(f\"Embedding shape: {embedding.shape} (sequence length × embedding dimension)\")\n",
    "        break\n",
    "\n",
    "# Example: Get the full embedding for a specific protein sequence\n",
    "def get_full_embedding_for_protein(protein_sequence, embedding_map, all_embeddings_dict):\n",
    "    if protein_sequence in embedding_map:\n",
    "        seq_id = embedding_map[protein_sequence]\n",
    "        return all_embeddings_dict[seq_id]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Load the original DataFrame to see some protein sequences\n",
    "df = pd.read_parquet('data/binding_data_200k.parquet')\n",
    "sample_protein = df['seq'].iloc[0]\n",
    "\n",
    "# Get full embedding for this protein\n",
    "sample_embedding = get_full_embedding_for_protein(sample_protein, protein_to_fullembedding_map, all_embeddings)\n",
    "if sample_embedding is not None:\n",
    "    print(f\"\\nFound full embedding for sample protein with shape: {sample_embedding.shape}\")\n",
    "    print(f\"This means we have embeddings for all {sample_embedding.shape[0]} amino acids in the protein\")\n",
    "    \n",
    "    # Visualize embedding dimensions\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(sample_embedding[:50, :50], aspect='auto', cmap='viridis')\n",
    "    plt.title('Visualization of first 50 amino acids × 50 embedding dimensions')\n",
    "    plt.xlabel('Embedding Dimension')\n",
    "    plt.ylabel('Amino Acid Position')\n",
    "    plt.colorbar(label='Embedding Value')\n",
    "    plt.savefig('sample_protein_embedding_visualization.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Show mean and standard deviation across positions\n",
    "    pos_mean = np.mean(sample_embedding, axis=0)\n",
    "    pos_std = np.std(sample_embedding, axis=0)\n",
    "    print(f\"Mean embedding across positions: {pos_mean[:5]}... (showing first 5 values)\")\n",
    "    print(f\"Std dev across positions: {pos_std[:5]}... (showing first 5 values)\")\n",
    "else:\n",
    "    print(\"Could not find embedding for the sample protein\")\n",
    "\n",
    "# Count how many proteins have embeddings for sequences longer than 1000 amino acids\n",
    "long_proteins = 0\n",
    "lengths = []\n",
    "for seq_id, embedding in all_embeddings.items():\n",
    "    length = embedding.shape[0]\n",
    "    lengths.append(length)\n",
    "    if length > 1000:\n",
    "        long_proteins += 1\n",
    "\n",
    "print(f\"\\nFound {long_proteins} proteins with sequences longer than 1000 amino acids\")\n",
    "print(f\"Min sequence length: {min(lengths)}\")\n",
    "print(f\"Max sequence length: {max(lengths)}\")\n",
    "print(f\"Mean sequence length: {np.mean(lengths):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85bb9be-8770-474b-b79c-4aab3b971040",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Ligand Preprocessing using RDKIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdb6a238-7d0f-46c4-aac3-eface5e04877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory for multi-scale ligand data\n",
    "os.makedirs('processed_data/ligands_multiscale_etkdg', exist_ok=True)\n",
    "os.makedirs('processed_data/ligands_multiscale_etkdg/batch_results', exist_ok=True)\n",
    "\n",
    "# Reuse existing functional groups definition\n",
    "FUNCTIONAL_GROUPS = {\n",
    "    'alcohol': '[OH]',\n",
    "    'aldehyde': '[CX3H1](=O)[#6]',\n",
    "    'ketone': '[#6][CX3](=O)[#6]',\n",
    "    'carboxylic_acid': '[CX3](=O)[OX2H1]',\n",
    "    'ester': '[#6][CX3](=O)[OX2][#6]',\n",
    "    'amide': '[NX3][CX3](=[OX1])[#6]',\n",
    "    'amine': '[NX3;H2,H1,H0;!$(NC=O)]',\n",
    "    'nitrile': '[NX1]#[CX2]',\n",
    "    'nitro': '[$([NX3](=O)=O),$([NX3+](=O)[O-])]',\n",
    "    'sulfonic_acid': '[$([#16X4](=[OX1])(=[OX1])([#6])[OX2H,OX1H0-]),$([#16X4+2]([OX1-])([OX1-])([#6])[OX2H,OX1H0-])]',\n",
    "    'sulfonamide': '[$([#16X4]([NX3])(=[OX1])(=[OX1])[#6]),$([#16X4+2]([NX3])([OX1-])([OX1-])[#6])]',\n",
    "    'phosphate': '[#15,#16]=[O]',\n",
    "    'halogen': '[F,Cl,Br,I]',\n",
    "    'aromatic': 'a',\n",
    "    'heterocycle': '[a;!c]'\n",
    "}\n",
    "\n",
    "# Define constants for physics-based features\n",
    "# Average bond lengths (in Angstroms) by bond type\n",
    "BOND_LENGTHS = {\n",
    "    Chem.BondType.SINGLE: 1.5,  # Average C-C single bond length\n",
    "    Chem.BondType.DOUBLE: 1.33, # Average C=C double bond length\n",
    "    Chem.BondType.TRIPLE: 1.18, # Average C≡C triple bond length\n",
    "    Chem.BondType.AROMATIC: 1.4, # Average aromatic bond length\n",
    "}\n",
    "\n",
    "def get_scaffold_hierarchy(mol):\n",
    "    \"\"\"\n",
    "    Generate a hierarchy of scaffolds from a molecule (multi-scale level 1)\n",
    "    \"\"\"\n",
    "    if mol is None:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Level 1: Basic Murcko scaffold (rings and linkers only)\n",
    "        basic_scaffold = MurckoScaffold.GetScaffoldForMol(mol)\n",
    "        basic_scaffold_smiles = Chem.MolToSmiles(basic_scaffold) if basic_scaffold else \"\"\n",
    "        \n",
    "        # Level 2: Scaffold with attachment points\n",
    "        # Use BRICS fragmentation to identify potential attachment points\n",
    "        fragments = list(BRICS.BRICSDecompose(mol))\n",
    "        attachment_points = set()\n",
    "        \n",
    "        # Get atoms that would be attachment points (where fragments connect)\n",
    "        for frag in fragments:\n",
    "            for match in re.finditer(r'\\[\\d+\\]', frag):\n",
    "                attachment_points.add(match.group())\n",
    "        \n",
    "        # Level 3: Complete molecule\n",
    "        complete_mol_smiles = Chem.MolToSmiles(mol)\n",
    "        \n",
    "        return {\n",
    "            'L1_basic_scaffold': basic_scaffold_smiles,\n",
    "            'L2_attachment_points': list(attachment_points),\n",
    "            'L3_complete_molecule': complete_mol_smiles\n",
    "        }\n",
    "    except:\n",
    "        return {\n",
    "            'L1_basic_scaffold': \"\",\n",
    "            'L2_attachment_points': [],\n",
    "            'L3_complete_molecule': Chem.MolToSmiles(mol) if mol else \"\"\n",
    "        }\n",
    "\n",
    "def calc_physics_features(mol):\n",
    "    \"\"\"\n",
    "    Calculate physics-based features using ETKDGv3 for more robust 3D structure generation\n",
    "    \"\"\"\n",
    "    if mol is None:\n",
    "        return {}\n",
    "    \n",
    "    physics_features = {}\n",
    "    \n",
    "    try:\n",
    "        # Make a copy of the molecule and add hydrogens\n",
    "        mol_h = Chem.AddHs(mol)\n",
    "        \n",
    "        # Check if molecule contains problematic atoms (metals, etc.)\n",
    "        has_problematic_atoms = False\n",
    "        for atom in mol_h.GetAtoms():\n",
    "            if atom.GetAtomicNum() > 18 and atom.GetAtomicNum() not in [35, 53]:  # Exclude Br, I\n",
    "                has_problematic_atoms = True\n",
    "                break\n",
    "        \n",
    "        # Generate 3D coordinates using ETKDGv3\n",
    "        params = AllChem.ETKDGv3()\n",
    "        params.randomSeed = 42\n",
    "        params.useSmallRingTorsions = True  # Better handling of small rings\n",
    "        params.useBasicKnowledge = True\n",
    "        params.enforceChirality = True\n",
    "        params.maxIterations = 1000  # More iterations for complex structures\n",
    "        \n",
    "        # Skip force field for problematic molecules\n",
    "        if has_problematic_atoms:\n",
    "            AllChem.EmbedMolecule(mol_h, params)\n",
    "            physics_features['force_field_type'] = \"skipped_problematic_atoms\"\n",
    "            physics_features['has_problematic_atoms'] = True\n",
    "        else:\n",
    "            # Try embedding with ETKDGv3\n",
    "            conf_id = AllChem.EmbedMolecule(mol_h, params)\n",
    "            \n",
    "            if conf_id >= 0:  # Successful embedding\n",
    "                # Try force field optimization - MMFF94 first, UFF as fallback\n",
    "                try:\n",
    "                    # Try MMFF\n",
    "                    results = AllChem.MMFFOptimizeMolecule(mol_h, maxIters=200, \n",
    "                                                          confId=0, ignoreInterfragInteractions=False)\n",
    "                    if results == 0:  # Successful optimization\n",
    "                        ff_type = \"MMFF\"\n",
    "                        # Get energy\n",
    "                        mp = AllChem.MMFFGetMoleculeProperties(mol_h)\n",
    "                        ff = AllChem.MMFFGetMoleculeForceField(mol_h, mp)\n",
    "                        if ff:\n",
    "                            physics_features['total_energy'] = ff.CalcEnergy()\n",
    "                    else:\n",
    "                        raise Exception(\"MMFF optimization failed\")\n",
    "                except:\n",
    "                    try:\n",
    "                        # Fall back to UFF\n",
    "                        AllChem.UFFOptimizeMolecule(mol_h, maxIters=200, confId=0)\n",
    "                        ff_type = \"UFF\"\n",
    "                        # Get energy\n",
    "                        ff = AllChem.UFFGetMoleculeForceField(mol_h)\n",
    "                        if ff:\n",
    "                            physics_features['total_energy'] = ff.CalcEnergy()\n",
    "                    except:\n",
    "                        ff_type = \"failed\"\n",
    "                        physics_features['total_energy'] = 0.0\n",
    "                \n",
    "                physics_features['force_field_type'] = ff_type\n",
    "            else:\n",
    "                # If embedding fails, use a simpler approach\n",
    "                physics_features['force_field_type'] = \"embedding_failed\"\n",
    "        \n",
    "        # Get ring strain information - this doesn't depend on 3D structure\n",
    "        ring_info = mol.GetRingInfo()\n",
    "        ring_sizes = [len(ring) for ring in ring_info.AtomRings()]\n",
    "        \n",
    "        # Calculate strain based on ring size\n",
    "        ring_strains = []\n",
    "        for size in ring_sizes:\n",
    "            if size == 3:\n",
    "                strain = 5.0  # High strain for 3-membered rings\n",
    "            elif size == 4:\n",
    "                strain = 3.0  # High strain for 4-membered rings\n",
    "            elif size == 5 or size == 6:\n",
    "                strain = 0.1  # Low strain for 5/6-membered rings\n",
    "            else:\n",
    "                strain = 1.0  # Medium strain for other ring sizes\n",
    "            \n",
    "            ring_strains.append(strain)\n",
    "        \n",
    "        if ring_strains:\n",
    "            physics_features['avg_ring_strain'] = sum(ring_strains) / len(ring_strains)\n",
    "            physics_features['total_ring_strain'] = sum(ring_strains)\n",
    "            physics_features['ring_count'] = len(ring_strains)\n",
    "            physics_features['ring_sizes'] = ring_sizes\n",
    "        else:\n",
    "            physics_features['avg_ring_strain'] = 0.0\n",
    "            physics_features['total_ring_strain'] = 0.0\n",
    "            physics_features['ring_count'] = 0\n",
    "            physics_features['ring_sizes'] = []\n",
    "        \n",
    "    except Exception as e:\n",
    "        # If anything fails, provide default values\n",
    "        physics_features = {\n",
    "            'force_field_type': 'exception',\n",
    "            'error_message': str(e),\n",
    "            'total_energy': 0.0,\n",
    "            'avg_ring_strain': 0.0,\n",
    "            'total_ring_strain': 0.0,\n",
    "            'ring_count': mol.GetRingInfo().NumRings() if mol else 0,\n",
    "            'ring_sizes': []\n",
    "        }\n",
    "    \n",
    "    return physics_features\n",
    "\n",
    "def get_molecular_graph(mol):\n",
    "    \"\"\"\n",
    "    Extract the molecular graph representation with detailed atom and bond features\n",
    "    \"\"\"\n",
    "    if mol is None:\n",
    "        return {'atoms': [], 'bonds': []}\n",
    "    \n",
    "    # Get ring information - use the proper RDKit method\n",
    "    ring_info = mol.GetRingInfo()\n",
    "    atom_rings = defaultdict(list)\n",
    "    bond_rings = defaultdict(list)\n",
    "    \n",
    "    # Map atoms to the rings they belong to\n",
    "    for ring_idx, ring in enumerate(ring_info.AtomRings()):\n",
    "        ring_size = len(ring)\n",
    "        for atom_idx in ring:\n",
    "            atom_rings[atom_idx].append(ring_size)\n",
    "    \n",
    "    # Map bonds to the rings they belong to\n",
    "    for ring_idx, ring in enumerate(ring_info.BondRings()):\n",
    "        ring_size = len(ring)\n",
    "        for bond_idx in ring:\n",
    "            bond_rings[bond_idx].append(ring_size)\n",
    "    \n",
    "    # Process atoms\n",
    "    atoms = []\n",
    "    for atom in mol.GetAtoms():\n",
    "        atom_idx = atom.GetIdx()\n",
    "        atom_features = {\n",
    "            'atomic_num': atom.GetAtomicNum(),\n",
    "            'formal_charge': atom.GetFormalCharge(),\n",
    "            'hybridization': int(atom.GetHybridization()),\n",
    "            'aromatic': int(atom.GetIsAromatic()),\n",
    "            'num_h': atom.GetTotalNumHs(),\n",
    "            'degree': atom.GetDegree(),\n",
    "            'chirality': int(atom.GetChiralTag()),\n",
    "            'is_in_ring': int(atom.IsInRing()),\n",
    "            'ring_sizes': sorted(atom_rings[atom_idx]),\n",
    "            'valence': atom.GetTotalValence()\n",
    "        }\n",
    "        atoms.append(atom_features)\n",
    "    \n",
    "    # Process bonds\n",
    "    bonds = []\n",
    "    for bond in mol.GetBonds():\n",
    "        bond_idx = bond.GetIdx()\n",
    "        bond_features = {\n",
    "            'start_atom': bond.GetBeginAtomIdx(),\n",
    "            'end_atom': bond.GetEndAtomIdx(),\n",
    "            'bond_type': int(bond.GetBondType()),\n",
    "            'is_conjugated': int(bond.GetIsConjugated()),\n",
    "            'is_aromatic': int(bond.GetIsAromatic()),\n",
    "            'is_in_ring': int(bond.IsInRing()),\n",
    "            'stereo': int(bond.GetStereo()),\n",
    "            'ring_sizes': sorted(bond_rings[bond_idx])\n",
    "        }\n",
    "        bonds.append(bond_features)\n",
    "    \n",
    "    return {'atoms': atoms, 'bonds': bonds}\n",
    "\n",
    "def process_smiles_batch_multiscale(args):\n",
    "    \"\"\"\n",
    "    Process a batch of SMILES strings to generate multi-scale ligand representations\n",
    "    \"\"\"\n",
    "    smiles_batch, batch_idx, chunkify = args\n",
    "    start_time = time.time()\n",
    "    ligand_data = {}\n",
    "    \n",
    "    print(f\"Worker {multiprocessing.current_process().name} processing batch {batch_idx} with {len(smiles_batch)} SMILES for multi-scale\")\n",
    "    \n",
    "    # Process molecules in smaller chunks to save intermediate results\n",
    "    if chunkify:\n",
    "        chunk_size = 10  # Process 10 molecules at a time\n",
    "        for i in range(0, len(smiles_batch), chunk_size):\n",
    "            chunk = smiles_batch[i:i+chunk_size]\n",
    "            chunk_start = time.time()\n",
    "            print(f\"  Worker {multiprocessing.current_process().name} processing chunk {i//chunk_size} of batch {batch_idx}\")\n",
    "            \n",
    "            for smiles in chunk:\n",
    "                try:\n",
    "                    # Convert SMILES to RDKit molecule\n",
    "                    mol = Chem.MolFromSmiles(smiles)\n",
    "                    \n",
    "                    if mol is None:\n",
    "                        print(f\"Failed to parse SMILES: {smiles}\")\n",
    "                        continue\n",
    "\n",
    "                    \n",
    "\n",
    "                    \n",
    "                    # Process molecule as before...\n",
    "                    # 1. Generate scaffold hierarchy\n",
    "                    scaffolds = get_scaffold_hierarchy(mol)\n",
    "                    \n",
    "                    # 2. Generate basic molecular properties\n",
    "                    properties = {}\n",
    "                    properties['molecular_weight'] = Descriptors.MolWt(mol)\n",
    "                    properties['logp'] = Descriptors.MolLogP(mol)\n",
    "                    properties['h_donors'] = Descriptors.NumHDonors(mol)\n",
    "                    properties['h_acceptors'] = Descriptors.NumHAcceptors(mol)\n",
    "                    properties['rotatable_bonds'] = Descriptors.NumRotatableBonds(mol)\n",
    "                    properties['tpsa'] = Descriptors.TPSA(mol)\n",
    "                    properties['qed'] = qed(mol)\n",
    "                    properties['heavy_atom_count'] = Descriptors.HeavyAtomCount(mol)\n",
    "                    properties['ring_count'] = Descriptors.RingCount(mol)\n",
    "                    properties['aromatic_rings'] = rdMolDescriptors.CalcNumAromaticRings(mol)\n",
    "                    properties['fraction_sp3'] = rdMolDescriptors.CalcFractionCSP3(mol)\n",
    "                    properties['num_stereocenters'] = len(Chem.FindMolChiralCenters(mol, includeUnassigned=True))\n",
    "                    \n",
    "                    # 3. Identify functional groups\n",
    "                    functional_groups = {}\n",
    "                    for group_name, smarts in FUNCTIONAL_GROUPS.items():\n",
    "                        pattern = Chem.MolFromSmarts(smarts)\n",
    "                        if pattern:\n",
    "                            matches = mol.GetSubstructMatches(pattern)\n",
    "                            functional_groups[group_name] = len(matches)\n",
    "                    \n",
    "                    # 4. Calculate physics-based features\n",
    "                    physics_features = calc_physics_features(mol)\n",
    "                    \n",
    "                    # 5. Get molecular graph representation\n",
    "                    graph = get_molecular_graph(mol)\n",
    "                    \n",
    "                    # 6. Decompose into fragments for multi-scale representation\n",
    "                    # Level 1: Bemis-Murcko scaffold\n",
    "                    scaffold_mol = MurckoScaffold.GetScaffoldForMol(mol)\n",
    "                    level1_graph = get_molecular_graph(scaffold_mol) if scaffold_mol else {'atoms': [], 'bonds': []}\n",
    "                    \n",
    "                    # Level 2: Find functional group attachment points\n",
    "                    level2_fragments = []\n",
    "                    try:\n",
    "                        brics_fragments = list(BRICS.BRICSDecompose(mol))\n",
    "                        for frag_smiles in brics_fragments:\n",
    "                            frag_mol = Chem.MolFromSmiles(frag_smiles)\n",
    "                            if frag_mol:\n",
    "                                frag_graph = get_molecular_graph(frag_mol)\n",
    "                                level2_fragments.append({\n",
    "                                    'smiles': frag_smiles,\n",
    "                                    'graph': frag_graph\n",
    "                                })\n",
    "                    except Exception as e:\n",
    "                        # If fragment decomposition fails, just log and continue\n",
    "                        pass\n",
    "                    \n",
    "                    # 7. Create multi-scale representation\n",
    "                    multi_scale = {\n",
    "                        'level1_scaffold': {\n",
    "                            'smiles': Chem.MolToSmiles(scaffold_mol) if scaffold_mol else \"\",\n",
    "                            'graph': level1_graph\n",
    "                        },\n",
    "                        'level2_fragments': level2_fragments,\n",
    "                        'level3_complete': {\n",
    "                            'smiles': smiles,\n",
    "                            'graph': graph\n",
    "                        }\n",
    "                    }\n",
    "                    \n",
    "                    # Store all data\n",
    "                    ligand_data[smiles] = {\n",
    "                        'properties': properties,\n",
    "                        'functional_groups': functional_groups,\n",
    "                        'physics_features': physics_features,\n",
    "                        'multi_scale': multi_scale\n",
    "                    }\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing SMILES {smiles} for multi-scale: {str(e)}\")\n",
    "            \n",
    "            chunk_end = time.time()\n",
    "            print(f\"  Chunk {i//chunk_size} of batch {batch_idx} completed in {chunk_end - chunk_start:.2f} seconds\")\n",
    "    else:\n",
    "        # Non-chunked processing (same as original code)\n",
    "        for smiles in smiles_batch:\n",
    "            try:\n",
    "                # Convert SMILES to RDKit molecule\n",
    "                print(\"Started\", smiles)\n",
    "                mol = Chem.MolFromSmiles(smiles)\n",
    "                \n",
    "                if mol is None:\n",
    "                    print(f\"Failed to parse SMILES: {smiles}\")\n",
    "                    continue\n",
    "\n",
    "                # Skip very large ligands\n",
    "                if mol.GetNumAtoms() > 150:\n",
    "                    print(f\"Skipping {smiles} due to large size ({mol.GetNumAtoms()} atoms)\")\n",
    "                    continue\n",
    "\n",
    "                print(\"Length\", mol.GetNumAtoms())\n",
    "                # Process molecule as in the chunked version...\n",
    "                # 1. Generate scaffold hierarchy\n",
    "                scaffolds = get_scaffold_hierarchy(mol)\n",
    "                print(\"scaffolds Done\")\n",
    "                \n",
    "                # 2. Generate basic molecular properties\n",
    "                properties = {}\n",
    "                properties['molecular_weight'] = Descriptors.MolWt(mol)\n",
    "                properties['logp'] = Descriptors.MolLogP(mol)\n",
    "                properties['h_donors'] = Descriptors.NumHDonors(mol)\n",
    "                properties['h_acceptors'] = Descriptors.NumHAcceptors(mol)\n",
    "                properties['rotatable_bonds'] = Descriptors.NumRotatableBonds(mol)\n",
    "                properties['tpsa'] = Descriptors.TPSA(mol)\n",
    "                properties['qed'] = qed(mol)\n",
    "                properties['heavy_atom_count'] = Descriptors.HeavyAtomCount(mol)\n",
    "                properties['ring_count'] = Descriptors.RingCount(mol)\n",
    "                properties['aromatic_rings'] = rdMolDescriptors.CalcNumAromaticRings(mol)\n",
    "                properties['fraction_sp3'] = rdMolDescriptors.CalcFractionCSP3(mol)\n",
    "                properties['num_stereocenters'] = len(Chem.FindMolChiralCenters(mol, includeUnassigned=True))\n",
    "\n",
    "                print(\"properties Done\")\n",
    "                # 3. Identify functional groups\n",
    "                functional_groups = {}\n",
    "                for group_name, smarts in FUNCTIONAL_GROUPS.items():\n",
    "                    pattern = Chem.MolFromSmarts(smarts)\n",
    "                    if pattern:\n",
    "                        matches = mol.GetSubstructMatches(pattern)\n",
    "                        functional_groups[group_name] = len(matches)\n",
    "                print(\"functional_groups Done\")\n",
    "                # 4. Calculate physics-based features\n",
    "                physics_features = calc_physics_features(mol)\n",
    "                print(\"physics_features Done\")\n",
    "                # 5. Get molecular graph representation\n",
    "                graph = get_molecular_graph(mol)\n",
    "                print(\"graph done\")\n",
    "                \n",
    "                # 6. Decompose into fragments for multi-scale representation\n",
    "                # Level 1: Bemis-Murcko scaffold\n",
    "                scaffold_mol = MurckoScaffold.GetScaffoldForMol(mol)\n",
    "                level1_graph = get_molecular_graph(scaffold_mol) if scaffold_mol else {'atoms': [], 'bonds': []}\n",
    "                print(\"Multi Scale L1 Done\")\n",
    "                # Level 2: Find functional group attachment points\n",
    "                level2_fragments = []\n",
    "                try:\n",
    "                    brics_fragments = list(BRICS.BRICSDecompose(mol))\n",
    "                    for frag_smiles in brics_fragments:\n",
    "                        frag_mol = Chem.MolFromSmiles(frag_smiles)\n",
    "                        if frag_mol:\n",
    "                            frag_graph = get_molecular_graph(frag_mol)\n",
    "                            level2_fragments.append({\n",
    "                                'smiles': frag_smiles,\n",
    "                                'graph': frag_graph\n",
    "                            })\n",
    "                except Exception as e:\n",
    "                    # If fragment decomposition fails, just log and continue\n",
    "                    pass\n",
    "                print(\"Multi Scale L2 Done\")\n",
    "                # 7. Create multi-scale representation\n",
    "                multi_scale = {\n",
    "                    'level1_scaffold': {\n",
    "                        'smiles': Chem.MolToSmiles(scaffold_mol) if scaffold_mol else \"\",\n",
    "                        'graph': level1_graph\n",
    "                    },\n",
    "                    'level2_fragments': level2_fragments,\n",
    "                    'level3_complete': {\n",
    "                        'smiles': smiles,\n",
    "                        'graph': graph\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                # Store all data\n",
    "                ligand_data[smiles] = {\n",
    "                    'properties': properties,\n",
    "                    'functional_groups': functional_groups,\n",
    "                    'physics_features': physics_features,\n",
    "                    'multi_scale': multi_scale\n",
    "                }\n",
    "                print(\"Completed\", smiles)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing SMILES {smiles} for multi-scale: {str(e)}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Worker completed multi-scale batch {batch_idx} in {end_time - start_time:.2f} seconds\")\n",
    "    return ligand_data\n",
    "\n",
    "def generate_multiscale_ligand_representations(parquet_file, output_dir, batch_size=100, n_jobs=8, chunkify=True):\n",
    "    \"\"\"\n",
    "    Generate multi-scale graph-based representations with SMILES-level resumption capability\n",
    "    \"\"\"\n",
    "    # Create directories\n",
    "    batch_dir = os.path.join(output_dir, 'batch_results')\n",
    "    os.makedirs(batch_dir, exist_ok=True)\n",
    "    \n",
    "    # Path for our SMILES mapping file\n",
    "    smiles_map_file = os.path.join(output_dir, 'processed_smiles_map.pkl')\n",
    "    \n",
    "    # Load existing SMILES map if it exists\n",
    "    processed_smiles = {}\n",
    "    if os.path.exists(smiles_map_file):\n",
    "        try:\n",
    "            with open(smiles_map_file, 'rb') as f:\n",
    "                processed_smiles = pickle.load(f)\n",
    "            print(f\"Found mapping file with {len(processed_smiles)} already processed SMILES\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading existing SMILES map: {e}\")\n",
    "            processed_smiles = {}\n",
    "    \n",
    "    # Find the highest existing batch index to avoid overwriting\n",
    "    highest_batch_idx = -1\n",
    "    if processed_smiles:\n",
    "        highest_batch_idx = max(processed_smiles.values())\n",
    "    print(f\"Highest existing batch index: {highest_batch_idx}\")\n",
    "    \n",
    "    print(\"Loading dataset...\")\n",
    "    df = pd.read_parquet(parquet_file)\n",
    "    \n",
    "    # Get unique ligands\n",
    "    unique_smiles = df['smiles'].unique()\n",
    "    print(f\"Total unique ligands: {len(unique_smiles)}\")\n",
    "    \n",
    "    # Filter out already processed SMILES\n",
    "    smiles_to_process = [s for s in unique_smiles if s not in processed_smiles]\n",
    "    print(f\"SMILES remaining to process: {len(smiles_to_process)}\")\n",
    "    \n",
    "    # Split remaining SMILES into batches\n",
    "    num_batches = (len(smiles_to_process) + batch_size - 1) // batch_size\n",
    "    smiles_batches = [smiles_to_process[i*batch_size:(i+1)*batch_size] for i in range(num_batches)]\n",
    "    \n",
    "    # Track the next available batch index\n",
    "    next_batch_idx = highest_batch_idx + 1\n",
    "    \n",
    "    # Process remaining batches\n",
    "    for batch_group_idx in range(0, len(smiles_batches), n_jobs):\n",
    "        # Prepare current set of batch arguments\n",
    "        current_batch_args = []\n",
    "        for i in range(batch_group_idx, min(batch_group_idx + n_jobs, len(smiles_batches))):\n",
    "            # Use a new unique batch index for each batch\n",
    "            current_batch_idx = next_batch_idx\n",
    "            next_batch_idx += 1\n",
    "            current_batch_args.append((smiles_batches[i], current_batch_idx, chunkify))\n",
    "        \n",
    "        with multiprocessing.Pool(processes=len(current_batch_args)) as pool:\n",
    "            # Process current batches\n",
    "            batch_results = list(tqdm(\n",
    "                pool.imap(process_smiles_batch_multiscale, current_batch_args),\n",
    "                total=len(current_batch_args),\n",
    "                desc=f\"Processing batch group {batch_group_idx//n_jobs + 1}/{(len(smiles_batches) + n_jobs - 1)//n_jobs}\"\n",
    "            ))\n",
    "        \n",
    "        # Save results and update SMILES map\n",
    "        for i, batch_result in enumerate(batch_results):\n",
    "            batch_idx = current_batch_args[i][1]  # Get the batch index from arguments\n",
    "            batch_file = os.path.join(batch_dir, f'batch_{batch_idx}.pkl')\n",
    "            \n",
    "            # Save batch results\n",
    "            with open(batch_file, 'wb') as f:\n",
    "                pickle.dump(batch_result, f)\n",
    "            \n",
    "            # Update the SMILES map with newly processed SMILES\n",
    "            for smiles in batch_result.keys():\n",
    "                processed_smiles[smiles] = batch_idx\n",
    "            \n",
    "            print(f\"Saved batch {batch_idx} with {len(batch_result)} ligands\")\n",
    "        \n",
    "        # Save updated SMILES map after each batch group\n",
    "        with open(smiles_map_file, 'wb') as f:\n",
    "            pickle.dump(processed_smiles, f)\n",
    "        \n",
    "        # Clear memory\n",
    "        batch_results = None\n",
    "        gc.collect()\n",
    "        try:\n",
    "            print(\"Clearing memory...\")\n",
    "            import ctypes\n",
    "            ctypes.cdll.LoadLibrary('libc.so.6')\n",
    "            ctypes.CDLL('libc.so.6').malloc_trim(0)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Combine all results\n",
    "    if len(processed_smiles) > 0:\n",
    "        print(\"Combining all batch results...\")\n",
    "        \n",
    "        # Get all batch files based on the SMILES mapping\n",
    "        batch_indices = set(processed_smiles.values())\n",
    "        batch_files = [os.path.join(batch_dir, f'batch_{idx}.pkl') for idx in batch_indices]\n",
    "        \n",
    "        combined_ligand_data = {}\n",
    "        \n",
    "        for batch_file in tqdm(batch_files, desc=\"Loading batch files\"):\n",
    "            try:\n",
    "                with open(batch_file, 'rb') as f:\n",
    "                    batch_data = pickle.load(f)\n",
    "                    combined_ligand_data.update(batch_data)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading batch file {batch_file}: {str(e)}\")\n",
    "        \n",
    "        # Save the final combined data\n",
    "        final_file = os.path.join(output_dir, 'ligand_multiscale_etkdg_representations.pkl')\n",
    "        with open(final_file, 'wb') as f:\n",
    "            pickle.dump(combined_ligand_data, f)\n",
    "        \n",
    "        print(f\"Saved multi-scale representations for {len(combined_ligand_data)} ligands to {final_file}\")\n",
    "    \n",
    "    return processed_smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27c2a1ba-6eac-4f50-be79-ab84f7120ddb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found mapping file with 16525 already processed SMILES\n",
      "Highest existing batch index: 3979\n",
      "Loading dataset...\n",
      "Total unique ligands: 16536\n",
      "SMILES remaining to process: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch group 1/1:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker ForkPoolWorker-625 processing batch 3982 with 1 SMILES for multi-scaleWorker ForkPoolWorker-624 processing batch 3981 with 5 SMILES for multi-scaleWorker ForkPoolWorker-623 processing batch 3980 with 5 SMILES for multi-scale\n",
      "\n",
      "\n",
      "StartedStartedStarted   [NH3+]CCCC[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)O)CC(=O)O)C)Cc1ccc(cc1)O)CC(C)C)CC(C)C)CCC(=O)O)CCSC)CCC(=O)N)[C@H](O)C)[C@H](O)C)[C@H](O)C)CCCNC(=N)N)CCCNC(=N)N)CCCNC(=N)N)C)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@H]1[NH2+]CCC1)CCCC[NH3+])CC(=O)N[NH3+]CCCC[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)NCC(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)O)CCSC)[C@H](CC)C)CC(=O)O)Cc1ccccc1)CC(C)C)CC(=O)O)[C@H](O)C)C(C)C)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@H]([C@H](O)C)[NH3+])CCSC)CCC(=O)O)CC(=O)N)CC(C)C)CO)CCCNC(=N)N)CCCNC(=N)N)CC(C)COC[C@@H]1NC(=O)[C@@H](NC(=O)CNC(=O)[C@H](Cc2ccccc2)NC(=O)[C@H](CCCNC(=N)N)NC(=O)[C@@H](NC(=O)[C@H](CC2=NC=NC2)NC(=O)[C@H](CO)NC(=O)[C@@H](NC(=O)CNC(=O)[C@@H](NC(=O)CSC[C@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC1=O)Cc1ccc(cc1)O)CC1=NC=NC1)C(C)C)CO)Cc1ccc(cc1)O)C(=O)NCC(=O)N)Cc1ccc(cc1)O)CC1=NC=NC1)[C@H](CC)C)Cc1ccc(cc1)O\n",
      "\n",
      "\n",
      "Skipping [NH3+]CCCC[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)O)CC(=O)O)C)Cc1ccc(cc1)O)CC(C)C)CC(C)C)CCC(=O)O)CCSC)CCC(=O)N)[C@H](O)C)[C@H](O)C)[C@H](O)C)CCCNC(=N)N)CCCNC(=N)N)CCCNC(=N)N)C)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@H]1[NH2+]CCC1)CCCC[NH3+])CC(=O)N due to large size (160 atoms)Skipping [NH3+]CCCC[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)NCC(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)O)CCSC)[C@H](CC)C)CC(=O)O)Cc1ccccc1)CC(C)C)CC(=O)O)[C@H](O)C)C(C)C)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@H]([C@H](O)C)[NH3+])CCSC)CCC(=O)O)CC(=O)N)CC(C)C)CO)CCCNC(=N)N)CCCNC(=N)N)CC(C)C due to large size (155 atoms)Skipping OC[C@@H]1NC(=O)[C@@H](NC(=O)CNC(=O)[C@H](Cc2ccccc2)NC(=O)[C@H](CCCNC(=N)N)NC(=O)[C@@H](NC(=O)[C@H](CC2=NC=NC2)NC(=O)[C@H](CO)NC(=O)[C@@H](NC(=O)CNC(=O)[C@@H](NC(=O)CSC[C@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC1=O)Cc1ccc(cc1)O)CC1=NC=NC1)C(C)C)CO)Cc1ccc(cc1)O)C(=O)NCC(=O)N)Cc1ccc(cc1)O)CC1=NC=NC1)[C@H](CC)C)Cc1ccc(cc1)O due to large size (155 atoms)\n",
      "\n",
      "\n",
      "StartedWorker completed multi-scale batch 3982 in 0.01 secondsStarted \n",
      " [NH3+]CCCC[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)O)CCCC[NH3+])CCCC[NH3+])NC(=O)[C@@H](NC(=O)[C@H]([C@H](CC)C)NC(=O)[C@]1(C)CCCCCC[C@@](C)(NC(=O)[C@@H](NC(=O)[C@H]([C@H](O)C)NC(=O)[C@H]([C@H](CC)C)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@H](CC2=CN=C3[C@H]2C=CC=C3)NC(=O)[C@@H](NC(=O)C)CC(=O)N)Cc2ccccc2)CC(=O)N)CC(=O)N)C(=O)N[C@H](C(=O)N[C@H](C(=O)N1)CC1=CN=C2[C@H]1C=CC=C2)CC(C)C)CCNC(=O)CNC(=O)[C@H](COP(=O)(O)O)[NH3+][NH3+]CCCOc1cc(nc2c1cccc2NC(=O)c1cc(O)c2c(n1)c(ccc2)NC(=O)c1cccc(n1)CNC(=O)c1cc(OCC(C)C)c2c(n1)c(ccc2)NC(=O)NCCCCOc1cccc(c1)CNC(=O)c1ccc(cc1)S(=O)(=O)N)C(=O)NCc1cccc(n1)C(=O)Nc1cccc2c1nc(cc2OCC(=O)O)C(=O)NCc1cccc(n1)C(=O)NCc1cccc(n1)C(=O)Nc1cccc2c1nc(cc2O)C(=O)NCc1cccc(n1)C(=O)Nc1cccc2c1nc(cc2CC(=O)O)C(=O)NCc1cccc(n1)C(=O)Nc1cccc2c1nc(cc2OCC(=O)O)C(=O)Nc1cccc2c1nc(cc2OCCC[NH3+])C(=O)O\n",
      "\n",
      "Skipping [NH3+]CCCC[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)O)CCCC[NH3+])CCCC[NH3+])NC(=O)[C@@H](NC(=O)[C@H]([C@H](CC)C)NC(=O)[C@]1(C)CCCCCC[C@@](C)(NC(=O)[C@@H](NC(=O)[C@H]([C@H](O)C)NC(=O)[C@H]([C@H](CC)C)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@H](CC2=CN=C3[C@H]2C=CC=C3)NC(=O)[C@@H](NC(=O)C)CC(=O)N)Cc2ccccc2)CC(=O)N)CC(=O)N)C(=O)N[C@H](C(=O)N[C@H](C(=O)N1)CC1=CN=C2[C@H]1C=CC=C2)CC(C)C)CCNC(=O)CNC(=O)[C@H](COP(=O)(O)O)[NH3+] due to large size (162 atoms)\n",
      "StartedSkipping [NH3+]CCCOc1cc(nc2c1cccc2NC(=O)c1cc(O)c2c(n1)c(ccc2)NC(=O)c1cccc(n1)CNC(=O)c1cc(OCC(C)C)c2c(n1)c(ccc2)NC(=O)NCCCCOc1cccc(c1)CNC(=O)c1ccc(cc1)S(=O)(=O)N)C(=O)NCc1cccc(n1)C(=O)Nc1cccc2c1nc(cc2OCC(=O)O)C(=O)NCc1cccc(n1)C(=O)NCc1cccc(n1)C(=O)Nc1cccc2c1nc(cc2O)C(=O)NCc1cccc(n1)C(=O)Nc1cccc2c1nc(cc2CC(=O)O)C(=O)NCc1cccc(n1)C(=O)Nc1cccc2c1nc(cc2OCC(=O)O)C(=O)Nc1cccc2c1nc(cc2OCCC[NH3+])C(=O)O due to large size (224 atoms) \n",
      "[NH3+]CCCC[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)O)CCCC[NH3+])CCCC[NH3+])CCCC[NH3+])NC(=O)[C@H]([C@H](CC)C)NC(=O)[C@@H](NC(=O)[C@H](CC1=CN=C2[C@H]1C=CC=C2)NC(=O)[C@@H](NC(=O)[C@H](CC1=CN=C2[C@H]1C=CC=C2)NC(=O)[C@@H](NC(=O)[C@H]([C@H](O)C)NC(=O)[C@H]([C@H](CC)C)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@H](CC(=O)N)[NH3+])CC1=CN=C2[C@H]1C=CC=C2)Cc1ccccc1)CC(=O)O)CC(=O)N)CC(C)C)Cc1ccc(cc1)OStarted\n",
      " [NH3+]CCCC[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H]1CSSC[C@H](NC1=O)C(=O)N1CCC[C@H]1C(=O)N[C@H](C(=O)N[C@H](C(=O)N1CCC[C@H]1C(=O)N[C@H](C(=O)O)Cc1ccc(cc1)O)[C@@H](O)C)CC(=O)O)[C@H](O)C)Cc1ccc(cc1)O)Cc1ccc(cc1)O)C(C)C)CC1=CN=C2[C@@H]1C=CC=C2)CC1=NC=NC1)NC(=O)[C@H](CC1=CN=C2[C@@H]1C=CC=C2)NC(=O)CNC(=O)[C@@H](NC(=O)[C@H](Cc1ccc(cc1)O)[NH3+])CCCNC(=N)N\n",
      "Skipping [NH3+]CCCC[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)O)CCCC[NH3+])CCCC[NH3+])CCCC[NH3+])NC(=O)[C@H]([C@H](CC)C)NC(=O)[C@@H](NC(=O)[C@H](CC1=CN=C2[C@H]1C=CC=C2)NC(=O)[C@@H](NC(=O)[C@H](CC1=CN=C2[C@H]1C=CC=C2)NC(=O)[C@@H](NC(=O)[C@H]([C@H](O)C)NC(=O)[C@H]([C@H](CC)C)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@H](CC(=O)N)[NH3+])CC1=CN=C2[C@H]1C=CC=C2)Cc1ccccc1)CC(=O)O)CC(=O)N)CC(C)C)Cc1ccc(cc1)O due to large size (157 atoms)\n",
      "StartedSkipping [NH3+]CCCC[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H]1CSSC[C@H](NC1=O)C(=O)N1CCC[C@H]1C(=O)N[C@H](C(=O)N[C@H](C(=O)N1CCC[C@H]1C(=O)N[C@H](C(=O)O)Cc1ccc(cc1)O)[C@@H](O)C)CC(=O)O)[C@H](O)C)Cc1ccc(cc1)O)Cc1ccc(cc1)O)C(C)C)CC1=CN=C2[C@@H]1C=CC=C2)CC1=NC=NC1)NC(=O)[C@H](CC1=CN=C2[C@@H]1C=CC=C2)NC(=O)CNC(=O)[C@@H](NC(=O)[C@H](Cc1ccc(cc1)O)[NH3+])CCCNC(=N)N due to large size (166 atoms) \n",
      "OC[C@@H]1NC(=O)[C@H](C)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@H](CCCNC(=N)N)NC(=O)[C@@H](NC(=O)[C@H](CSSC[C@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC1=O)CC1=NC=NC1)[C@H](O)C)CC1=CN=C2[C@@H]1C=CC=C2)Cc1ccccc1)C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)O)CCC(=O)O)CCC(=O)O)C)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@H](C(C)C)[NH3+])CC(=O)O)CCC(=O)O)CC1=CN=C2[C@@H]1C=CC=C2)[C@H](CC)C)[C@H](CC)CStarted\n",
      " CSCC[C@@H]1NC(=O)[C@H](CCCNC(=N)N)NC(=O)[C@H](C)NC(=O)[C@@H](NC(=O)[C@H](CC(=O)O)NC(=O)[C@H](CSSC[C@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC1=O)CC1=CN=C2[C@H]1C=CC=C2)CCC(=O)O)CC1=CN=C2[C@@H]1C=CC=C2)CCC(=O)O)C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)O)CC(C)C)CCCNC(=N)N)CCC(=O)O)Cc1ccccc1)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)CNC(=O)C[NH3+])CC(=O)N)CCC(=O)O)[C@H](CC)C\n",
      "Skipping OC[C@@H]1NC(=O)[C@H](C)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@H](CCCNC(=N)N)NC(=O)[C@@H](NC(=O)[C@H](CSSC[C@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC1=O)CC1=NC=NC1)[C@H](O)C)CC1=CN=C2[C@@H]1C=CC=C2)Cc1ccccc1)C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)O)CCC(=O)O)CCC(=O)O)C)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@H](C(C)C)[NH3+])CC(=O)O)CCC(=O)O)CC1=CN=C2[C@@H]1C=CC=C2)[C@H](CC)C)[C@H](CC)C due to large size (154 atoms)\n",
      "StartedSkipping CSCC[C@@H]1NC(=O)[C@H](CCCNC(=N)N)NC(=O)[C@H](C)NC(=O)[C@@H](NC(=O)[C@H](CC(=O)O)NC(=O)[C@H](CSSC[C@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC1=O)CC1=CN=C2[C@H]1C=CC=C2)CCC(=O)O)CC1=CN=C2[C@@H]1C=CC=C2)CCC(=O)O)C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)O)CC(C)C)CCCNC(=N)N)CCC(=O)O)Cc1ccccc1)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)CNC(=O)C[NH3+])CC(=O)N)CCC(=O)O)[C@H](CC)C due to large size (163 atoms) \n",
      "CC[C@@H]([C@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)C[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@H]([C@H](CC)C)NC(=O)C[C@H](CC1=c2ccccc2=NC1)NC(=O)[C@H]([C@H](CC)C)[NH3+])C)CCC(=O)N)CCC(=O)O)CC(C)C)CCCNC(=N)N)CCCNC(=N)N)CC(=O)NCC(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](CC(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@@H](Cc1ccc(cc1)O)CC(=O)N)Cc1ccc(cc1)O)C)CC(=O)N)Cc1ccccc1)CCC(=O)O)CC(=O)O)CStarted\n",
      " CC(C[C@@H](C(=O)N[C@H](C(=O)N)CC1=CN=C2[C@@H]1C=CC=C2)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H]1CCCN1C(=O)[C@H](Cc1c(F)c(F)c(c(c1F)F)F)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@H](Cc1c(F)c(F)c(c(c1F)F)F)NC(=O)[C@@H](NC(=O)[C@H](Cc1c(F)c(F)c(c(c1F)F)F)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@H](Cc1c(F)c(F)c(c(c1F)F)F)NC(=O)[C@@H](NC(=O)[C@H]1[NH2+]CCC1)COP(=O)(O)O)CC(=O)O)COP(=O)(O)O)CC(=O)O)CCC(=O)O)CC(=O)O)C)C)C\n",
      "Skipping CC[C@@H]([C@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)C[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@H]([C@H](CC)C)NC(=O)C[C@H](CC1=c2ccccc2=NC1)NC(=O)[C@H]([C@H](CC)C)[NH3+])C)CCC(=O)N)CCC(=O)O)CC(C)C)CCCNC(=N)N)CCCNC(=N)N)CC(=O)NCC(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](CC(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@@H](Cc1ccc(cc1)O)CC(=O)N)Cc1ccc(cc1)O)C)CC(=O)N)Cc1ccccc1)CCC(=O)O)CC(=O)O)C due to large size (166 atoms)\n",
      "Worker completed multi-scale batch 3981 in 0.02 secondsSkipping CC(C[C@@H](C(=O)N[C@H](C(=O)N)CC1=CN=C2[C@@H]1C=CC=C2)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H]1CCCN1C(=O)[C@H](Cc1c(F)c(F)c(c(c1F)F)F)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@H](Cc1c(F)c(F)c(c(c1F)F)F)NC(=O)[C@@H](NC(=O)[C@H](Cc1c(F)c(F)c(c(c1F)F)F)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@H](Cc1c(F)c(F)c(c(c1F)F)F)NC(=O)[C@@H](NC(=O)[C@H]1[NH2+]CCC1)COP(=O)(O)O)CC(=O)O)COP(=O)(O)O)CC(=O)O)CCC(=O)O)CC(=O)O)C)C)C due to large size (164 atoms)\n",
      "\n",
      "Worker completed multi-scale batch 3980 in 0.03 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch group 1/1: 100%|██████████| 3/3 [00:00<00:00, 113.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved batch 3980 with 0 ligands\n",
      "Saved batch 3981 with 0 ligands\n",
      "Saved batch 3982 with 0 ligands\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing memory...\n",
      "Combining all batch results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading batch files: 100%|██████████| 3975/3975 [00:13<00:00, 290.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved multi-scale representations for 16525 ligands to processed_data/ligands_multiscale_etkdg/ligand_multiscale_etkdg_representations.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'CCCCCCCCCCCCCCCCCCCC(=O)O': 0,\n",
       " 'OC[C@H]1O[C@H](Oc2cccc(c2)N(=O)=O)[C@@H]([C@H]([C@H]1O)O)O': 1,\n",
       " 'COc1ccc(cc1)c1c(onc1c1cc(C(C)C)c(cc1O)O)NC(=O)C1=CC1': 2,\n",
       " 'OC[C@@H](C(=O)N[C@@H]([C@H](CC)C)C(=O)O)NC(=O)[C@H](CC1=CN=C2[C@@H]1C=CC=C2)NC(=O)[C@H](CC(=O)O)[NH3+]': 3,\n",
       " 'CO[C@@H]1[C@H](O[C@H]([C@@H]1O)n1ccc(=O)[nH]c1=O)[C@H](C(=O)N)O[C@H]1OC(=C[C@@H]([C@@H]1O)O)C(=O)N[C@H]1CCCCNC1=O': 4,\n",
       " 'OC(=O)[C@@H](NC1=NC(C)(C)Cc2c1cccc2)Cc1ccccc1': 5,\n",
       " 'Cc1ccc(cc1c1ccc2c(c1)[nH]c(=O)n2C)NC(=O)c1cccc(c1)C(F)(F)F': 6,\n",
       " 'O[C@H]1O[C@H](COP(=O)(O)O)[C@H]([C@H]([C@H]1O)O)O': 7,\n",
       " 'CC[C@@H]([C@@H](C(=O)N[C@H](C(=O)NCC(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N1CCC[C@H]1C(=O)N[C@H](C(=O)N[C@H](C(=O)O)CC(=O)N)[C@H](CC)C)Cc1ccc(cc1)O)[C@H](O)C)[C@H](O)C)C(C)C)NC(=O)[C@@H](NC(=O)[C@H](Cc1ccccc1)[NH3+])CC(=O)N)C': 8,\n",
       " 'OCC[C@@H]1CCCCN1c1cc(NCC2=CC=CN(C2)O)n2c(n1)c(CC)cn2': 9,\n",
       " 'CCO[C@H](C(=O)O)Cc1ccc(cc1)OCCc1ccc(cc1)OS(=O)(=O)C': 10,\n",
       " 'O[C@@H]1[C@@H](COS(=O)(=O)NC(=O)[C@@H]2CCC[NH2+]2)O[C@H]([C@@H]1O)n1cnc2c1ncnc2N': 11,\n",
       " 'CCNC(=O)c1nc([nH]c(=O)c1O)[C@@H]1CCCN1C(=O)C': 12,\n",
       " 'OC[C@H]1O[C@H](C[C@H]([C@@H]1O)F)n1ccc(nc1=O)NC(=O)c1ccccc1': 13,\n",
       " 'C[N@@H+]1CC[N@H+](CC1)Cc1ccc(cc1C(F)(F)F)NC(=O)c1ccc(c(c1)C#Cc1cnc2n1nccc2)C': 14,\n",
       " 'CC[NH+](CCNC(=O)[C@@H]1C(=NC(=C1C)/C=C/1\\\\C(=O)Nc2c1cc(F)cc2)C)CC': 15,\n",
       " 'NC(=O)C[C@H]1CCc2c1c1c(ncnc1s2)O[C@@H]1CC[C@H](CC1)[NH+]1CCOCC1': 16,\n",
       " 'O=C([C@@H]1C=Nc2c1cccc2)c1ccco1': 17,\n",
       " 'OC[C@H]([C@H]([C@@H]1O[C@@H](C[C@@H]([C@H]1NC(=O)C)NC(=N)N)C(=O)O)O)O': 18,\n",
       " 'OC(=O)C[C@H](c1ccc2c(c1)nnn2C)c1ccc(c(c1)C)Cl': 19,\n",
       " 'COc1cccc(c1C(=O)N[C@@H](B(O)O)C)OC': 20,\n",
       " 'C[N@@H+]1CCN(CC1)C(=O)N[C@H](C(=O)N[C@H](CCS(=O)(=O)c1ccccc1)CCc1ccccc1)Cc1ccccc1': 21,\n",
       " 'Clc1ccc(cc1)S(=O)(=O)Nc1ccc(cc1C(=O)O)Br': 22,\n",
       " 'COc1ccc(cc1)c1ccc(cc1)S(=O)(=O)c1ccccc1CC(=O)O': 23,\n",
       " '[NH3+][C@H](C(=O)O)C(C)C': 24,\n",
       " 'COCCOCCOCCOCCOCCOc1ccc(cc1)C(CC(C)(C)C)(C)C': 25,\n",
       " 'C[NH+]1CCN(CC1)c1ccc(c(c1)Nc1nccc(n1)c1cc(cn1C)C(=O)N)OC(F)(F)F': 26,\n",
       " 'Brc1ccc2c(c1)c1nc([nH]c(=O)c1o2)c1ccc(cc1Cl)NCC1CC[NH2+]CC1': 27,\n",
       " 'COc1nc(nc(c1F)C)N1C[C@@H]2[C@](C1)(NC(=N)N(C2=O)C)c1c(F)cccc1F': 28,\n",
       " 'CCCS(=O)(=O)Nc1ccc(c(c1F)C(=O)C1=CN=C2[C@H]1C=C(C=N2)c1ccc(cc1)OC)F': 29,\n",
       " 'OC1=C(C2=Nc3ccc(cc3S(=O)(=O)N2)NS(=O)(=O)C)C(=O)N([N@@H+]2[C@H]1CCC2)CCC(C)(C)C': 30,\n",
       " 'OC(=O)c1ccccc1SCc1ccc(c(c1)Cl)Cl': 31,\n",
       " 'C#CCCCn1c(Cc2cc(OC)ccc2OC)nc2c1nc(F)nc2N': 32,\n",
       " '[NH3+]CCCC[C@@H]1NC(=O)[C@H](CC(C)C)NC(=O)[C@H](CCCNC(=N)N)NC(=O)[C@H](CCCNC(=O)N)NC(=O)[C@@H](NC(=O)[C@H](CO)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@H]2N(C(=O)CNC1=O)O2)CCC(=O)N)Cc1ccccc1)CC(=O)O)CC(C)C)[C@H](O)C': 33,\n",
       " 'CNC(=O)[C@H](Cc1ccccc1)NC(=O)[C@@H](CC(=O)NO)CC(C)C': 34,\n",
       " 'COc1cc(OC)c(c(c1Cl)N1Cc2cnc(nc2N(C1=O)C1CCN(CC1)C(=O)C)Nc1ccccc1)Cl': 35,\n",
       " 'O[C@@H](Cn1nc(c2c1CCN(C2)C(=O)C)c1ccc(c(c1)SCC(=O)N1CCCC1)C(F)(F)F)C[N@@H+]1CC[C@@H](CC1)C(O)(C)C': 36,\n",
       " 'CCc1sc2c(c1c1cc(N3CC[N@H+](CC3)C)c(c(c1C)Cl)O)c(ncn2)O[C@@H](C(=O)O)Cc1ccccc1': 37,\n",
       " 'C[N@H+]1CCC[C@H]1CCn1cc(c2c1cccc2)C1=C(C(=O)NC1=O)C1=c2ccccc2=NC1': 38,\n",
       " 'OC(=O)C[C@H]1[C@H](C[NH2+][C@@H]1C(=O)O)C(C)C': 39,\n",
       " 'O=C(N[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)O)Cc1ccc(cc1)O)Cc1ccccc1)Cc1ccc(cc1)O)OCc1ccccc1': 40,\n",
       " 'O=C(N[C@@H]([P@H](=O)O)Cc1ccccc1)OCc1ccccc1': 41,\n",
       " 'CCCCC/C=C\\\\C/C=C\\\\C=C\\\\C=C\\\\C[C@H](CCCC(=O)O)O': 42,\n",
       " 'Cc1nc2c(O)cccc2c(=O)[nH]1': 43,\n",
       " '[NH3+]Cc1ccc(cc1)C(=O)N[C@H]1Cc2cccc(c2OB1O)C(=O)O': 44,\n",
       " 'C[N@@H+]1CC[N@H+](CC1)Cc1ccc(cc1C(F)(F)F)NC(=O)Nc1ccc(cc1)Oc1ccnc(n1)N': 45,\n",
       " 'Clc1ccc2c(c1)N(CC2(C)C)C(=O)C[N@@H+]1CC[NH2+][C@@H](C1)C': 46,\n",
       " 'CC(=O)c1cc(c2n1cccc2)c1ccccn1': 47,\n",
       " 'C[C@@H]1Cn2ncc(c2CN1c1ccnc2c1cc[nH]2)c1ccc(cc1)S(=O)(=O)N(C)C': 48,\n",
       " '[NH3+]CCCC[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N)CCCC[NH3+])Cc1ccccc1)CO)Cc1ccccc1)C)CC1=NC=NC1)CC1=NC=NC1)NC(=O)[C@@H](C[P@@](=O)([C@H](CCc1ccccc1)[NH3+])O)CC(C)C': 49,\n",
       " 'ONC(=O)CCCCC[NH3+]': 50,\n",
       " 'CCNC(=O)N1CCC(CC1)Nc1ncc(c(n1)C1=c2ccccc2=NC1)Cl': 51,\n",
       " 'O=C(N[C@H]([C@@H]([NH2+]CC(=O)O)O)CSC1=C(C)C(=O)c2c(C1=O)cccc2)CC[C@@H](C(=O)O)[NH3+]': 52,\n",
       " 'OC[C@H]1O[C@@H]2C[C@@H]1O[P@](=O)(O)OC[C@H]1O[C@@H](n3c(=O)nc([C@@H]4N2C(=O)NC(=O)[C@]4(C)O)c(C)c3)C[C@@H]1O': 53,\n",
       " 'O=c1c2cc(ccc2oc2c1cccc2)S(=O)(=O)Nc1nnn[nH]1': 54,\n",
       " 'O[C@@H]1[C@@H](CO[P@](=O)(C[P@](=O)(OP(=O)(O)O)O)O)O[C@H]([C@@H]1O)n1cnc2c1ncnc2N': 55,\n",
       " 'CCC(Nc1ncnc2c1cc(OC)c(c2OC)OC)CC': 56,\n",
       " 'Cc1cccnc1NC(P(=O)(O)O)P(=O)(O)O': 57,\n",
       " 'OC[C@H]1O[C@@H]2O[C@@H]3[C@@H](CO)O[C@@H]([C@@H]([C@H]3O)O)O[C@@H]3[C@@H](CO)O[C@@H]([C@@H]([C@H]3O)O)O[C@@H]3[C@@H](CO)O[C@@H]([C@@H]([C@H]3O)O)O[C@@H]3[C@H](O[C@H](O[C@@H]4[C@H](O[C@H](O[C@@H]5[C@H](O[C@H](O[C@H]1[C@@H]([C@H]2O)O)[C@H](O)[C@H]5O)CO)[C@H](O)[C@H]4O)CO)[C@H](O)[C@H]3O)CO': 58,\n",
       " 'Clc1cccc(c1)N1C(=O)[C@H](C=C([C@H]1C)c1c(C)noc1C)C(=O)NCc1ccc(cc1)S(=O)(=O)C': 59,\n",
       " 'NC(=O)c1cc2c(nc1N)nc(nc2C1=NC=CC1)C(C)(C)C': 60,\n",
       " 'C[N@@H+]1[C@@H]2CC[C@H]1C[C@H](C2)OC(c1ccccc1)c1ccccc1': 61,\n",
       " 'O=Cc1c(COP(=O)(O)O)c(/N=N/c2cc3c(c(c2)S(=O)(=O)O)cc(cc3S(=O)(=O)O)N(=O)=O)nc(c1O)C': 62,\n",
       " 'C[N@@H+]1CCC[C@H]1c1cccnc1': 63,\n",
       " 'OC[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)O)C(C)C)CC(C)C)Cc1ccccc1)NC(=O)[C@@H](NC(=O)[C@H]1[NH2+]CCC1)CC(C)C': 64,\n",
       " 'CO[C@H]([C@H](/C(=C\\\\C(=O)N)/OC)C)/C=C/c1csc(n1)c1csc(n1)[C@H](/C=C/C=C/C(C)C)C': 65,\n",
       " 'Fc1cc(F)c(c(c1)c1[nH]ncc1)O': 66,\n",
       " 'OC[C@H]1O[C@H](O)[C@@H]([C@H]([C@@H]1O[C@@H]1O[C@H](CO)[C@H]([C@@H]([C@H]1O)O)O)O)O': 67,\n",
       " 'O=C(c1ccccc1)CSc1[nH]c2c(n1)c(=O)[nH]c(n2)N': 68,\n",
       " 'Fc1cccc(c1)C(=O)N/N=C/c1ccc(o1)Sc1ncccn1': 69,\n",
       " 'OC[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N1CCC[C@H]1C(=O)N[C@H](C(=O)O)CO)Cc1ccc(cc1)OP(=O)(O)O)C)NC(=O)[C@H](Cc1ccccc1)[NH3+]': 70,\n",
       " 'CCCC(=O)C': 71,\n",
       " 'O=C1C=C(N=C([C@H]1O)C(=O)O)c1ccccc1C': 72,\n",
       " 'O=C1NC(=O)C(C(=O)N1)(C)c1ccc(cc1)Oc1ccccc1': 73,\n",
       " 'CCOc1ccc(cc1CSc1ccsc1C(=O)NS(=O)(=O)c1ccccc1)C(F)(F)F': 74,\n",
       " 'COc1cc(ccc1Nc1ncc2c(n1)c(ncc2)NCC(C)(C)C)c1cnn(c1)C': 75,\n",
       " 'O=C([C@@H](n1c(nc2c1cc(F)c(c2)F)c1ccc(cc1)Cl)C1CCCCC1)Nc1ccc(cc1Cl)C(=O)O': 76,\n",
       " '[NH3+]CCCC[C@@H](C(=O)N[C@H](C(=O)O)[C@H](O)C)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@H](C(C)C)NC(=O)[C@@H]1CCCN1C(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@H](CC(=O)N)[NH3+])Cc1ccccc1)CC(=O)O)CC(=O)N)Cc1ccc(cc1)O)CCCNC(=N)N': 77,\n",
       " 'CC[NH+](CCN(C(=O)Cn1c(SCc2ccc(cc2)F)nc(=O)c2c1CCC2)C[C@]12C[C@](C1)(C2)c1ccc(cc1)C(F)(F)F)CC': 78,\n",
       " 'CC[C@@H]([C@@H](C(=O)O)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H]1CCCN1C(=O)[C@@H](NC(=O)[C@@H]1CCCN1C(=O)[C@H](CCCNC(=N)N)[NH3+])CCCNC(=N)N)CC(=O)O)CC(=O)O)CC(C)C)CCC(=O)O)C': 79,\n",
       " 'O=C1N(CCc2c1cc([nH]2)c1ccnc(n1)NC1CCOCC1)Cc1ccccc1': 80,\n",
       " 'COc1cccc(c1)[C@H](c1sc(nc1N)Nc1ccccc1)O': 81,\n",
       " 'Fc1ccc2c(c1)c(OC#CC1CC1)c(c(=O)[nH]2)C(C)C': 82,\n",
       " 'N#C[C@@]1(NC(=O)C2([NH3+])CCCCC2)C[C@@H]1c1ccc(cc1)c1ccc(cc1)S(=O)(=O)N1CC[N@H+](CC1)C': 83,\n",
       " 'OC(=O)CCC[N+]1(C)CCCC1': 84,\n",
       " 'ONC(=O)c1ccc(cc1)CN(C(=O)c1ccc(cc1)N(C)C)CC(=O)Nc1ccc(cc1)C': 85,\n",
       " 'N#CCC[N@@H+](CCP(=O)(O)O)CCn1cnc2c1nc(N)[nH]c2=O': 86,\n",
       " 'Clc1ccccc1NC(=O)c1cnn2c1cccc2': 87,\n",
       " 'C/C(=C\\\\C=C/C(=C\\\\C(=O)O)/C)/C=C/C1=C(C)CCCC1(C)C': 88,\n",
       " '[NH3+]CC(=O)N[C@H]1CSSC[C@@H]2NC(=O)[C@H](CCCNC(=N)N)NC(=O)[C@H]3N(C(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC1=O)CSSC[C@H](NC(=O)[C@H](CCCNC(=N)N)NC(=O)[C@@H](NC(=O)[C@@H](NC2=O)C)CC1=CN=C2[C@@H]1C=CC=C2)C(=O)N)CO)CC(=O)O)CCC3': 89,\n",
       " 'COC(=O)c1c(O)cc(c(c1CCc1nccn1Cc1ccccc1)Cl)O': 90,\n",
       " '[NH3+]CCCC[C@@H](C(=O)N1CCC[C@H]1C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)NCC(=O)N[C@H](C(=O)N[C@H](C(=O)O)CCSC)CC(C)C)Cc1ccccc1)Cc1ccccc1)CCC(=O)N)CCC(=O)N)NC(=O)[C@@H]1CCCN1C(=O)[C@H](CCCNC(=N)N)[NH3+]': 91,\n",
       " 'Brc1cc2c(s1)c(cnc2N)C(=O)N': 92,\n",
       " 'C[C@]12[NH2+][C@@H](c3c2cccc3)Cc2c1cccc2': 93,\n",
       " 'OC1=CCC(=CN1Cc1ccccc1)c1c(C)noc1C': 94,\n",
       " 'CO[C@@H](C(=O)N[C@@H](C(=O)N1[C@H]2C[C@H](O)[C@H](C[C@H]2C[C@H]1C(=O)NCCC1=CCN(C1)C(=N)N)O)[C@@H](C(C)C)Cl)COS(=O)(=O)O': 95,\n",
       " 'CCCC[C@@H](C(=O)N[C@H](C(=O)N[C@@H]([C@H](CC)C)C(=O)N)CCCNC(=N)N)NC(=O)[C@H](C(C)C)NC(=O)CNC(=O)C': 96,\n",
       " 'Clc1ccc(cc1)c1nn(c2c1c(N)ncn2)C(C)(C)C': 97,\n",
       " 'CC[C@@H]([C@@H]1NC(=O)[C@H](CC2=NC=NC2)NC(=O)[C@@H](NC(=O)[C@H](CO)NC(=O)CNC(=O)[C@H](CC(=O)N)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@H](CSSC[C@H](NC1=O)C(=O)O)[NH3+])CCC(=O)N)CC(C)C)[C@H](CC)C)CC(=O)N)[C@H](O)C)CC1=CN=C2[C@@H]1C=CC=C2)C': 98,\n",
       " 'ONC(=O)[C@@H](c1ccc(cc1)Br)NC(=O)OC(C)(C)C': 99,\n",
       " 'OC[C@H]([C@H]([C@@H]1O[C@](C[C@@H]([C@H]1NC(=O)C)O)(O[P@](=O)(OC[C@H]1O[C@H]([C@@H]([C@@H]1O)O)n1ccc(=N)[nH]c1=O)O)C(=O)O)O)O': 100,\n",
       " 'OC[C@H]1O[C@@H](O)[C@@H]([C@H]([C@@H]1O[C@@H]1O[C@H](CO)[C@H]([C@@H]([C@H]1O)O)O)O)O': 101,\n",
       " 'Clc1ccc2c(c1)[C@H](c1ccccc1)[C@H](C(=O)N2)c1onc(c1)C': 102,\n",
       " 'O[C@@H]1[C@@H](CO[P@@](=O)(O[P@](=O)(OC[C@H]2O[C@@H]([C@@H]([C@@H]2O)O)O)O)O)O[C@H]([C@@H]1O)n1cnc2c1ncnc2N': 103,\n",
       " 'CCCCCCCCCCC[C@H](CC(=O)O[C@@H]1[C@@H]([NH3+])[C@H](O[C@@H]([C@H]1O)CO)O[P@@](=O)(O[P@@](=O)(OC[C@H]1O[C@H]([C@@H]([C@@H]1O)O)n1ccc(=O)[nH]c1=O)O)O)O': 104,\n",
       " 'CCc1ccc2c(c1)[C@@H]([NH2+]C[C@H]([C@@H]1Cc3cccc(c3)CCCCn3cc(C(=O)N1)ccc3=O)O)CC1(O2)CCC1': 105,\n",
       " 'COc1ccc(cc1)CC(=O)Nc1ncn(c1)[C@@H]1C[C@@H](C1)NC(=O)C': 106,\n",
       " '[NH3+]CC(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)NCC(=O)N[C@H](C(=O)NCC(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N1CCC[C@H]1C(=O)N[C@H](C(=O)O)CC1=CN=C2[C@@H]1C=CC=C2)CC(=O)N)CC(=O)N)CCC(=O)O)CO)CO)CC1=CN=C2[C@H]1C=CC=C2)CO)CC(=O)O)CO)C': 107,\n",
       " 'OC[C@@H](C(=O)O)NC(=O)[C@H](Cc1ccc(cc1)OP(=O)(O)O)NC(=O)[C@@H]1CCC[NH2+]1': 108,\n",
       " 'FC(CN(S(=O)(=O)c1ccccc1)c1ccc(cc1)C(C(F)(F)F)(C(F)(F)F)O)(F)F': 109,\n",
       " 'O[C@H]1C[C@@H](C(=O)N1O)P(=O)(O)O': 110,\n",
       " 'O=C(c1ccc(cc1)O)N1CCOCC1': 111,\n",
       " 'O=c1[nH]c(=O)c2c([nH]1)N(C[C@@H]([C@@H]([C@@H](COP(=O)(O)O)O)O)O)c1c(N2)cc(c(c1)C)C': 112,\n",
       " 'OC[N@@H+]1CC[C@]2(CC1)OCc1c(O2)ccc(c1)Br': 113,\n",
       " 'OC[C@H]1O[C@@H](Sc2ccc(cc2)C)[C@@H]([C@H]([C@H]1O)n1nnc(c1)c1c(F)c(F)c(c(c1F)F)NC)O': 114,\n",
       " 'O=C([C@@H]1CCCN1C(=O)C[NH2+]C1CCCCCC1)NCc1ccc(cc1)C(=N)N': 115,\n",
       " 'Oc1cc(/C=C/c2ccc(cc2)B(O)O)cc(c1)O': 116,\n",
       " 'C[N@@H+]1CC[N@H+](CC1)Cc1ccc(cc1)C(=O)Nc1ccc(c(c1)Nc1scc(n1)c1cccnc1)C': 117,\n",
       " 'Fc1ccc2c(c1)CC=N2': 118,\n",
       " 'O=C1COc2c(N1)c(O)ccc2[C@H](C[NH2+]C(Cc1ccccc1C)(C)C)O': 119,\n",
       " '[NH3+]CCCC[C@@H]1NC(=O)[C@@H]2CSSC[C@H](NC(=O)[C@H](CCCNC(=N)N)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@H](CSSC[C@@H](C(=O)N[C@H](C(=O)N2)CC(=O)N)[NH3+])NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@H]2N(C(=O)[C@@H](NC1=O)C)CCC2)CCC(=O)O)[C@H](O)C)Cc1ccccc1)CC(C)C)Cc1ccc(cc1)O)CC1=CN=C2[C@H]1C=CC=C2)C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N)CC1=NC=NC1)CCC(=O)N)CC(C)C': 120,\n",
       " 'O=C(N1CCC[C@H]1c1[nH]c(=O)c(c(n1)C(=O)NC[C@@H]1Cc2c(O1)cccc2)O)CSc1ccccc1Cl': 121,\n",
       " 'OC(=O)C(CC(=O)O)(CC(=O)O)O': 122,\n",
       " 'Oc1ccc(cc1)C(=O)N/N=C/c1ccc(cc1)C(C)C': 123,\n",
       " 'COc1ccccc1Nc1scc(n1)c1sc(nc1C)NC(=O)C': 124,\n",
       " 'O=C(Cc1ccccc1)Nc1cccc(c1)c1nc2n(c1c1ccnc(n1)Nc1ccc(cc1)N1CCOCC1)ccs2': 125,\n",
       " 'Oc1ccc(cc1C)c1nc(=O)[nH]c(c1)c1ccccc1': 126,\n",
       " 'OC[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)O)CC(C)C)CCCNC(=N)N)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H]1CCCN1C(=O)[C@H](CO)[NH3+])CO)COP(=O)(O)O)C)CCCC[NH3+]': 127,\n",
       " 'NS(=O)(=O)c1ccc2c(c1)CCC2': 128,\n",
       " 'CSCC[C@@H](C(=O)N1CCC[C@H]1C(=O)N[C@H](C(=O)N[C@H](C(=O)O)COP(=O)(O)O)CCSC)NC(=O)[C@H](Cc1ccc(cc1)O)NC(=O)CNC(=O)[C@H](CC(=O)O)[NH3+]': 129,\n",
       " 'Fc1ccc2c(c1)N(Cc1ccccc1)C(=O)c1c(N2)cccc1': 130,\n",
       " '[NH3+]CCCC[C@@H](C(=O)N[C@H](C(=O)N)CO)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@H](C(C)C)NC(=O)CNc1ccc2c(c1)NNc1c(CC2)ccc(c1)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@H](CO)[NH3+])C)CCCNC(=N)N)C)CC1=NC=NC1)CC(C)C)CCCNC(=N)N': 131,\n",
       " 'O=c1sn(c2c1cc(cc2)S(=O)(=O)N)C': 132,\n",
       " '[NH3+]C[C@H](c1ccccc1)NC(=O)C[C@H]1CNC(=O)c2n1cc(c2)c1ccnc(c1)F': 133,\n",
       " 'CCCCCCCCCCCCCO[P@@](=O)(OC[C@H]1O[C@H]([C@@H]([C@@H]1O)O)n1cnc2c1ncnc2N)O': 134,\n",
       " 'OP(=O)(C(c1ccc(cc1)CC(n1nnc2c1cccc2)(c1ccccc1)Cc1ccc(cc1)C(P(=O)(O)O)(F)F)(F)F)O': 135,\n",
       " 'OC[C@H]([C@H]([C@@H]1OC(=C[C@@H]([C@H]1NC(=O)C)O)C(=O)O)O)O': 136,\n",
       " 'OC[C@H]1O[C@H]([C@@H]([C@H]([C@@H]1O)O)O)c1cc(O)c(cc1O)Br': 137,\n",
       " 'CNC(=O)[C@@H]1C[C@H](CN1C(=O)C)O': 138,\n",
       " '[NH3+]CCCC[C@@H](C(=O)N[C@H](C(=O)NCc1ccc(cc1)C(=N)N)CCCNC(=N)N)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@H](CCCNC(=N)N)[NH3+])CCCNC(=N)N)CCCNC(=N)N)CCCNC(=N)N': 139,\n",
       " 'COc1ccc2c(c1CN1C(=O)[C@H](CN(c3c1cccc3)C(=O)c1ccc(cc1)N)NC(=O)[C@@H]([NH2+]C)C)cccc2': 140,\n",
       " 'COC(=O)[C@@H]1C[C@@H](O[C@H]1[C@@H](NC(=O)C)CC(C)C)C(=O)O': 141,\n",
       " 'CSCC[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)O)C)[C@H](CC)C)Cc1ccc(cc1)O)CC(=O)O)Cc1ccccc1)C)C)Cc1ccccc1)[NH3+]': 142,\n",
       " 'COc1cc(ccc1Nc1ncc(c(n1)Nc1ccccc1S(=O)(=O)C(C)C)Cl)N1CCC(CC1)[N@@H+]1CC[N@@H+](CC1)C': 143,\n",
       " 'N#C/C=C/c1cc(C)c(c(c1)C)Oc1nc(N[C@@H]2CC[N@H+](CC2)Cc2ccc(cc2)S(=O)(=O)N)nc2c1scc2': 144,\n",
       " 'OCc1cc2n(c1N[C@H]([C@@]([S@@](=O)O)(COC(=O)NCC[NH3+])C)C(=O)O)cccc2': 145,\n",
       " 'CCCc1sc(nc1CSc1nc(N)cc(n1)N)c1ccc(c(c1)OCCNS(=O)(=O)C)OC': 146,\n",
       " 'CCCn1cnc(c1)CCNC(=O)Nc1nc2c(s1)cc(cc2)c1cncc(c1)OC': 147,\n",
       " 'OC[C@@H](C(=O)NCC(=O)O)NC(=O)[C@@H]1CCCN1C(=O)CNC(=O)[C@@H](NC(=O)CNC(=O)[C@@H]1CCCN1C(=O)[C@H](CO)[NH3+])CCC(=O)O': 148,\n",
       " 'C[NH+](CC(=O)Nc1ccc2c(c1)c1ccccc1c(=O)[nH]2)C': 149,\n",
       " 'OC(=O)CCNC(=O)[C@@H](C(COP(=O)(O)O)(C)C)O': 150,\n",
       " 'Nc1[nH]c(=O)c2c(n1)n(cn2)[C@@H]1O[C@@H]([C@H]([C@H]1O)O)CO[P@](=O)(O[P@](=O)(O[P@](=O)(O[P@@](=O)(O[P@@](=O)(OC[C@H]1O[C@H]([C@@H]([C@@H]1O)O)n1cnc2c1ncnc2N)O)O)O)O)O': 151,\n",
       " 'C[NH+](C[C@@H](COc1ccc(cc1)Nc1nccc(n1)c1cnn2c1ccc(n2)N1CCOCC1)O)C': 152,\n",
       " 'Nc1ccc2c(c1)CN(CC2)C(=O)OC(C)(C)C': 153,\n",
       " 'OCCCO[P@@](=O)(O[C@@H]1[C@@H](CO[P@@](=O)(O[C@H]2C[N@H+](C[C@@H]2CO[P@](=O)(O[C@@H]2[C@@H](CO)O[C@H]([C@@H]2OC)n2cnc3c2[nH]c(=N)[nH]c3=O)O)CC2=CN=C3C2=NC=N[C@H]3N)O)O[C@H]([C@@H]1OC)n1cnc2c1[nH]c(=N)[nH]c2=O)O': 154,\n",
       " 'CC[C@@H]([C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N)C(C)C)[C@H](CC)C)CC1=NC=NC1)NC(=O)[C@@H](NC(=O)[C@H]([C@H](O)C)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H]1CCCN1C(=O)[C@@H]([NH3+])C)CC(=O)O)Cc1ccc(cc1)OP(=O)(O)O)CO)C': 155,\n",
       " 'OC[C@H]1[N@H+](C[C@@H]([C@H]([C@@H]([C@@H](C(=O)O)O)O)O)O)C[C@H]([C@@H]1O)O': 156,\n",
       " 'OC[C@H]1O[C@@H](O[C@@H]2[C@@H](CO)O[C@@H]([C@@H]([C@H]2O)NC(=O)C)O)[C@@H]([C@H]([C@@H]1O[C@@H]1O[C@H](CO)[C@H]([C@@H]([C@H]1NC(=O)C)O)O[C@@H]1O[C@H](CO)[C@H]([C@@H]([C@H]1NC(=O)C)O)O)O)NC(=O)C': 157,\n",
       " 'Nc1sc2c(-c3c(C2)cccc3OCP(=O)(O)O)n1': 158,\n",
       " 'OC[C@H]1C[NH2+][C@@H](CS[C@@H]1[C@@H]1[NH2+][N@H+]2[C@H](C1)CCSC2)C(=O)O': 159,\n",
       " '[NH3+]Cc1ccc(cc1)c1cc2ccoc2c(c1)S(=O)(=O)Nc1ccsc1C(=O)N[C@H](C(=O)O)CCCNC(=N)N': 160,\n",
       " 'Clc1cnc2n1cc(nc2Nc1ccc(cc1)N1CCOCC1)c1cccnc1': 161,\n",
       " 'O[C@@H]1[C@@H](CO[P@](=O)(OP(=O)(O)O)O)O[C@H]([C@@H]1O)n1cnc2c1[nH]c(=N)[nH]c2=O': 162,\n",
       " 'CCCOc1cc(OCCCC[NH+](C)C)cc(c1)Oc1cc2c(cc1NS(=O)(=O)c1ccc(c(c1)OC)OC)n(c(=O)n2C)C': 163,\n",
       " 'O=C(N[C@H](C(=O)O)CO[P@](=O)(N[C@H](C(=O)O)CCC(=O)O)O)CC[C@@H](C(=O)O)[NH3+]': 164,\n",
       " 'CN(C(=N)N)CC[C@H]([NH3+])CC(=O)N[C@H]1C=C[C@@H](O[C@@H]1C(=O)O)n1ccc(=O)[nH]c1=O': 165,\n",
       " 'O[C@H]1CC[N@@H+](C1)Cc1[nH]c(=O)c2c(n1)c1cc(Br)ccc1o2': 166,\n",
       " 'COc1ccc(cc1)[C@@]1(NC(=N)c2c1sc(n2)C)c1cccc(c1)c1cncnc1': 167,\n",
       " 'O/N=C\\\\1/CCc2c1ccc(c2)c1[nH]c(nc1c1ccncc1)c1ccc(cc1)OCC[NH+](C)C': 168,\n",
       " 'O=C1NC(=O)N=C(C1)C[NH2+]CCCC[NH2+]Cc1cccc(c1)C(=O)O': 169,\n",
       " 'CCOc1ccccc1c1onc(n1)c1ccncc1': 170,\n",
       " 'OC[C@@H](C(=O)N[C@H](C(=O)O)C(C)C)NC(=O)[C@H]([C@H](O)C)NC(=O)[C@H](C(C)C)NC(=O)[C@@H](NC(=O)[C@H](Cc1ccc(cc1)O)NC(=O)[C@@H](NC(=O)C[NH3+])CO)CC(C)C': 171,\n",
       " '[NH3+]CCCC[C@@H]1NC(=O)[C@@H](NC(=O)CNC(=O)[C@H](Cc2ccc(cc2)O)NC(=O)[C@H](CC(C)C)NC(=O)[C@H](CCCNC(=N)N)NC(=O)[C@H](CSSC[C@H](NC(=O)CNC(=O)[C@@H](NC(=O)[C@@H](NC1=O)[C@H](CC)C)CC1=NC=NC1)C(=O)NCC(=O)O)NC(=O)C[NH3+])Cc1ccccc1': 172,\n",
       " 'CCN1[C@@H](NC(C)C)Nc2c([C@@H]1O)scc2c1cccnc1': 173,\n",
       " '[NH3+]CCNC(=O)c1ccnc(c1)c1nccc(c1)C(=O)O': 174,\n",
       " 'COc1ncc(c(n1)OC)c1nc2c(n1C(C)C)[C@@H](N(C2=O)c1cc(Cl)ccc1C)c1ccc(cc1C)Cl': 175,\n",
       " 'Nc1cc(NCCCc2ccc(cc2)F)cc(n1)C(F)(F)F': 176,\n",
       " 'CCCSc1c(F)c(F)c(c(c1F)F)S(=O)(=O)N': 177,\n",
       " 'OC[C@@H](C(=O)NCC(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)O)Cc1ccccc1)CCC(=O)O)Cc1ccc(cc1)O)CC(C)C)C)CC(=O)O)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@H]1[NH2+]CCC1)CCCNC(=N)N)CC(=O)O)CO)Cc1ccc(cc1)O': 178,\n",
       " 'Nc1ncnc2c1ncn2[C@@H]1O[C@@H](C[C@H]1O)CO[P@](=O)(O[C@@H]1[C@@H](COP(=O)(O)O)O[C@H]([C@@H]1O)n1cnc2c1ncnc2N)O': 179,\n",
       " 'N=C1SC[C@H]2[C@@](N1)(CN(C2)c1ncccn1)[C@@H]1C[C@H]1C(=O)NC[C@@H]1C[C@H]1C': 180,\n",
       " 'Fc1ccc(cc1)C1=CC[N@@H+](CC1)CCCc1nc2c(C)cccc2c(=O)[nH]1': 181,\n",
       " 'CC(=O)N[C@H]1[NH2+]C[C@@H]([C@@H]([C@@H]1O)O)C(=O)O': 182,\n",
       " '[NH3+]CCC(=O)N1CCC[C@H]1C(=O)N[C@H](C(=O)N[C@H]1CSSC[C@H](NC(=O)[C@H](CO)NC(=O)[C@@H](NC(=O)[C@H](CO)NC(=O)CCNC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC1=O)C(C)C)Cc1ccc(cc1)O)CCCNC(=N)N)CC1=CN=C2[C@H]1C=CC=C2)C(=O)N)Cc1ccc(cc1)O': 183,\n",
       " '[NH3+]Cc1ccc(cc1)C[C@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)c1cnccn1)CC(C)C)CC(C)C)CCS(=O)(=O)C': 184,\n",
       " 'FCCOc1cc(ccc1OC)c1nc(c(s1)CC)CSc1nc(N)cc(n1)N': 185,\n",
       " 'OC[C@H]1O[C@H]([C@@H]([C@@H]1O)O)N1C=CCC(=C1)C(=O)N': 186,\n",
       " 'COC(=O)[C@H](C(C)C)NC(=O)[C@H](C(C)C)NC(=O)[C@@H](C[C@@H]([C@@H](NC(=O)[C@@H](NC(=O)[C@@H]([NH3+])C)C)Cc1ccccc1)O)Cc1ccccc1': 187,\n",
       " 'Fc1ccc(c(c1)F)c1cc(nc2=C(N(CC=c12)O)c1c(Cl)cccc1Cl)C1CC[NH2+]CC1': 188,\n",
       " 'OC[C@H]1OC(=O)[C@@H]([C@H]([C@@H]1O)O)NC(=O)C': 189,\n",
       " 'O[C@@H]1[C@H](O)[C@H](O[C@H]1n1cc(F)c(=O)[nH]c1=O)COP(=O)(O)O': 190,\n",
       " 'COc1ccc(cc1)Cn1nnc(n1)c1cccc(c1)C(=O)NCc1ccc(cc1)C(=O)O': 191,\n",
       " 'CCCOc1cncc2n1c(nn2)c1cnn(c1c1ccc(cc1)CC)C': 192,\n",
       " 'O[C@@H]1[C@H](O)[C@H](O[C@H]1N1C=CC=C(C1)C(=O)N)CO[P@@](=O)(O[P@](=O)(OC[C@H]1O[C@H]([C@@H]([C@@H]1O)O)n1cnc2c1ncnc2N)O)O': 193,\n",
       " 'Cc1cc2nc(oc2c(c1)c1ccco1)N': 194,\n",
       " '[NH3+]CCCC[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@@H](CCC(=O)O)C=O)CC(C)C)CC(=O)O)CC(C)C)CCCNC(=N)N)CCCNC(=N)N)NC(=O)[C@H](C(C)C)NC(=O)[C@H]1[NH2+]CCC1': 195,\n",
       " 'CCCOc1ccc2c(c1)c(c[nH]2)C[C@H](C(=O)N[C@H](C(=O)NCc1ccc(cc1)C(=N)N)CCC(=O)N)NS(=O)(=O)CC': 196,\n",
       " '[NH3+]CCCC[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N1CCC[C@H]1C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N1CCC[C@H]1C(=O)O)CO)Cc1ccc(cc1)O)CC(=O)N)CCCNC(=N)N)C(C)C)Cc1ccc(cc1)O)CCSC)CC1=NC=NC1)[C@H](O)C)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@H](CCCNC(=N)N)[NH3+])C)CO)C': 197,\n",
       " 'CC(C[C@H](NC(=O)[C@@H](NC(=O)[C@H](Cc1cccnc1)NC(=O)C)CC(=O)O)CC(=O)N[C@H](C(=O)N[C@H](C(=O)C)CC(=O)O)CCC(C)C)C': 198,\n",
       " 'C[C@H]1OC(=N[C@@H]1C(=O)N(CC[C@H]1C=NC=N1)O)c1cccc(c1O)O': 199,\n",
       " 'CCC(Nc1cc(C)nc(c1C)Oc1c(C)cc(cc1C)C)CC': 200,\n",
       " 'OC[C@H]1O[C@@H](O)[C@@H]([C@H]([C@H]1O)O)NC(=O)C': 201,\n",
       " 'O[C@H]1C[C@@H](O[C@@H]1COP(=O)(O)O)n1ccc(=O)[nH]c1=O': 202,\n",
       " 'O=C1NCC(=O)NCC(=O)N[C@H]2CCC[N+]3=C(O[Fe]456(O[N@](CCC[C@@H](C(=O)NC1)NC(=O)[C@H](CCC[N@@](O6)[C@@H](O5)C)NC2=O)[C@H](C)O4)O3)C': 203,\n",
       " 'COc1cc(/C=C/C(=O)O)ccc1O': 204,\n",
       " 'CC(C[C@@H]1NC(=O)[C@H](CCCCNC(=O)CNC(=O)[C@@H](NC1=O)Cc1ccccc1)NC(=O)[C@@H](NC(=O)C)CCCNC(=N)N)C': 205,\n",
       " 'Cc1nnc(s1)N1CCN(CC1)c1ncnc2c1cc(s2)CC(F)(F)F': 206,\n",
       " 'CC(C[N@@H+]1CCc2c([C@H]1c1ccc(cc1)/C=C/C(=O)O)cc(c(c2)O)C)C': 207,\n",
       " 'CC#CCn1c2C(=O)N(Cc3ccc(cc3)C#N)[C@H]3N(c2nc1N1CCC[C@H](C1)[NH3+])CCN3': 208,\n",
       " 'OC[C@H]1O[C@H](O[C@@H]2[C@@H](CO)O[C@@H]([C@@H]([C@H]2O)O)O)[C@@H]([C@H]([C@@H]1O[C@H]1O[C@H](CO)[C@H]([C@@H]([C@H]1O)O)O)O)O': 209,\n",
       " 'OC(=O)CNC(=O)[C@@H]1CCCN1C(=O)[C@@H]1CCCN1C(=O)CNC(=O)[C@@H]1CCCN1C(=O)[C@@H]1CCCN1C(=O)CNC(=O)[C@@H]1CCCN1C(=O)[C@@H]1CCC[NH2+]1': 210,\n",
       " 'N#CC1=C(N)OC2=C([C@]31c1cc(cc(c1CC3(C)C)F)N1CC[C@H](CC1)CC(=O)O)[C@H](C)N=N2': 211,\n",
       " 'CCCCCCCC(=O)O[C@@H]1[C@@H](OC(=O)[C@H](CC)C)[C@H]([C@H]2[C@H]1[C@@](C)(OC(=O)C)C[C@@H]([C@]1([C@H]2O[C@H]([C@@]1(C)O)O)O[C@@H](CCC)O)O[C@H](CCC)O)C': 212,\n",
       " 'OCCOc1cc2ncnc(c2cc1S(=O)(=O)C(C)(C)C)Nc1ccc2c(c1)ncs2': 213,\n",
       " 'OC[C@H]1O[C@@H](O[C@@H]2[C@@H](CO)O[C@H]([C@@H]([C@H]2O)O)O[C@@H]2[C@@H](CO)O[C@H]([C@@H]([C@H]2O)O)O)[C@@H]([C@H]([C@@H]1O[C@@H]1O[C@H](CO)[C@H]([C@@H]([C@H]1O)O)O[C@@H]1O[C@H](CO)[C@H]([C@@H]([C@H]1O)O)O)O)O': 214,\n",
       " 'O=c1[nH]cnc2c1ccnc2n1ncc(c1)C1CC[NH2+]CC1': 215,\n",
       " 'CCNC(=O)Nc1ncc(c(c1)n1cccn1)C(=O)Nc1cccnc1': 216,\n",
       " 'CCCC(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H]1[C@@H](C)OC(=O)[C@@H](NC(=O)[C@H](Cc2ccc(c(c2)Cl)O)NC(=O)[C@@H](N2C(=O)[C@@H](NC(=O)[C@@H](NC1=O)CC(C)C)CC[C@H]2O)[C@H](O)C)C(C)C)[C@H](O)C)C': 217,\n",
       " 'CCO[C@@H]1C[C@H]2[N@H+](C1)C[C@H](N(C2)C(=O)[C@H](C1CCC(CC1)(F)F)NC(=O)[C@@H]([NH2+]C)C)C(=O)N[C@@H]1CCOc2c1cccc2': 218,\n",
       " 'COC(=O)c1c(O)cc(c(c1CCc1nccn1Cc1ccccc1)C(=O)OC)O': 219,\n",
       " 'CCCCCCCc1cc(O)c2c(n1=O)cccc2': 220,\n",
       " 'O=C([C@@H]1CCC[N@@H+](C1)C1CCN(CC1)C(=O)c1c2ccccc2cc2c1cccc2)N1CCOCC1': 221,\n",
       " 'CSCC[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)O)C)Cc1ccc(cc1)O)NC(=O)[C@H](CC(=O)O)[NH3+]': 222,\n",
       " 'COc1ccc(cc1)NC(=O)[C@@H](Nc1[nH]c(=O)c2c(n1)n(nc2)c1ccccc1C)C': 223,\n",
       " 'O=C1N[C@@H](c2c1cccc2)C1=c2ccccc2=N[C@H]1[C@@H]1[NH2+]CCC1': 224,\n",
       " 'NC(=N)NCCC[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N)CCCNC(=N)N)CCCNC(=N)N)NC(=O)[C@@H]1CCCN1C(=O)[C@@H]1CCCN1C(=O)[C@H](C(C)C)NC(=O)[C@@H]1CCCN1C(=O)[C@@H]1CCCN1C(=O)[C@@H]1CCCN1C(=O)[C@H](C(C)C)NC(=O)C': 225,\n",
       " 'S[C@H](C(=O)N[C@H]1CCS[C@@H]2N(C1=O)[C@@H](CCC2)C(=O)O)Cc1ccccc1': 226,\n",
       " 'N#Cc1ccc(cc1Br)COC(=O)[C@@H]1[NH2+]C[C@@]2(C1)C(=O)Nc1c2cccc1': 227,\n",
       " 'O[C@@H]1[C@H](O)[C@H](O[C@H]1n1ccc(=O)[nH]c1=O)COP(=O)(O)O': 228,\n",
       " 'OC[C@@H](C(=O)O)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@H]([C@H](O)C)NC(=O)[C@H](CCC(=O)N)[NH3+])C)CCCNC(=N)N)CCCC[N+](C)(C)C': 229,\n",
       " 'O=C1NC(=N)S/C/1=C/c1ccc(cc1)O': 230,\n",
       " 'CC(OC(=O)n1c(cc2c1cc(nc2)Nc1ccc(cc1)c1cnc(n1C)C)c1cnn(c1)C)C': 231,\n",
       " 'O=C(Nc1ccc(cc1)I)NC1CCCCC1': 232,\n",
       " 'OC[C@H]1O[C@@H](Sc2ccccc2N(=O)=O)[C@@H]([C@H]([C@H]1O)O)O': 233,\n",
       " '[NH3+]Cc1ccc(cc1)c1ncc(nc1c1ccc(cc1)c1cocc1)NCC1CC[NH2+]CC1': 234,\n",
       " 'CO[C@@H]1O[C@@H](C(=O)O)[C@H]([C@@H]([C@H]1OS(=O)(=O)O)O)O[C@H]1O[C@H](COS(=O)(=O)O)[C@H]([C@@H]([C@H]1NS(=O)(=O)O)OS(=O)(=O)O)O': 235,\n",
       " 'Nc1ccc(c(c1)C(=O)O)C(=O)O': 236,\n",
       " 'N=CCCSC[C@H]1O[C@H]([C@@H]([C@@H]1O)O)n1cnc2c1ncnc2N': 237,\n",
       " 'O[C@H]1CC[C@@H]2[C@]1(C)CC[C@H]1[C@H]2CCc2c1cc(O)c(c2)O': 238,\n",
       " 'COc1ccc(c(c1[C@H]1C[C@H]1NC(=S)Nc1ccc(cn1)C#N)O)C(=O)C': 239,\n",
       " 'OC(=O)C[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N1CCC[C@H]1C(=O)O)[C@H](O)C)CCC(=O)O)NC(=O)[C@H](CC1=CN=C2[C@H]1C=CC=C2)NC(=O)[C@@H](NC(=O)[C@H](CC(=O)N)[NH3+])CCCNC(=N)N': 240,\n",
       " 'CSCC[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)O)C)C)C)C)C)C)C)CC(C)C)C)[C@H](CC)C)NC(=O)C': 241,\n",
       " '[SeH]CC(=C)C(=O)N1CCC[C@H]1C(=O)O': 242,\n",
       " 'ONC(=O)[C@H](N(S(=O)(=O)c1ccc(c(c1)C)F)C)C': 243,\n",
       " 'CC[C@@H](CN(S(=O)(=O)c1ccc(cc1)N)C[C@H]([C@H](Cc1ccccc1)NC(=O)O[C@H]1CO[C@@H]2[C@H]1CCO2)O)C': 244,\n",
       " 'O=CC[C@@H](N1c2cc(C)c(cc2N(c2c1c(O)nc(n2)O)C[C@@H]([C@@H]([C@@H](CO[P@](=O)(O[P@@](=O)(OC[C@H]1O[C@H]([C@@H]([C@@H]1O)O)n1cnc2c1ncnc2N)O)O)O)O)O)C)c1ccccc1': 245,\n",
       " '[NH3+]CCC[C@@H](C(=O)O)NC(=O)[C@@H](N[P@](=O)(CNC(=O)OCc1ccccc1)O)CC(C)C': 246,\n",
       " 'Nc1ccc(cn1)c1nc2cc(sc2c(n1)N1CCOCC1)C[NH+]1CCN(CC1)S(=O)(=O)C': 247,\n",
       " 'CC(C[C@@H](C(=O)N[C@H](C(=O)O)C)NC(=O)[C@H](Cc1ccc(cc1)O)[NH3+])C': 248,\n",
       " 'Nc1nc(sc1[C@@H](c1cccc(c1)C(F)(F)F)O)Nc1ccccc1': 249,\n",
       " 'OC(=O)CCC1=NC2=CC=CCC2=N1': 250,\n",
       " 'O=Cc1cc(cnc1OC)c1cc(c2ncc(o2)C[N@@H+]2CC[N@H+](CC2)C(C)C)c2c(c1)[nH]nc2': 251,\n",
       " 'CCC(C(=O)N[C@H](C(=O)NC1(CCCC1)C(=O)NC(c1ccccc1)c1ccccc1)CCCNC(=N)N)(NC(=O)C(C)C)CC': 252,\n",
       " 'OC[C@H]1O[C@@H](O)[C@@H]([C@H]([C@H]1O)O)O': 253,\n",
       " 'CC(=O)NCCc1cccc(c1)c1cccc(c1)C(=O)O': 254,\n",
       " 'CSCC[C@@H](C(=O)N[C@@H](CC(=O)O)C=O)NC(=O)[C@@H](NC(=O)[C@H](CC(=O)O)[NH3+])CCC(=O)N': 255,\n",
       " 'O=C(Nc1nc2c(s1)cc(cc2)Sc1nnc2n1nc(cc2)c1ccc(cc1)F)NCC[NH+]1CCOCC1': 256,\n",
       " 'OC(=O)CC[C@@H](C(=O)O)NC(=O)c1ccc(cc1)N(Cc1cnc2c(n1)c(N)nc(n2)N)C': 257,\n",
       " 'ONC(=O)N(CCOc1ccc(cc1)c1ccc(cc1)C#N)C': 258,\n",
       " 'C[C@@H]1N=NC(=C1CCCOc1cc(ccc1F)C(=O)O)C': 259,\n",
       " 'COc1cccc(c1)Nc1ncc(c(n1)OCC1CC[NH2+]CC1)c1ccc(cc1)C(=O)NCC(=O)Nc1ccccc1': 260,\n",
       " 'OC(=O)C[C@@H](/C=C/C[C@H](/C=C/[C@H]1[C@@H](C)CCCC1(C)C)C)C': 261,\n",
       " 'OC(=O)c1cccc(c1)CN1CCCn2c(C1=O)c(CCCOc1cc(C)c(c(c1)C)Cl)c1c2c(ccc1)c1c(C)nn(c1C)C': 262,\n",
       " 'O=C(N1CCN(CC1)C(=O)C)Cc1ccc(cc1)Nc1ncc(c(n1)Nc1ccc(cc1)C(=O)Nc1ccccc1Cl)F': 263,\n",
       " 'C[C@H]([NH3+])C(=O)O': 264,\n",
       " 'O=c1cc(oc2c1cccc2c1cccc2c1sc1c2cccc1)N1CCOCC1': 265,\n",
       " 'CCNC(=O)Nc1nc(c(s1)c1ccc[nH]1)CO': 266,\n",
       " 'CO[C@@H]1[C@@H](OC)[C@H](O[C@@H]2[C@@H](COS(=O)(=O)O)O[C@@H]([C@@H]([C@H]2OS(=O)(=O)O)OS(=O)(=O)O)O[C@@H]2[C@@H](O[C@H]([C@@H]([C@H]2OC)OS(=O)(=O)O)O[C@@H]2[C@@H](COS(=O)(=O)O)O[C@@H]([C@@H]([C@H]2OS(=O)(=O)O)OS(=O)(=O)O)OC)C(=O)O)O[C@@H]([C@H]1O[C@H]1O[C@H](COS(=O)(=O)O)[C@H]([C@@H]([C@H]1OS(=O)(=O)O)OC)OC)C(=O)O': 267,\n",
       " 'CNC(=O)c1ccccc1Nc1nc(ncc1Cl)Nc1ccc(cc1OC)N1CCOCC1': 268,\n",
       " 'S=C1NC(=O)/C(=C/c2ccc(cc2)NCc2ccc(c(c2)C(=O)/N=C(\\\\C(=O)O)/C=C/C(=O)O)F)/S1': 269,\n",
       " 'CC(C[C@@H](C(=O)O)NC(=O)[C@@H](NC(=O)[C@H]([C@H](O)C)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@H]([C@H](O)C)NC(=O)[C@@H](NC(=O)[C@H]1[NH2+]CCC1)CC1=NC=NC1)CC(=O)N)CCC(=O)O)COP(=O)(O)O)C': 270,\n",
       " 'CSCC[C@@H](C(=O)N[C@H]([C@H](C[C@H](C(=O)NCCCC)C)O)CC(C)C)NC(=O)[C@@H](NC(=O)C)CC(C)C': 271,\n",
       " 'CCS(=O)(=O)Oc1ccccc1': 272,\n",
       " 'OCCCc1nnn(c1)[C@H]1C=C(C[C@H]([C@@H]1NC(=O)C)OC(CC)CC)C(=O)O': 273,\n",
       " 'COc1ccccc1NC(=O)c1ccc(cc1)NC(=O)CCc1nc2ccccc2c(=O)[nH]1': 274,\n",
       " 'Clc1ccc(c(c1)Cl)CS[C@H]1N=NC(=N1)N': 275,\n",
       " 'O=Cc1cc2n(c1N[C@H]([C@@](S(=O)=O)(COC(=O)Cc1ccccc1)C)C(=O)O)cccc2': 276,\n",
       " 'C[C@H]([NH3+])C(=O)Nc1cccc(n1)N': 277,\n",
       " 'O=C1N(C)[C@H](S)c2c(N1C)nc([nH]2)SCC[NH+]1CCCC1': 278,\n",
       " 'CC(c1ccc(cc1)[C@@H]1NC(=O)c2c(N1)nccc2)C': 279,\n",
       " 'Fc1cc(ccc1O)c1nn(c2c1c(N)ncn2)Cc1nc2cccc(c2c(=O)n1c1ccccc1C)C': 280,\n",
       " 'Nc1ccc(cn1)NC(=O)c1cc(ccc1C)C(=O)NCCC1CCCC1': 281,\n",
       " 'CNC(=O)c1nccc(c1)Oc1ccc(c(c1)F)NC(=O)Nc1cc(nn1c1ccc2c(c1)cccn2)C(C)(C)C': 282,\n",
       " 'OC[C@H]1O[C@H]([C@@H]([C@H]([C@@H]1O)O)O)n1ccc(=O)[nH]c1=O': 283,\n",
       " 'Clc1cccc(c1c1cc2cnc(nc2n(c1=O)C)Nc1ccc(c(c1)C)F)Cl': 284,\n",
       " 'C/C(=C\\\\CO[P@](=O)(OP(=O)(O)O)O)/CCC=C(C)C': 285,\n",
       " 'N=C1N[C@](C(=O)N1C)(c1ccsc1)c1cccc(c1)c1cccnc1': 286,\n",
       " '[NH3+]CCCOc1cc(ccc1C(=O)Nc1ccccc1)Nc1cc(O)cc(c1)OC': 287,\n",
       " 'NC(=O)CC[C@@H](C(=O)N[C@H](C(=O)O)[C@H](O)C)NC(=O)[C@@H](NC(=O)[C@H]([C@H](O)C)NC(=O)[C@@H](NC(=O)[C@@H]([NH3+])C)CCCNC(=N)N)CCCC[N+](C)(C)C': 288,\n",
       " 'OCc1ccccc1O[P@](=O)(OCc1ccccc1)O': 289,\n",
       " 'OC[C@@H](C(=O)N1CCC[C@H]1C(=O)O)NC(=O)[C@H]([C@H](O)C)NC(=O)[C@@H]1CCCN1C(=O)[C@@H](NC(=O)[C@H](Cc1ccc(cc1)O)[NH3+])COP(=O)(O)O': 290,\n",
       " 'NC(=N)NCCCC(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)O)Cc1ccccc1)[C@@H](O)C)Cc1ccccc1': 291,\n",
       " 'Brc1ccc(cc1)[C@H](NC(=O)c1cc(F)ccc1O)C': 292,\n",
       " 'ONC(=O)C[C@H](C(=O)N[C@@H]1[C@H](O)Cc2c1cccc2)Cc1cccc(c1)O': 293,\n",
       " 'O=C1C=C[C@@H](C(=C1)O)S(=O)(=O)n1cc2c(c1)ccc(c2)O': 294,\n",
       " '[NH3+]CCCC[C@@H](C(=O)N[C@H](C(=O)N[C@@H]([C@H](O)C)C=O)CO)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@H]([C@H](O)C)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@H]([C@H](O)C)NC(=O)[C@@H](NC(=O)[C@@H]([NH3+])C)CCCNC(=N)N)CCCC[N+](C)(C)C)CCC(=O)N)C)CCCNC(=N)N': 295,\n",
       " 'COC1=C(CC(=O)N2[C@@H]1SC[C@H]2C(=O)O)Cc1cccc2c1cccc2': 296,\n",
       " 'O[C@@H]1[C@H]2CO[P@@](=O)(O)O[P@](=O)(O)OCC3=C([C@H]([C@H](n4c5ncn([C@@H]([C@@H]1O)O2)c(=O)c5nc4)O3)O)O': 297,\n",
       " 'CC(Oc1cc(ccc1NC(=O)c1csc(n1)c1cccc(c1)C(=O)Nc1ccccc1)C(=O)N1CC[NH+](CC1)C)C': 298,\n",
       " '[NH3+]C[C@H](c1ccc(cc1)Cl)c1ccc(cc1)C1=CN=NC1': 299,\n",
       " '[NH3+]CCCC[C@@H](C(=O)NCC(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@@H]([C@H](OP(=O)(O)O)C)C(=O)N[C@H](C(=O)O)CC1=NC=NC1)[C@H](O)C)C(C)C)C(C)C)Cc1ccc(cc1)O)CC(=O)N)[NH3+]': 300,\n",
       " 'Clc1cc2c(nnc(c2cc1Cl)NC1CCCC1)c1ccncc1': 301,\n",
       " 'Nc1nc2ccccc2nc1N1CCCC1': 302,\n",
       " 'CS(=O)(=O)Nc1ccc(cn1)Nc1ncc2c(n1)n(nc2)C1CCCCCC1': 303,\n",
       " 'NC(=N)NCCC[C@@H]1NC(=O)[C@H](CCCNC(=N)N)NC(=O)[C@H](CCCNC(=N)N)NC(=O)CCCC(=O)NCCCC[C@H](NC1=O)C(=O)N[C@H](C(=O)NCc1ccc(cc1)C(=N)N)CCCNC(=N)N': 304,\n",
       " 'F[C@@H]1CN(CC[C@@H]1Oc1cc(OCC(O)(C)C)ccc1C(=O)N)C(=O)Cc1ccc(cc1)OC(F)(F)F': 305,\n",
       " '[NH3+]CCCC[C@@H](C(=O)N[C@@H]([C@H](OP(=O)(O)O)C)C(=O)N[C@H](C(=O)NCC(=O)N1CCC[C@H]1C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)O)CC(=O)O)CO)CC(=O)O)CCC(=O)O)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@H](CCCC[NH3+])[NH3+])CC(C)C)CCSC)Cc1ccccc1': 306,\n",
       " 'Fc1ccc(cc1)C[C@@H]1NC(=O)[C@H](CC2CCCCC2)NC(=O)/C=C/C(=O)N[C@@H](CCCNC(=O)[C@@H](NC1=O)Cc1ccc(cc1)N(=O)=O)C(=O)O': 307,\n",
       " 'F[C@H]1CCN(C1)C(=O)[C@H]([C@@H](c1onc(n1)c1ccc(cc1F)S(=O)(=O)C)CC1CC1)[NH3+]': 308,\n",
       " '[NH3+][C@H]1C[C@@H]1c1ccccc1': 309,\n",
       " 'CCCCC[NH2+]CC(P(=O)(O)O)P(=O)(O)O': 310,\n",
       " 'OC[C@H]1O[C@@]([C@H]([C@@H]1O)O)(CO)O[C@H]1O[C@H](CO)[C@H]([C@@H]([C@H]1O)O)O': 311,\n",
       " 'CNC(=O)c1ccc2c(c1)n1c(nnc1c(n2)C)c1cc(OCC(C)C)ccc1Cl': 312,\n",
       " 'COC(=O)C[C@@H]1ON=C(C1)c1ccc(cc1)O': 313,\n",
       " 'OC[C@H]1C[C@H]([C@@H]([C@@H]1O)O)Nc1ncnc2c1c(ccc2)OCc1ccccc1': 314,\n",
       " '[NH3+]CCC(=O)N1CCN(CC1)C(=O)[C@@H](NS(=O)(=O)c1c(cc(cc1C(C)C)C(C)C)C(C)C)Cc1cccc(c1)C(=N)N': 315,\n",
       " 'O[C@@H]1C[S@@H]2[C@H]([C@@H]1O)CCCC2': 316,\n",
       " 'SC[C@@H](C(=O)N1CCC[C@H]1C(=O)N)NC(=O)[C@H]([C@H](O)C)[NH3+].CSCC[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)O)CCCNC(=N)N)C)[C@H](O)C)CS)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@H]([C@H](O)C)[NH3+])CS)CCCNC(=N)N)CCC(=O)N)CO': 317,\n",
       " 'O=C(OC(C)(C)C)Nc1cccc(c1)B(O)O': 318,\n",
       " 'OP(=O)(O[C@@H]1[C@H](OP(=O)(O)O)[C@H](OP(=O)(O)O)[C@H]([C@@H]([C@H]1OP(=O)(O)O)OP(=O)(O)O)OP(=O)(O)O)O': 319,\n",
       " 'Fc1ccc(cc1)C[NH+]1CCN(CC1)C(=O)c1ccc(cc1N(=O)=O)N(=O)=O': 320,\n",
       " 'OC(=O)Cc1sc2c(c1C)cc(cc2)Cl': 321,\n",
       " 'OC[C@@H](C(=O)N[C@@H]1[C@@H](CO)O[C@H]([C@@H]1O)n1cnc2c1ncnc2N)[NH3+]': 322,\n",
       " 'Nc1nc(N)c2c(n1)n(cn2)C[C@@H](OCP(=O)(O)O)C': 323,\n",
       " 'O=C([C@H](Cc1ccc(cc1)O)NC(=O)[C@H](Cc1ccccc1)NC(=O)[C@H]([C@H](O)C)NC(=O)[C@@H]([NH3+])C)N[C@H](C(=O)N[C@H](C(=O)O)C)C(C)C': 324,\n",
       " 'FC1=N[C@H](Cl)C2=NC=NC2=N1': 325,\n",
       " '[NH3+]CCCC[C@@H](C(=O)N[C@H](C(=O)O)CCC(=O)N)NC(=O)[C@H]([C@H](O)C)NC(=O)[C@@H](NC(=O)[C@@H]([NH3+])C)CCCNC(=N)N': 326,\n",
       " '[NH3+]CCCC[C@@H](C(=O)N1CCC[C@H]1C(=O)N1CCC[C@H]1C(=O)N[C@H](C(=O)N[C@H](C(=O)N1CCC[C@H]1C(=O)N[C@H](C(=O)N1CCC[C@H]1C(=O)O)CCCNC(=N)N)CC(C)C)Cc1ccc(cc1)O)[NH3+]': 327,\n",
       " 'O=C(c1nc2c([nH]1)cccc2)c1ccc[nH]1': 328,\n",
       " 'CC(=O)Nc1ccc2c(c1)ncn2C': 329,\n",
       " 'NC(=N)NS(=O)(=O)c1ccc(cc1)N/C=C/1\\\\C(=O)Nc2c1cccc2': 330,\n",
       " 'CC(=O)N[C@H]([C@@H](C[NH2+][C@H]1CC2(CCC2)Oc2c1cc(cn2)CC(C)(C)C)O)Cc1ccc(cc1)F': 331,\n",
       " 'CCCCCCCCCCCCCCCCC[C@H]1C[N@H+](C)[C@H](C(=O)N([C@@H]1C(=O)O)C)[C@@H]([C@H]1O[C@H]([C@@H]([C@@H]1O)O)n1ccc(=O)[nH]c1=O)O[C@@H]1O[C@@H]([C@H]([C@H]1O)O)C[NH3+]': 332,\n",
       " 'CCNC(=O)c1noc(c1c1ccc(cc1)C[NH+]1CCOCC1)c1cc(C(C)C)c(cc1O)O': 333,\n",
       " 'Clc1ccc(cc1)N([C@H]1C=CN(c2c1cccc2)Cc1ccc(cc1)c1ccc(cc1)CN1CC=C(c2c1cccc2)N(c1ccc(cc1)Cl)C)C': 334,\n",
       " 'Fc1ccc(c(c1)F)CNC(=O)c1cn2C[C@@H]3OCC[C@H](N3C(=O)c2c(c1=O)O)C': 335,\n",
       " 'OC[C@@H](C(=O)NS(=O)(=O)OC[C@H]1O[C@H]([C@@H]([C@@H]1O)O)n1cnc2c1ncnc2N)[NH3+]': 336,\n",
       " 'OC[C@H]1O[C@H](O[P@](=O)(O[P@](=O)(OC[C@H]2O[C@H]([C@@H]([C@@H]2O)O)n2cnc3c2[nH]c(=N)[nH]c3=O)O)O)[C@H]([C@H]([C@@H]1O)O)O': 337,\n",
       " 'C[S@H](C[C@H]1O[C@H]([C@@H]([C@@H]1O)O)n1cnc2c1ncnc2N)CC[C@@H](C(=O)O)[NH3+]': 338,\n",
       " 'CO[C@H]([C@@H]([C@H]([C@@H](/C=C/C(C)(C)C)O)O)O)C(=O)NC1Cc2c(C1)cccc2': 339,\n",
       " 'Nc1ncc2c(n1)nc(c(c2)c1c(Cl)cccc1c1cccnc1)N1C[C@@H]2[C@H](C1)[C@H]2[NH3+]': 340,\n",
       " 'O=C1N(Cc2c1c(ccc2)C(=O)O)c1cccc(c1)C(F)(F)F': 341,\n",
       " 'O[C@@H]1[C@@H]2O[P@@](=O)(O)OC[C@H]3O[C@H]([C@@H]([C@@H]3O[P@](=O)(OC[C@H]2O[C@H]1n1cnc2c1[nH]c(=N)[nH]c2=O)O)O)n1cnc2c1[nH]c(=N)[nH]c2=O': 342,\n",
       " 'CC[C@H]([C@H](C(=O)N[C@H](C(=O)NCc1ccc(cc1)C(=N)N)CCC(=O)N)NS(=O)(=O)Cc1ccccc1)C': 343,\n",
       " 'CC[C@@H]([C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N)C)C)C(C)C)CCC(=O)O)CC(=O)O)Cc1ccc(cc1)OP(=O)(O)O)NC(=O)[C@H](CC1=NC=NC1)NC(=O)C)C': 344,\n",
       " 'C/C=C/O[C@@]1(C[C@@H](O[C@@]2(CC[C@H]([C@H](O2)[C@@H](CO)O)O)C(=O)O)[C@H]([C@H](O1)[C@@H](CO)O)O)C(=O)O': 345,\n",
       " 'O=CN(C[C@H](C(=O)NNc1nc(C)nc(c1F)N1CC[N@@H+]2[C@@H](C1)COCC2)CC1CCCC1)O': 346,\n",
       " 'CC(C[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)NCC(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)O)C(C)C)CC1=CN=C2[C@H]1C=CC=C2)Cc1ccc(cc1)O)CCC(=O)N)CC(=O)N)CCCNC(=N)N)NC(=O)[C@@H]([NH3+])C)C': 347,\n",
       " 'O=C1N/C(=C\\\\C2=CN=C3C2=CCC(=C3C)Cl)/C(=O)N1Cc1ccc(c(c1)F)F': 348,\n",
       " 'Fc1ccc(cc1)c1ncn(c1c1ccncc1)CC1CC1': 349,\n",
       " 'OC[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)O)CCCNC(=N)N)C(C)C)NC(=O)NCCCCNC(=N)N': 350,\n",
       " 'NC(=O)OCC1=C2[C@@H](O)[C@H](C=[N+]2c2c1c(O)c(N)c(c2O)C)[NH3+]': 351,\n",
       " 'O=C(O[C@@H]1C[C@](O)(C[C@H]([C@@H]1O)O)C(=O)O)CCc1ccc(c(c1)O)O': 352,\n",
       " 'Fc1ccc(cc1)C(=C1CC[NH+](CC1)CCc1c(C)nc2n(c1=O)ccs2)c1ccc(cc1)F': 353,\n",
       " '[NH3+]CCCC[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)O)CCCC[NH3+])CCCC[NH3+])NC(=O)CNC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@]1(C)CCC/C=C\\\\CCC[C@](C)(NC(=O)[C@@H](NC(=O)[C@H]([C@H](O)C)NC(=O)[C@H]([C@H](CC)C)[NH3+])Cc2ccccc2)C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N1)CC(C)C)CC(C)C)CC(=O)O)Cc1ccc(cc1)O)Cc1ccc(cc1)O': 354,\n",
       " 'COc1cc(C(=O)O)c2c(c1C)OC(=O)c1c(O2)c(C)c(cc1C)O': 355,\n",
       " 'OC[C@H]1O[C@@H](S[C@@H]2O[C@H](CO)[C@@H]([C@@H]([C@H]2O)O)O)[C@@H]([C@H]([C@H]1O)O)O': 356,\n",
       " 'OC[C@H]1O[C@H](Oc2ccc(cc2C)c2cc(cc(c2)C(=O)NC)C(=O)NC)[C@H]([C@H]([C@@H]1O)O)O': 357,\n",
       " 'COc1ccc(cc1Cc1cnc2c(c1C)c(N)nc(n2)N)OCCc1ccccc1': 358,\n",
       " 'CC(=N)N1CC[C@@H](C1)Oc1ccc(cc1)[C@@H](C(=O)O)Cc1ccc2c(c1)cc(cc2)C(=N)N': 359,\n",
       " 'C[C@H]([C@H](c1ccc(cc1)O)O)C[N@@H+]1CC[C@H](CC1)Cc1ccccc1': 360,\n",
       " 'CCC(=O)Nc1ccc2c(c1)n([C@@H]1CC[N@H+](CC1)Cc1ccc(cc1)c1nc3CC=NC(=O)c3cc1c1ccccc1)c(=O)n2C': 361,\n",
       " 'NC(=O)Cc1c(C)n(c2c1cc(OCCCC(=O)O)cc2)Cc1ccccc1': 362,\n",
       " 'O=C1N(CCc2c1n(nn2)Cc1ccc2c(c1)cccn2)c1ccccc1': 363,\n",
       " 'Cc1nnc(o1)c1ccc2c(c1)c(co2)c1ccc(cc1)[S@@H](O)C': 364,\n",
       " 'O[C@@H]1[C@@H](CO[P@@](=O)(O[P@](=O)(NP(=O)(O)O)O)O)O[C@H]([C@@H]1O)n1cnc2c1ncnc2N': 365,\n",
       " 'OC[C@H]1O[C@@H](O[C@@H]2[C@@H](CO)O[C@H]([C@@H]([C@H]2O)[NH3+])O[C@@H]2[C@@H](CO)OC(=O)[C@@H]([C@H]2O)[NH3+])[C@@H]([C@H]([C@@H]1O[C@@H]1O[C@H](CO)[C@H]([C@@H]([C@H]1[NH3+])O)O[C@@H]1O[C@H](CO)[C@H]([C@@H]([C@H]1[NH3+])O)O)O)[NH3+]': 366,\n",
       " 'SC[C@H](C(=O)N1CCC[C@@H]1C(=O)O)C': 367,\n",
       " 'O[C@H]1C[C@@H](O[C@@H]1COP(=O)(O)O)n1c(=O)[nH]c2c1nc(N)[nH]c2=O': 368,\n",
       " 'N=C1COC[C@@](N1)(C)c1cccc(c1)Nc1cccc2c1ncc(c2)Cl': 369,\n",
       " 'OC[C@H]([C@@H](c1ccc(cc1)N(=O)=O)O[P@@](=O)(Cc1ccc(cc1)NC(=O)C(F)(F)F)O)NC(=O)C(Cl)Cl': 370,\n",
       " 'COc1cc(C)c(cc1C)C[NH2+]C[C@H](c1ccccc1)NC(=O)c1noc(c1)C': 371,\n",
       " 'OC[C@H]([C@H]([C@@H]1O[C@H](C[C@@H]([C@H]1NC(=O)C)O)C(=O)O)O)O': 372,\n",
       " '[NH3+]CCCC[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)O)CC(=O)O)C(C)C)CCCC[NH3+])C)CCCNC(=N)N)NC(=O)[C@@H](NC(=O)[C@@H]1CCCN1C(=O)[C@@H]1CCCN1C(=O)C[NH3+])CCCC[NH3+]': 373,\n",
       " 'C[C@H]([C@@H](Cc1ccc2c(c1)OCO2)C)Cc1ccc2c(c1)OCO2': 374,\n",
       " 'Nc1nc2CCSCc2c(n1)C': 375,\n",
       " 'O=C(COc1cccc2c1C(=O)N(C2=O)[C@H]1CCC(=O)NC1=O)NCCCCCCCCNC(=O)C[C@@H]1N=C(c2ccc(cc2)Cl)c2c(-n3c1nnc3C)sc(c2C)C': 376,\n",
       " 'CCC[C@@H](C(=O)O)[NH2+]C[C@H]([C@@H](NC(=O)[C@H](Cc1cccs1)NC(=O)[C@H]([C@H](CC)C)NC(=O)[C@H](CCC(=O)O)[NH3+])CC(C)C)O': 377,\n",
       " 'CCCCC(=O)NO': 378,\n",
       " 'CC1(C)CN=C(S1)N1CCN(CC1)c1ncnc2c1cc(s2)C(C(F)(F)F)(F)F': 379,\n",
       " 'O=C(N1CC[C@H](CC1)CC[NH+]1CCCC1)Cn1nnc(c1)CCOc1ccc2c(c1)C[N@@H+](C2)CCCOc1cc2NC(=N)C(c2cc1OCC1CC1)(C)C': 380,\n",
       " 'OP(=O)(O[C@@H]1[C@H](OP(=O)(O)O)[C@H](OP(=O)(O)O)[C@@H]([C@@H]([C@H]1OP(=O)(O)O)OP(=O)(O)O)OP(=O)(O)O)O': 381,\n",
       " '[NH3+]CCCC[C@@H]1NC(=O)[C@@H](NC(=O)CNC(=O)[C@H](Cc2ccc(cc2)O)NC(=O)[C@H](CC(C)C)NC(=O)[C@H](CCCNC(=N)N)NC(=O)[C@@H](NC(=O)CNC(=O)C)Cn2cc(-c3cc(-c4cn(C[C@]5(N(C(=O)[C@@H](NC(=O)[C@@H](NC1=O)[C@H](CC)C)CC1=NC=NC1)O5)C(=O)NCC(=O)NCC(=O)N)nn4)cc(c3)C(=O)O)nn2)Cc1ccccc1': 382,\n",
       " 'O[C@H]1C[C@@H](O[C@@H]1CO[P@](=O)(O[P@](=O)(OP(=O)(O)O)O)O)n1cnc2c1ncnc2N': 383,\n",
       " 'CC(CCN1C(=O)[C@H](C2=NS(=O)(=O)c3c(N2)ccc(c3)NS(=O)(=O)C)[C@H](n2c1ncc2)O)C': 384,\n",
       " '[NH3+]CCC(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N1CCC[C@H]1C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N1CCC[C@H]1C(=O)N[C@H](C(=O)O)Cc1ccc(cc1)O)C(C)C)CO)CC(=O)O)Cc1ccc(cc1)O)C)CC(C)C)CC1=CN=C2[C@@H]1C=CC=C2': 385,\n",
       " 'O=C1N(Cc2c1c(ccc2)C(=O)O)c1ccc(c(c1)F)O': 386,\n",
       " 'C[NH+]1CCN(CC1)S(=O)(=O)c1ccc(cc1)c1cnc(c(c1)C(=O)Nc1cnccc1C[NH+]1CCCC1)N': 387,\n",
       " 'CC(C[C@@H](C(=O)N[C@H](C(=O)O)Cc1ccccc1)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)C)CCC(=O)N)CC(C)C)C)C': 388,\n",
       " 'O=CN1C[C@@H](CC[C@H]1C(=O)NNC(=O)C)[NH2+]OS(=O)(=O)O': 389,\n",
       " 'O=C1C=CC2=C(C1)[C@H]1C(=N2)C=C(C2=C1C(=O)NC2=O)c1ccccc1': 390,\n",
       " 'N#Cc1ccc(cc1Cn1c2c(nc1N1CCC[C@H](C1)[NH3+])N[C@H]1N(C2=O)CC(=O)N1C)F': 391,\n",
       " 'O=C(c1cccc(c1O)O)NCc1cc(CNC(=O)c2cccc(c2O)O)cc(c1)CNC(=O)c1cccc(c1O)O': 392,\n",
       " 'O[C@@H]1[C@@H](CO[P@](=O)(O[P@@](=O)(OP(=O)(O)O)O)O)O[C@H]([C@@H]1O)n1cnc2c1ncnc2N': 393,\n",
       " 'OC[C@H](Cn1c(C#N)cc2c1ccc(c2)C[N@@H+]1CC[C@H](CC1)Nc1ncnc2c1cc(s2)CC(F)(F)F)O': 394,\n",
       " 'OCCn1c(C[NH2+]CCCNC2=NC3=CC=CCC3=N2)cc2c1cc(Cl)cc2Cl': 395,\n",
       " 'CC(c1cnn2c1nc(N[C@@H]1CC[C@H](CC1)[NH3+])cc2Nc1ccc(cc1)S(=O)(=O)N(C)C)C': 396,\n",
       " 'CC[C@@H]([C@@H](C(=O)N1CCC[C@H]1C(=O)N[C@H](C(=O)N1CCC[C@H]1C(=O)N)CCCNC(=N)N)NC(=O)[C@@H](NC(=O)[C@H](Cc1ccc(cc1)O)NC(=O)[C@H](CC(C)C)[NH3+])CC1CCCCC1)C': 397,\n",
       " 'O=CN1C[C@@H](CC[C@H]1C(=O)N)[NH2+]OS(=O)(=O)O': 398,\n",
       " 'O=C(CC(=O)C(=O)O)NCCCc1cccc(c1)Oc1ccccc1': 399,\n",
       " 'OC[C@@H](n1ccc(cc1=O)c1ccnc(n1)Nc1ccnc(c1)C#CCCNC(=O)O[C@@H]1CCCC=CCC1)c1ccc(c(c1)F)Cl': 400,\n",
       " 'CC[C@@H]([C@@H](C(=O)N[C@H](C(=O)N1CCC[C@H]1C(=O)N)CC1=NC=NC1)NC(=O)[C@H](Cc1ccc(cc1)O)[NH3+])C': 401,\n",
       " 'OC[N@@H+]([C@H](C(=O)O)CCCN1C(=O)N[C@]2(C1=O)O[C@@H]([C@H]([C@H]2O)O)COP(=O)(O)O)O': 402,\n",
       " 'Cc1cccc(c1)c1nc(c(n1c1cc(Cl)ccc1C)c1cccc(c1)Cl)C(=O)O': 403,\n",
       " 'CCC[N@H+]1CCC[C@]21CCN(C2=O)[C@H](C(=O)N[C@H]([C@@H](C[NH2+]Cc1cccc(c1)C(C)C)O)Cc1ccccc1)C': 404,\n",
       " 'OC(=O)C[NH2+][C@@H](C(=O)N1CCC[C@H]1C(=O)NCc1ccc(s1)C(=N)N)CC1CCCCC1': 405,\n",
       " 'O=C(c1ccc2c(c1)ccc(c2)C(=N)N)Nc1cccc(c1)OC1CCCC1': 406,\n",
       " 'CCOC(=O)c1c(C)nn(c1C)c1cccc(c1)N(=O)=O': 407,\n",
       " 'OC[C@@H](Nc1nc(Nc2nccc(c2)NC(=O)c2c(Cl)cccc2Cl)cc(n1)C)C': 408,\n",
       " 'Cc1ccc(cc1)N/C=C/1\\\\C(=O)CCc2c1oc1c2cc(c(c1Cl)Cl)O': 409,\n",
       " 'COC(=O)N[C@@H](C(C)(C)C)C(=O)N[C@H](Cc1ccc(cc1)c1ccccn1)C[C@@H]([C@H](Cc1ccccc1)NC(=O)[C@H](C(C)(C)C)n1ccn(c1=O)Cc1cccc(n1)C(O)(C)C)O': 410,\n",
       " 'CC/C(=C(\\\\c1ccc(cc1)O)/c1ccc(cc1)OCC[NH+](C)C)/c1ccccc1': 411,\n",
       " 'CCCCCCCCCCCC(=O)SCCNC(=O)CCNC(=O)[C@@H](C(CO[P@](=O)(O[P@](=O)(OC[C@H]1O[C@H]([C@@H]([C@@H]1OP(=O)(O)O)O)n1cnc2c1ncnc2N)O)O)(C)C)O': 412,\n",
       " 'CC[C@@H]([C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)O)CC(=O)O)CC(C)C)Cc1ccc(cc1)O)Cc1ccc(cc1)OP(=O)(O)O)NC(=O)[C@@H](NC(=O)[C@H](CCC(=O)O)[NH3+])CC(=O)O)C': 413,\n",
       " 'NC(=O)c1cc2c(s1)c(ncn2)N1CCC(CC1)CCNS(=O)(=O)C': 414,\n",
       " 'CON(C(=O)N1[NH2+][C@@H](S[C@@]1(CCC[NH3+])c1ccccc1)c1cc(F)ccc1F)C': 415,\n",
       " 'O=C1N[C@@H]2[C@H](N1)[C@@H](SC2)CCCCCc1nnn(c1)Cc1cccc(c1)F': 416,\n",
       " 'OC[C@@H](C(=O)N[C@H](C(=O)O)Cc1ccccc1)NC(=O)[C@@H](NC(=O)[C@H](C(C)C)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@H](Cc1ccccc1)[NH3+])CCC(=O)O)CC(=O)O)CC(C)C)CCCNC(=N)N)CC(C)C': 417,\n",
       " 'C=Cc1nc2c(n1[C@@H]1O[C@@H]([C@H]([C@H]1O)O)C[N@H+](CCCCO[NH3+])C)ncnc2N': 418,\n",
       " 'O=C1NC(=O)[C@@H](S1)Cc1ccc(cc1)OCCc1ccc(cn1)[C@@H](O)C': 419,\n",
       " 'OC[C@@H](C(=O)N[C@@H]([C@H](OP(=O)(O)O)C)C(=O)N)NC(=O)[C@H](Cc1cncn1CCCCCCCCc1ccccc1)NC(=O)C': 420,\n",
       " 'N#Cc1ccc(cc1)[C@H]1NC(=O)N(C2=C1C(=O)N[N@H+](C2)C)c1cccc(c1)C(F)(F)F': 421,\n",
       " 'O=C([C@H]1C=c2c(=N1)c(ccc2)N(=O)=O)Nc1ccc2c(c1)cccc2/N=N/C(=N)N': 422,\n",
       " 'N=C1NC(=O)[C@@H]2C(=N1)N(C[N@@H+]2C)[C@@H]1O[C@@H]([C@H]([C@H]1O)O)CO[P@](=O)(O[P@@](=O)(O[P@@](=O)(OC[C@H]1O[C@H]([C@@H]([C@@H]1O)O)n1cnc2c1[nH]c(=N)[nH]c2=O)O)O)O': 423,\n",
       " 'O=C(c1ccc(cc1)C(C)(C)C)Nc1cc(nn1c1ccccc1)c1ccccc1': 424,\n",
       " 'C1CC[C@@H]2[C@@H](C1)[NH2+][Pt][NH2+]2': 425,\n",
       " 'CSCC[C@@H](C(=O)N[C@H](C(=O)N)CC(C)C)NC(=O)[C@@H](NC(=O)C(NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@H](CCCNC(=N)N)[NH3+])CCCNC(=N)N)Cc1ccccc1)(C)C)C': 426,\n",
       " 'OC[C@H]([C@H]([C@@H](Cn1c(=O)[nH]c2c1[nH]c(=O)[nH]c2=O)O)O)O': 427,\n",
       " 'OC[C@H]([C@H]([C@@H]1O[C@](OC[C@H]2O[C@@H](O)[C@@H]([C@H]([C@H]2O)O)O)(C[C@@H]([C@H]1NC(=O)C)O)C(=O)O)O)O': 428,\n",
       " 'OC[C@@H](C(=O)N[C@H](C(=O)N1CCC[C@H]1C(=O)N[C@H](C(=O)N1CCC[C@H]1C(=O)N[C@H](C(=O)O)CC(C)C)C(C)C)C)NC(=O)[C@@H]1CCCN1C(=O)[C@H]([C@H](O)C)NC(=O)[C@@H]1CCC[NH2+]1': 429,\n",
       " 'Cc1ccc(cc1)[C@@H]1[C@H](NC(=O)c2cccc(c2)C)[C@H](O)Nc2c1c(C)nn2c1ccccc1': 430,\n",
       " '[NH3+]CCNS(=O)(=O)c1ccc(c2c1cncc2)Cl': 431,\n",
       " 'O=C[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@H](CC(=O)O)[NH3+])CC(=O)N)CC(C)C)CC(=O)O': 432,\n",
       " 'O[C@@H]1[C@H](O)[C@H](O[C@H]1n1cnc2c1ncnc2N)COP(=O)(O)O': 433,\n",
       " 'Brc1c(Br)c(Br)c(c2c1[nH]c(=O)[nH]2)Br': 434,\n",
       " 'O=C(N[C@@H]([C@@H]1[NH2+][C@H](C(S1)(C)C)C(=O)O)C(=O)NCc1ccccc1)Cc1ccccc1': 435,\n",
       " 'O=N(=O)c1cccc(c1)n1c(=O)n(Cc2ccncc2)c(=O)c2c1nccc2': 436,\n",
       " 'NC1=CCN(C(=O)N1)[C@@H]1O[C@H]2[C@H]([C@H]1O)O[P@](=O)(OC2)O': 437,\n",
       " 'CCCOC(=O)N1CCN([C@H](C1)C)C(=O)c1ccc2c(c1)nc1c(c2)CC[C@H](C1)C[NH3+]': 438,\n",
       " 'O[C@@H]1[C@H](O)[C@H](O[C@H]1N1CC=CC(=C1)C(=O)O)CO[P@](=O)(O[P@@](=O)(OC[C@H]1O[C@H]([C@@H]([C@@H]1O)O)n1cnc2c1ncnc2N)O)O': 439,\n",
       " 'CCNC(=O)C1=C[C@@H]2C(=N1)N=CC=C2N1CCOC[C@@H]1C': 440,\n",
       " 'C=CC[N@H+](CCCCCCOc1ccc2c(c1)n(C)nc2c1ccc(cc1)Br)C': 441,\n",
       " 'OCCC[C@H]1CC[C@@H](C[C@@H]1[NH2+]CC1=Cc2c(CC1)nc(n2C[C@H]1[NH2+][C@@H](C)CC=C1O)NCCC[NH+]1CCOCC1)C': 442,\n",
       " 'OC(=O)C[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)O)Cc1ccc(cc1)O)CCC(=O)O)Cc1ccccc1)Cc1ccc(cc1)O)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@H](C(C)C)NC(=O)[C@@H]1CCCN1C(=O)[C@H](CC1=NC=NC1)[NH3+])C)CCC(=O)O)C': 443,\n",
       " 'OC(=O)c1ccc(o1)c1ccccc1OC(F)(F)F': 444,\n",
       " 'Nc1ncnc2c1nc(s2)c1n(cnc1c1ccccc1)CC1CCCCC1': 445,\n",
       " 'COc1ccc2c(n1)ccc(c2)Br': 446,\n",
       " 'CCC(CN(S(=O)(=O)c1ccc(cc1)[C@@H](O)C)C[C@H]([C@H](Cc1ccccc1)NC(=O)O[C@H]1CO[C@@H]2[C@H]1CCO2)O)CC': 447,\n",
       " 'O=C(Nc1cscc1)NCCC[NH2+]Cc1cc(Cl)cc(c1O)I': 448,\n",
       " 'O[C@@H]1[C@H](O)[C@H](O[C@H]1n1cnc2c1[nH]c(=N)[nH]c2=O)COP(=O)(O)O': 449,\n",
       " 'Cn1cnc(c1)S(=O)(=O)Nc1cc2c(cc1Oc1cccc(c1)OC[C@@H]1COCC1)n(c(=O)n2C)C': 450,\n",
       " 'COc1cc2c(ccnc2cc1OCCC[NH+]1CCOCC1)Oc1ccc(cc1F)c1cnc(n(c1=O)C)Cc1ccccc1': 451,\n",
       " '[NH3+]CCCC[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N1CCC[C@H]1C(=O)N[C@H](C(=O)N[C@H](C(=O)NCC(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)O)CCC(=O)N)CC(C)C)CCC(=O)O)[C@H](O)C)COP(=O)(O)O)CC(=O)O)C(C)C)CCC(=O)O)[NH3+]': 452,\n",
       " 'N#Cc1ccc(cc1)[C@H](c1cncn1C)OCc1nc(Cl)c(cc1c1cccc(c1)Cl)C#N': 453,\n",
       " 'O=C1NC(=O)[C@@H]2[C@H](N1)N(C[C@@H]([C@@H]([C@@H](CO[P@@](=O)(O[P@](=O)(OC[C@H]1O[C@H]([C@@H]([C@@H]1O)O)n1cnc3c1ncnc3N)O)O)O)O)O)c1c(N2)cc(c(c1)C)C': 454,\n",
       " 'COc1ccc2-c3c(C4CCCCC4)c4c(n3C[C@@]3([C@H](c2c1)C3)C(=O)N1[C@@H]2CC[C@H]1C[N@@H+](C2)C)cc(cc4)C(=O)NS(=O)(=O)N(C)C': 455,\n",
       " '[NH3+]CCCC[C@H](C(=O)N[C@@H](C(=O)N[C@@H](C(=O)N[C@@H](C(=O)O)CCCNC(=N)N)CC(C)C)CC(C)C)NC(=O)[C@H](NC(=O)[C@H](NC(=O)[C@H](NC(=O)[C@H]([NH2+][C@H]([C@H](NC(=O)[C@H]([NH2+][C@@H]([C@@H](CC(=O)N)[NH3+])O)CC1=CN=C2C1=CC=CC2)Cc1ccc(cc1)O)O)C)CC(=O)N)CC(C)C)CCC(=O)O': 456,\n",
       " 'OC[C@H]([C@@H]([C@H](Cc1ccccc1)NC(=O)[C@H](Cc1ccc(cc1)OC)NC(=O)[C@H](NC(=O)C[NH+]1CCOCC1)C)O)C': 457,\n",
       " 'O[C@@H]1C[C@H](N(C1)C(=O)[C@H](C(C)(C)C)NC(=O)COCCOCCOCCNC(=O)C[C@@H]1N=C(c2ccc(cc2)Cl)c2c(-n3c1nnc3C)sc(c2C)C)C(=O)NCc1ccc(cc1)c1scnc1C': 458,\n",
       " 'Cc1cc(nc2c1cccc2)N1CC[NH2+]CC1': 459,\n",
       " 'CCC[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)O)C(C)C)CO)Cc1ccccc1)CCC(=O)N)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H]([NH3+])C)C)Cc1ccc(cc1)O': 460,\n",
       " 'O=C(Nc1ccccc1)Cc1nc(cc(=O)[nH]1)N1CCOCC1': 461,\n",
       " 'N#Cc1ccc(cc1)N(Cc1cncn1C)CCN(S(=O)(=O)c1ccccc1C)CC1CCN(CC1)C(=O)OC(C)(C)C': 462,\n",
       " 'O=C(Nc1cc(nn1c1cccc(c1)C)C(C)(C)C)Nc1cccc(c1)Nc1ncnc2c1cc(N)cc2': 463,\n",
       " 'N=CCNC(=O)CNC(=O)[C@H]([C@H](O)C)NC(=O)Cc1cc(Cl)cc(c1)Cl': 464,\n",
       " '[NH3+]CCCC[C@@H]1NC(=O)[C@H](CC(C)C)NC(=O)[C@H](C)NC(=O)[C@H](CCCNC(=N)N)NC(=O)[C@H](CO)NC(=O)[C@H](CO)NC(=O)[C@H](CC(C)C)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@H](CSSC[C@H](NC1=O)C(=O)O)[NH3+])CCC(=O)N)Cc1ccc(cc1)O)CC(=O)N': 465,\n",
       " '[NH3+]CCCC[C@@H](C(=O)O)NC(=O)[C@H]([C@H](CC)C)NC(=O)[C@@H](NC(=O)[C@H](CC1=CN=C2[C@H]1C=CC=C2)NC(=O)[C@@H](NC(=O)[C@H](CC1=CN=C2[C@@H]1C=CC=C2)NC(=O)[C@@H](NC(=O)[C@H]([C@H](O)C)NC(=O)[C@H]([C@H](CC)C)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@H](CC(=O)N)[NH3+])CC1=CN=C2[C@H]1C=CC=C2)Cc1ccccc1)CC(=O)O)CC(=O)N)CC(C)C)Cc1ccc(cc1)O': 466,\n",
       " 'O=C(Nc1ccc(cc1)OC[C@H]1OCC[N@H+](C1)C(C)(C)C)NC1CCCCC1': 467,\n",
       " '[NH3+]CCCC[C@@H](C(=O)O)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@H]([C@H](CC)C)NC(=O)[C@@H](NC(=O)[C@H](CC(=O)O)[NH3+])CC(=O)O)COP(=O)(O)O)CC(C)C)CC(C)C': 468,\n",
       " 'N#Cc1cccc(c1)c1csc(c1)[C@]12NC(=N)N(C(=O)[C@@H]2CN(C1)c1ncc(cn1)F)C': 469,\n",
       " 'Fc1cc(C(=O)N)c2c(c1)[nH]c(n2)c1cc2c(o1)C[NH2+]CC2': 470,\n",
       " 'OC(=O)[C@@H]([NH3+])CC1=NC=NC1': 471,\n",
       " 'C[N@@H+]1CCN(C(=O)[C@H]1c1ccc(cc1)Nc1nc(cn(c1=O)C)c1cccc(c1C)NC(=O)c1cc2c(s1)CCCC2)C': 472,\n",
       " 'CCCNC(=O)c1cn2c(c1C)c(ncn2)Nc1cc(ccc1C)C(=O)NC1CC1': 473,\n",
       " 'COC(=O)N1CC[NH+](CC1)[C@@H]1CC[C@H](CC1)Nc1ncnc2c1cc(C#N)cc2': 474,\n",
       " 'CC(=O)Nc1ccc2c(c1)ncnc2Nc1ccccc1': 475,\n",
       " 'OCc1cc2n[nH]c(=O)n2c2c1ccc(c2)C1=NCC=C1': 476,\n",
       " 'C[N@@H+]1CC[N@H+](CC1)Cc1ccc(cc1)C(=O)Nc1ccc(c(c1)Nc1nccc(n1)c1cccnc1)C': 477,\n",
       " 'Clc1ccc(cc1)c1nc2c(n1[C@@H](C1CCCCC1)COc1ccc(cc1F)C(=O)O)cc(c(c2)F)F': 478,\n",
       " 'Clc1cccc(c1)COc1ccccc1B(O)O': 479,\n",
       " 'OC1(CC[NH2+]CC1)c1ccc(cc1)Br': 480,\n",
       " 'N#Cc1ccc(nc1)C(=O)Nc1ccc(c(c1)[C@@]1(C)NC(=N)O[C@H](C1)C(F)(F)F)F': 481,\n",
       " 'ONC(=O)C[C@H](n1nnc(c1)CNC(=O)c1ccc(cc1)F)Cc1ccc2c(c1)cccc2': 482,\n",
       " 'N#Cc1cc(cc(c1)Cl)Oc1c(=O)n(ccc1C(F)(F)F)Cc1n[nH]c(=O)n1C': 483,\n",
       " 'OC(=O)C[NH2+]CP(=O)(O)O': 484,\n",
       " 'OC[C@H]1O[C@@H](O)[C@@H]([C@H]([C@@H]1O[C@H]1O[C@H](CO)[C@H]([C@@H]([C@H]1O)O)O)O)O': 485,\n",
       " 'O[C@@H]1[C@H]([NH3+])[C@H](O[C@H]1n1cnc2c1ncnc2N)CO[P@](=O)(OP(=O)(O)O)O': 486,\n",
       " 'OC(=O)c1cccc(c1)c1ccccn1': 487,\n",
       " 'COC(=O)c1cccc2c1cc[nH]2': 488,\n",
       " 'C[NH+](CCCC(=O)Nc1ccc(cc1)C(=O)N1CCC[C@H](C1)Nc1ncc(c(n1)C1=c2ccccc2=NC1)Cl)C': 489,\n",
       " '[NH3+]NC(=O)c1ccc(o1)c1ccc(cc1)Cl': 490,\n",
       " 'COCc1sc2nc1C(=O)NCC(=O)N[C@H](c1scc(-c3scc(-c4c(-c5nc(C(=O)N[C@H](c6nc(C(=O)N[C@H]2C(C)C)c(C)s6)CC(=O)NC)cs5)ccc(n4)c2scc(n2)N(C(=O)O[C@@H]2CC[C@H](CC2)C(=O)O)CCCCC(=O)O)n3)n1)[C@H](c1ccccc1)O': 491,\n",
       " 'Clc1ccc(c(c1)O)Oc1ccc2c(c1)ccc(c2)O': 492,\n",
       " 'Oc1ccc2c(c1)CC[C@@H]1[C@@H]2CC[C@]2([C@H]1CC[C@@H]2O)C': 493,\n",
       " 'O=C(C1CC1)Nc1sc2c(c1C(=O)N)CCCC2': 494,\n",
       " 'O=Cc1ccc(c(c1)O)Oc1ccc(cc1Cl)Cl': 495,\n",
       " 'OC(=O)CC[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)O)CCC(=O)O)C(C)C)CC(=O)O)CC(=O)O)NC(=O)[C@@H]([NH3+])C': 496,\n",
       " 'CC(Oc1ccc(cc1F)CC[C@]1(CC(=CC(=O)O1)O)C1CCCC1)C': 497,\n",
       " 'O=C(c1ccc(cc1)S(=O)(=O)N)NCc1cc(F)c(c(c1)F)F': 498,\n",
       " 'COc1cc2CN(C=Cc2cc1OS(=O)(=O)N)Cc1cc(OC)c(c(c1)OC)OC': 499,\n",
       " '[NH3+]Cc1cccc(c1)[C@@H](C(=O)O)C[P@](=O)([C@H](C(C)C)NS(=O)(=O)c1cccc2c1nsn2)O': 500,\n",
       " 'OC[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)O)C(C)C)CCC(=O)O)C)Cc1ccc(cc1)O)NC(=O)[C@@H]1CCCN1C(=O)[C@@H]1CCCN1C(=O)[C@@H](NC(=O)[C@H](CCC(=O)O)[NH3+])C': 501,\n",
       " 'O[C@@H](C[NH2+]C(C)(C)C)COc1nsnc1N1CCOCC1': 502,\n",
       " 'Cc1ccc(cc1)C[C@@H](CNc1nnc(s1)C1=CC2=CN=N[C@H]2C=C1)[NH3+]': 503,\n",
       " 'N=C(c1sc2c(c1)c(=O)n(cc2c1cccc(c1)C(F)(F)F)C)NC1CCS(=O)(=O)CC1': 504,\n",
       " '[NH3+]CCCC[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)O)CCCC[NH3+])Cc1ccccc1)CC(C)C)C)CCC(=O)O)CCC(=O)N)CCCC[NH3+])CC(=O)O)[C@H](O)C)NC(=O)[C@@H](NC(=O)[C@H]([C@H](O)C)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@H]([C@H](CC)C)NC(=O)[C@@H]1CCCN1C(=O)[C@H](C(C)C)NC(=O)[C@H](CO)[NH3+])CCC(=O)N)CS)CC(=O)O': 505,\n",
       " 'OCCS(=O)(=O)c1c(NC2CCCCCCC2)c(F)c(c(c1F)F)S(=O)(=O)N': 506,\n",
       " 'CC(=O)NCCCCCCNC(=O)C': 507,\n",
       " 'O=C([C@@H]1CCCN1C(=O)C[NH2+]C1CCCC1)NCc1ccc(cc1)C(=N)N': 508,\n",
       " 'O=C(C1CC[NH2+]CC1)Nc1cc(Cl)ccc1O': 509,\n",
       " 'CC[C@@H]([C@@H]1NC(=O)CCCNC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC1=O)CC(=O)N)CC(=O)N)CC(=O)N)C': 510,\n",
       " 'CCOc1cccc(c1O)C(=O)Nc1ccc(cc1)C(=N)N': 511,\n",
       " 'OC[C@@H]1[C@@H](O)[C@H](O)[C@@H]([C@@H]2N1C(=O)C(=O)N2)O': 512,\n",
       " 'OC(=O)[C@@H](n1nccn1)Cc1ccc(cc1)CCCc1nc(oc1C)c1ccccc1': 513,\n",
       " 'O=C([C@@H]1CCCN(C1)c1cc(C)nc2n1ncn2)Nc1ccc2c(c1)cccc2': 514,\n",
       " 'O[C@@H]1[C@H](O)[C@H](O)[C@H]([C@@H]([C@H]1O)O)O': 515,\n",
       " 'O=C([C@H]1C[C@](CN1C(=O)Nc1ccc(cc1)Cl)(O)c1ccc(cc1F)F)Nc1ccc(cn1)n1ccccc1=O': 516,\n",
       " 'COc1cccc(c1)[C@H](c1sc(nc1N)Nc1ccc(cc1)S(=O)(=O)N)O': 517,\n",
       " 'ONC(=O)c1coc(n1)c1ccccc1C(F)(F)F': 518,\n",
       " 'CCCC[C@@H](C(=O)[C@H](C(C)C)NC(=O)Nc1ccc(cn1)C)CN(C=O)O': 519,\n",
       " 'O[C@@H]1C[C@@](O)(C=C([C@H]1O)OCc1cc2c(s1)ccc(c2)C)C(=O)O': 520,\n",
       " 'OC[C@H]1O[C@H](O)[C@@H]([C@H]([C@H]1O)O)O': 521,\n",
       " 'CSCC[C@@H](C(=O)N[C@H](C(=O)N[C@@H](CC(=O)O)CO)CCC(=O)N)NC(=O)[C@@H](NC(=O)C)CC(=O)O': 522,\n",
       " 'O=C1CC(=[N+]2CCOCC2)Nc2c1cccc2c1nnn(c1)Cc1cccc(c1)C(=O)O': 523,\n",
       " 'CSCC[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N1CCC[C@H]1C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N1CCC[C@H]1C(=O)N)CCC(=O)O)CCC(=O)O)CC1=CN=C2[C@@H]1C=CC=C2)Cc1ccccc1)CO)C)CC(C)C)CCC(=O)N)CCCNC(=N)N)C)CCCNC(=N)N)NC(=O)[C@@H]1CCCN1C(=O)[C@H](C(C)C)NC(=O)C': 524,\n",
       " 'CC[C@@]1(CCC(=O)NC1=O)c1ccc(cc1)N': 525,\n",
       " 'O=C1NC(=O)[C@@H](S1)Cc1ccc(cc1)OC[C@@]1(C)CCc2c(O1)c(C)c(c(c2C)O)C': 526,\n",
       " 'OC[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N1CCC[C@H]1C(=O)N[C@H](C(=O)O)C)Cc1ccc(cc1)O)COP(=O)(O)O)Cc1[nH]cnc1)NC(=O)[C@@H](NC(=O)[C@@H]([NH3+])C)CCCNC(=N)N': 527,\n",
       " '[NH3+]CCCCCCNS(=O)(=O)c1cccc2c1cccc2Cl': 528,\n",
       " '[NH3+]CCCC[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)O)CCC(=O)N)[C@H](CC)C)CC(=O)N)CC(=O)O)CCCNC(=N)N)CC(C)C)C(C)C)CCCCNC(=O)C)CCCNC(=N)N)CC1=NC=NC1)CCCNC(=N)N)NC(=O)[C@@H](NC(=O)CNC(=O)C[NH3+])C': 529,\n",
       " 'O=C(c1ccc(c(c1)N)C)Nc1ccc(c2c1c(cc(c2)S(=O)(=O)O)S(=O)(=O)O)S(=O)(=O)O': 530,\n",
       " 'COC(=O)c1oc2c(c1Nc1ccc3c(c1)CC/C/3=N\\\\O)ccnc2': 531,\n",
       " 'COc1cccc(c1)CC(=O)Nc1scc(n1)c1ccncc1': 532,\n",
       " 'C[NH2+][C@H]1[C@H](O[C@@H]([C@@H]([C@@H]1O)O)C)O[C@H]1[C@@H](C=C2/C/1=C\\\\C#C[C@@H]1[C@](C#C2)(O1)[C@H]1COC(=O)O1)OC(=O)c1c(O)ccc2c1cc(OC)cc2C': 533,\n",
       " 'C/C=C/C(=C/[C@@]1(C)SC(=O)C(=C1O)C)/C': 534,\n",
       " 'OC[C@H]1O[C@@H]([NH2+]OC(=O)Nc2ccccc2)[C@@H]([C@H]([C@@H]1O)O)NC(=O)C': 535,\n",
       " 'C=CCc1ccc(c(c1)c1cc(/C=C/C)ccc1O)O': 536,\n",
       " 'C[C@@H]1CCN(C[C@@H]1N(C1=NC=NC2=NC=C[C@@H]12)C)C(=O)N1CCCC1': 537,\n",
       " 'C=CC(=O)N1CC[C@@H](C1)n1nc(c2c1ncnc2N)C#Cc1cc(OC)cc(c1)OC': 538,\n",
       " 'OCC(=O)[C@@]1(O)CC[C@@H]2[C@]1(C)C[C@H](O)[C@H]1[C@H]2CCC2=CC(=O)CC[C@]12C': 539,\n",
       " 'O=C1CC[C@]2(C(=C1)CC[C@@H]1[C@@H]2CC[C@]2([C@H]1CC[C@@H]2C(=O)C)C)C': 540,\n",
       " 'Clc1ccc(cc1)C[C@@]1(CC[N@H+](CC1)Cc1ccccc1)C(=O)CC(=O)C(=O)O': 541,\n",
       " 'O[C@@H]1[C@H](C)O[C@@H]([C@H]([C@@H]1O)O)O': 542,\n",
       " 'O[C@@H]1[C@@H](CO[P@@](=O)(OP(=O)(O)O)O)O[C@H]([C@@H]1O)n1cnc2c1ncnc2N': 543,\n",
       " 'O=Cc1ccc(cc1)NC(=N)N': 544,\n",
       " 'C[N+]1(C)[C@@H]2CC[C@H]1C[C@@H](C2)OC1c2ccccc2CCc2c1cccc2': 545,\n",
       " 'O=C(N[C@H](C(=O)O)CCC(=O)O)N[C@H](C(=O)O)CCCC(=O)Nc1ccc(cc1)F': 546,\n",
       " '[NH3+]CCCCCC(=O)N[C@@H](C(C)(C)C)C(=O)N[C@@H](C(C)(C)C)C(=O)N[C@H](C(=O)N[C@H]([C@@H](C(=O)N[C@@H](c1ccccc1)CC)O)C)CC(=O)N(C)C': 547,\n",
       " 'O=CN([C@](CS(=O)(=O)N1CC[C@H](CC1)CCc1c(C)noc1C)(C[C@@H](c1ncc(cn1)F)C)C)O': 548,\n",
       " 'ONC(=O)/C=C/C(=C/[C@H](C(=O)c1ccc(cc1)N(C)C)C)/C': 549,\n",
       " 'Fc1ccc(cc1)C1=NC(=N[C@@H]1c1ccncc1)c1ccc(cc1)[S@H](O)C': 550,\n",
       " 'Nc1ncc2c(n1)nc(c(c2)c1ccccc1)N1CCCC1': 551,\n",
       " 'OC(=O)c1ccc(cc1)Cn1c(=O)c2cc(C#CCc3ccccc3)ccc2n(c1=O)C': 552,\n",
       " 'Nc1cccc(c1)B(O)O': 553,\n",
       " 'OCc1ccc(c(c1)c1ccc(nc1)N1CCC(CC1)C(=O)O)F': 554,\n",
       " 'OC[C@H]([C@H]([C@@H]1O[C@](O[C@@H]2[C@@H](O)[C@@H](O[C@@H]([C@@H]2O)CO)O[C@@H]2[C@@H](CO)O[C@@H]([C@@H]([C@H]2O[C@@H]2O[C@@H](C)[C@H]([C@H]([C@@H]2O)O)O)NC(=O)C)O)(C[C@@H]([C@H]1NC(=O)C)O)C(=O)O)O)O': 555,\n",
       " 'O=C([C@@H]1SCCN1C(=O)Cn1nc(c2c1nccc2)C(=O)N)Nc1cccc(n1)Br': 556,\n",
       " 'COC(=O)N[C@@H](C(c1ccccc1)c1ccccc1)C(=O)Nc1ccccc1CC[C@H]1OC[C@H]([NH2+]C1)COC(=O)NCc1ccccc1': 557,\n",
       " 'C/N=C/1\\\\CCCC=C1C(=O)O[C@@H]1[C@@H](CO[P@@](=O)(O[P@](=O)(OP(=O)(O)O)O)O)O[C@H]([C@@H]1O)n1cnc2c1ncnc2N': 558,\n",
       " 'O=C1Nc2c(/C/1=C\\\\1/Nc3c(C1=O)cccc3)cc(cc2)S(=O)(=O)O': 559,\n",
       " 'CC(=O)C[C@@H](c1c(=O)oc2c(c1O)cccc2)c1ccccc1': 560,\n",
       " 'CC[C@@]1(C)CC(=CNC1=O)c1ccc(s1)S(=O)(=O)Nc1cc2c(=O)[nH]ccc2cc1F': 561,\n",
       " 'S=c1[nH]c2ccccc2c(=O)[nH]1': 562,\n",
       " 'CC[NH+](Cc1cc(ccc1O)Nc1ccnc2c1ccc(c2)Cl)CC': 563,\n",
       " 'OC[C@H]([C@@H]([C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)C[NH+]1CCOCC1)CCc1ccccc1)CC(C)C)Cc1ccccc1)CC(C)C)O)C': 564,\n",
       " 'NC(=N)NCCCCNC(=O)[C@H](NC(=O)[C@H](CC(=O)N[C@H](C(=O)N1CCC[C@H]1C(=O)N)CCCNC(=N)N)O)CC(C)C': 565,\n",
       " 'O[C@@H]1[C@@H](CO[P@](=O)(O[P@](=O)(OP(=O)(O)O)O)O)O[C@H]([C@@H]1O)n1cnc2c1ncnc2N': 566,\n",
       " 'C=C[C@@H]1C[C@]1(NC(=O)[C@@H]1C[C@H](CN1C(=O)[C@H](NC(=O)CC)C)Oc1nccc2c1cc(Br)cc2)C(=O)NS(=O)(=O)C1CC1': 567,\n",
       " 'CC(C[C@@H](C(=O)N[C@H](C(=O)N1CCC[C@H]1C(=O)N1CCC[C@H]1C(=O)N1CCC[C@H]1C(=O)O)Cc1ccc(cc1)O)NC(=O)[C@@H]1CCCN1C(=O)[C@@H]1CCCN1C(=O)[C@@H]1CCCN1C(=O)[C@@H]1CCCN1C(=O)[C@H](C(C)C)NC(=O)[C@@H]1CCCN1C(=O)[C@H](C(C)C)NC(=O)[C@H](Cc1ccccc1)[NH3+])C': 568,\n",
       " 'CC(c1ccc(cc1)S(=O)(=O)N[C@@H]1SC(=N[NH+]1C)S(=O)(=O)N)(C)C': 569,\n",
       " 'NC(=O)c1cnc(nc1Nc1cccc(c1)n1nccn1)N[C@@H]1CCCC[C@@H]1[NH3+]': 570,\n",
       " 'Nc1cccc(n1)Cn1c(ccc1c1ccccc1Cl)c1ccc(cc1)Oc1cncnc1': 571,\n",
       " 'OC[C@H]1N[C@@H](NO)[C@@H]([C@H]([C@@H]1O)O)O': 572,\n",
       " 'COc1cccc(n1)c1cc(F)ccc1[C@@H]1NC(=O)c2c(C1)nc(nc2C)N': 573,\n",
       " 'O=C(CC(=O)O)C[NH2+]Cc1cc(F)c(cc1F)OCc1cccc(c1C)c1ccc2c(c1)OCCO2': 574,\n",
       " 'O=C(c1ccc2c(c1)nccn2)NCc1ccco1': 575,\n",
       " 'O[C@@H]1[C@H](O)[C@H](O[C@H]1N1C=CN2C1=CC=C(C2)C(=O)NCc1ccc(cc1)S(=O)(=O)c1cc(F)cc(c1)F)COP(=O)(O)O': 576,\n",
       " 'OC(=O)Cc1cc(Cl)c(c(c1)Cl)Oc1ccc(c(c1)C(C)C)O': 577,\n",
       " 'CCC[C@@H](C(=O)N[C@H](C(=O)NCC(=O)O)CCCC[NH2+]C)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H]([NH3+])C)CC1=NC=NC1)CCCNC(=N)N)CC(C)C)CC(C)C': 578,\n",
       " 'Clc1ccc(cc1Cl)Cn1c(=O)n(C)c2c(c1=O)n(C)c(n2)N1CCOCC1': 579,\n",
       " 'O=C1CCCCC/C=C/CCN(C(=O)c2c(C1)c(Cl)c(cc2O)O)C': 580,\n",
       " 'O=C(C1CC1)Nc1nccc(c1)c1ccc2c(c1)cncc2': 581,\n",
       " 'O=C(Nc1c(O)ccc(c1O)C(=O)O)CC[C@]1(C)C(=O)C=C[C@@]23[C@H]1C[C@H](C[C@H]3O)C(=C)C2': 582,\n",
       " 'NC(=O)CC[C@@H](C(=O)N[C@H](C(=O)O)[C@H](O)C)NC(=O)[C@@H](NC(=O)[C@H]([C@H](O)C)NC(=O)[C@@H](NC(=O)[C@@H]([NH3+])C)CCCNC(=N)N)CCCC[NH+](C)C': 583,\n",
       " 'COc1ccc(cc1)[C@@H](N(C(=O)c1o[nH]c(=O)c1)C)C(=O)Nc1ccc(cc1)[Si](C)(C)C': 584,\n",
       " 'OC[C@@H](C(=O)N[C@H](C(=O)N1CCC[C@H]1C(=O)N[C@H](C(=O)O)[C@H](O)C)CC(=O)O)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@H](CO)[NH3+])Cc1ccccc1)CC(C)C)CCC(=O)N)CCCNC(=N)N)Cc1ccc(cc1)OP(=O)(O)O)COP(=O)(O)O': 585,\n",
       " 'C[NH2+][C@H](C(=O)N[C@@H](C(C)(C)C)C(=O)N1C[C@H](C[C@H]1C(=O)N[C@@H]1CCCc2c1cccc2)Oc1ccccc1)C': 586,\n",
       " 'CCOC(=O)n1ccc2c1cc(cc2)C(=O)Nc1cc(ccc1C)C(=O)N[C@@H]1C=C1': 587,\n",
       " 'N=C1SC[C@H]2[C@@](N1)(OCC2)c1cc(ccc1F)NC(=O)c1ccc(cn1)Br': 588,\n",
       " 'OC(=O)Cc1ccccc1NC(=O)c1ccc(cc1)Cc1c[nH]c2c1c(=O)[nH]c(=N)[nH]2': 589,\n",
       " 'CC(=O)N1CCc2c(C1)c(nn2[C@@H]1COCC1)N1CCCc2c1ccc(c2)c1cnn(c1)C': 590,\n",
       " 'OCc1nccc(c1)c1c[nH]nc1c1ccc(cc1)F': 591,\n",
       " 'NC(=O)C[C@@H](C(=O)N[C@H]([C@@H](C[N@@H+]1C[C@H]2CCCC[C@H]2C[C@H]1C(=O)NC(C)(C)C)O)Cc1ccccc1)NC(=O)c1ccc2c(n1)cccc2': 592,\n",
       " 'N#C/N=C/1\\\\SCCN1Cc1ccc(nc1)Cl': 593,\n",
       " 'CC(C[C@@H](C(=O)NS(=O)(=O)OC[C@H]1O[C@H]([C@@H]([C@@H]1O)O)n1ccc(=O)n(c1=O)C)[NH3+])C': 594,\n",
       " 'COc1ccc(cc1OC)OCCN1C[C@H]2CCC[C@@H](C1=O)N2S(=O)(=O)c1ccc2c(c1)sc(=O)[nH]2': 595,\n",
       " '[NH3+]CCCCC[C@@H](SC[C@H]1O[C@H]([C@@H]([C@@H]1O)O)n1cnc2c1ncnc2N)CC[NH3+]': 596,\n",
       " 'Nc1snnc1c1ccc(cc1)C(F)(F)F': 597,\n",
       " 'C=CCOC(=O)N[C@H]([C@@H](c1noc(n1)Cc1ccc(cc1)C(=O)NC1Cc2c(C1)cccc2)O)CC[C@H]1C[NH2+]CC1': 598,\n",
       " 'OC[C@H]1O[C@H](Oc2ccc(cc2)N(=O)=O)[C@@H]([C@H]([C@@H]1O)O)NC(=O)C': 599,\n",
       " 'O=C1CC[C@H](N1)COc1nccc2c1cc(OC(C)C)c(c2)C(=O)N': 600,\n",
       " 'OC(=O)[C@H](N1C(=O)[C@@H]2[C@H](C1=O)[C@@H]1C[C@H]2CC1)C': 601,\n",
       " 'CCCCCCC[C@@H]([NH2+][C@H]([C@@H](/C=C/CCCCCCCCCCCCC)O)CO[C@H]1O[C@H](CO)[C@H]([C@@H]([C@H]1O)O)O[C@@H]1O[C@H](CO)[C@@H]([C@@H]([C@H]1O)O)O)O': 602,\n",
       " 'CO[C@@H]1[C@@H](OC(=O)N)[C@@H](O)[C@@H](OC1(C)C)Oc1ccc2c(c1C)oc(=O)c(c2O)NC(=O)c1ccc(c(c1)CC=C(C)C)O': 603,\n",
       " 'C#Cc1cccc(c1)COC(=O)NCCCC[C@@H](C(=O)O)[NH3+]': 604,\n",
       " '[NH3+]CCNS(=O)(=O)c1ccc(cc1)c1ccc(cc1)CSC1=NC(=O)[C@@H]2C(=N1)CCC2': 605,\n",
       " 'OC(=O)CC[C@@H](C(=O)N[C@H](C(=O)N[C@H]([C@H](O)C)CC(=O)O)C(C)C)NC(=O)[C@@H](NC(=O)C)CC(=O)O': 606,\n",
       " 'OC[C@H]([C@@H]([C@H](Cc1ccccc1)NC(=O)[C@H](Cc1ccc(cc1)OC)NC(=O)[C@@H](NC(=O)C[NH+]1CCOCC1)C)O)C': 607,\n",
       " 'O[C@H]1C[C@@H](O[C@@H]1CO[P@](=O)(O[P@](=O)(OP(=O)(O)O)O)O)n1cc(C)c(=O)[nH]c1=O': 608,\n",
       " 'Clc1ccc(cc1)C[C@@]1(CC[N@@H+](CC1)Cc1ccccc1)[C@@H](CC(=O)C(=O)O)O': 609,\n",
       " 'Nc1[nH]c(=O)c2c(n1)n(cn2)C[C@@H](OCCP(=O)(O)O)COCCP(=O)(O)O': 610,\n",
       " 'Fc1ccc(c(c1)Cl)c1cc(cc2c1CNC(=O)N2c1c(Cl)cccc1Cl)[C@@H]1CC[N@H+](CC1)C(C)C': 611,\n",
       " 'O[C@@H]1[C@@H](OP(=O)(O)O)[C@H](OP(=O)(O)O)[C@H]([C@@H]([C@@H]1O)OP(=O)(O)O)O': 612,\n",
       " '[NH3+]CC(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N1CCC[C@H]1C(=O)N[C@H](C(=O)N[C@H](C(=O)O)CC)[C@H](O)C)CC(=O)N)CCC(=O)O)Cc1ccc(cc1)OP(=O)(O)O': 613,\n",
       " 'O=c1[nH]c2CCCc2c(c1)C(F)(F)F': 614,\n",
       " 'OC(=O)[C@H](CC1=c2ccc(cc2=NC1)Br)SS[C@H](C(=O)O)CC1=c2ccc(cc2=NC1)Br': 615,\n",
       " 'Nc1nc(ncc1c1cccc(c1Cl)Cl)N1C[C@H](C)[NH2+][C@@H](C1)C': 616,\n",
       " 'O=c1[nH]c(=O)n(cc1/C=C/P(=O)(O)O)[C@@H]1O[C@H]2[C@@H](C1)O[C@](OC2)(c1ccccc1)P(=O)(O)O': 617,\n",
       " 'CCCCNc1ncc(c(n1)N[C@@H]1CC[C@H](CC1)O)c1ccccn1': 618,\n",
       " 'ONC(=O)c1nnn(c1)c1cc(Cl)ccc1Oc1ccc(cc1)F': 619,\n",
       " 'OC(=O)c1ccc2c3c1cccc3c(=O)n1c2nc2c1cccc2': 620,\n",
       " '[NH3+]CCCC[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)O)CCCNC(=N)N)C)[C@H](O)C)CCC(=O)N)NC(=O)[C@H]([C@H](O)C)NC(=O)[C@@H](NC(=O)[C@@H]([NH3+])C)CCCNC(=N)N': 621,\n",
       " 'OC(=O)[C@H](CCSC[C@H]1O[C@H]([C@@H]([C@@H]1O)O)n1cnc2c1ncnc2N)[NH3+]': 622,\n",
       " 'O=C(Nc1cccc(c1)C(=O)NCCc1ccc(cc1)S(=O)(=O)N)CN1CC(=O)*[Cu]*C(=O)C1': 623,\n",
       " 'N#Cc1ccc2c(c1)c(C)c(cc2N(C(=O)CC)C)Oc1ccccc1OCCn1ccc(=O)[nH]c1=O': 624,\n",
       " 'N#C[C@@H]1CC[C@H](CN1C=O)[NH2+]OS(=O)(=O)O': 625,\n",
       " 'CC[C@@H]([C@@H](C(=O)N[C@H](C(=O)O)C)NC(=O)[C@H](C(C)C)NC(=O)[C@H](C(C)C)NC(=O)[C@@H](NC(=O)[C@H]([C@H](O)C)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@H](CO)[NH3+])CCC(=O)O)C)C)C)CO)C)C': 626,\n",
       " 'O=C[C@H](C[C@@H]1CCC[NH2+][C@H]1O)NC(=O)[C@@H](NC(=O)c1noc(c1)C)Cc1ccc(cc1)F': 627,\n",
       " 'O=C[C@H]([C@H](NS(=O)(=O)O)CNC(=O)NCc1cc(=O)c(cn1O)O)NC(=O)/C(=N\\\\OC(C(=O)O)(C)C)/c1csc(n1)N': 628,\n",
       " 'Cc1ncc(cn1)c1ccn2c(n1)c(cn2)c1ccc2c(c1)[C@](C)(Cc1ccccc1)C(=O)N2': 629,\n",
       " 'COc1ccc2n(c1)ncc2C(=O)Nc1ccc(cc1C)NC(=O)c1nn(c2c1cccc2)C': 630,\n",
       " 'CC[C@@H]([C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)O)CCCC[NH3+])CC1=NC=NC1)NC(=O)[C@H](C(C)C)NC(=O)C[C@@H]([C@@H](NC(=O)[C@H](CC1=NC=NC1)NC(=O)[C@H](Cc1ccccc1)NC(=O)[C@@H]1CCCN1C(=O)[C@H](CC1=NC=NC1)NC(=O)[C@@H]1CCC[NH2+]1)CC(C)C)O)C': 631,\n",
       " 'CC(=O)Nc1ccc(c(c1)[C@H]1CCCN1C(=O)[C@@H](c1ccccc1)Nc1ccc2c(c1)ccnc2N)S(=O)(=O)C(C)C': 632,\n",
       " '[NH3+]CCCC[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)O)C)C)NC(=O)[C@H]([C@H](OP(=O)(O)O)C)NC(=O)[C@@H](NC(=O)[C@@H]([NH3+])C)CCCNC(=N)N': 633,\n",
       " 'O=C([C@H]1C=c2c(=N1)cccc2)N[C@H]1CCCN(C1)c1[nH]nc(c1)c1ccncc1': 634,\n",
       " 'NC(=N)NCCC[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N)CCCNC(=N)N)C)[C@H](O)C)CCC(=O)N)CCCC[N+](C)(C)C)[C@H](O)C)NC(=O)[C@@H]([NH3+])C': 635,\n",
       " 'Fc1ccc(c(c1)F)c1cc(cc2c1cnc(=O)n2c1c(Cl)cccc1Cl)N1CC[NH2+]CC1': 636,\n",
       " 'Oc1ccc(cc1O)c1cc(=O)c2c(o1)cccc2': 637,\n",
       " 'Fc1cc(C[C@@H]2CS(=O)(=O)C[C@@H]([C@H]2O)[NH2+]Cc2cccc(c2)C(F)(F)C)cc(c1N)Br': 638,\n",
       " 'N=CC1(CC1)NC(=O)[C@H]1[NH2+]C[C@@H](C1)S(=O)(=O)c1ccccc1C(F)(F)F': 639,\n",
       " 'Clc1ccc(cc1)C[C@H]1[NH2+]CC[C@@H](C1)NC(=O)c1c[nH]c2c1cc(Br)cc2': 640,\n",
       " 'C[N@@H+]1CC[C@H](CC1)Oc1ccc(cc1)OCc1ccccc1NC(=O)c1cc2c(n1C)ccs2': 641,\n",
       " 'NC(=N)NCCC[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)O)C)Cc1ccc(cc1)O)NC(=O)[C@H](C(C)C)NC(=O)[C@H](C(C)C)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@H](Cc1ccc(cc1)O)[NH3+])CC1=NC=NC1)COP(=O)(O)O': 642,\n",
       " 'O[C@H](C[N@@H+]1CCc2c(C1)cccc2)CNC(=O)c1cccc(c1)NC1CCOCC1': 643,\n",
       " 'Fc1cc(C[C@@H]2CS(=O)(=O)C[C@@H]([C@H]2O)[NH2+]Cc2cccc(c2)C(C)(C)C)cc(c1N)C[C@@H](C(F)(F)F)O': 644,\n",
       " 'OC[C@@H](C(=O)O)NC(=O)[C@@H]1CCCN1C(=O)[C@@H]1CCCN1C(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H]1CCCN1C(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@H]([C@H](O)C)[NH3+])CC(=O)O)CC(C)C)COP(=O)(O)O)CC(=O)O': 645,\n",
       " 'FC1=CC=c2c(C1)c1C(=O)N=CCc1c1c2=NC(=N1)C(C)(C)C': 646,\n",
       " 'OCC(=O)[C@@]12O[C@@H](O[C@@H]1C[C@@H]1[C@]2(C)C[C@H](O)[C@H]2[C@H]1CCC1=CC(=O)C=C[C@]21C)C1CCCCC1': 647,\n",
       " 'CSCC[C@H]([NH3+])C(=O)O': 648,\n",
       " 'Clc1ccc(c(c1)Cl)C[C@H](C(=O)N1Cc2c(C1)cccc2)[NH3+]': 649,\n",
       " 'CC([C@H](c1nc2cc(Cl)ccc2c(=O)n1Cc1ccccc1)N(C(=O)c1ccc(cc1)C)C)C': 650,\n",
       " 'Fc1c(F)c(c(c(c1S(=O)(=O)CCc1ccccc1)NCc1ccccc1)F)S(=O)(=O)N': 651,\n",
       " 'OC[C@@H](NC(=O)[C@H](C(C)C)NC(=O)[C@@H](NC(=O)OCc1ccccc1)C)Cc1ccccc1': 652,\n",
       " 'CC(C[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N1CCC[C@H]1C(=O)O)Cc1ccc(cc1)O)CC(=O)N)NC(=O)[C@@H](NC(=O)[C@@H]1CCCN1C(=O)[C@H](C(C)C)NC(=O)[C@@H](NC(=O)[C@H]([C@H](O)C)NC(=O)[C@H](Cc1ccccc1)[NH3+])CC(=O)O)C)C': 653,\n",
       " 'O=C1CCCN1c1cccc(c1)NS(=O)(=O)c1c(F)cccc1F': 654,\n",
       " 'OC(=O)CC[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)O)C(C)C)C)NC(=O)[C@@H]([NH3+])C': 655,\n",
       " 'C[NH2+]CC[N@@H+]1CC[C@@H](CC1)c1nccc(c1)CN(C(=O)c1cccc(c1)Oc1ccccc1)C': 656,\n",
       " 'O=CN([C@H](C1=c2ccc(cc2=N[C@H]1C(=O)O)Cl)C(=O)NC(C)(C)C)Cc1ccc(c(c1)F)F': 657,\n",
       " '[NH3+]CCCC[C@@H](C(=O)N[C@@]1(C)CC/C=C/CC[C@@H](C)C[C@](C)(NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC1=O)CC(C)C)CC1=NC=NC1)CCCNC(=N)N)C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N)CO)CC(=O)O)CCC(=O)N)CC(C)C)NC(=O)[C@H](CC1=NC=NC1)NC(=O)C': 658,\n",
       " 'O=c1[nH]cnc2c1ccnc2n1ncc(c1)CC[NH+]1CCCCC1': 659,\n",
       " 'C[S@@H](C[C@H]1O[C@H]([C@@H]([C@@H]1O)O)n1cnc2c1ncnc2N)CC[C@@H](C(=O)O)[NH3+]': 660,\n",
       " 'O=C(N[C@H](C(=O)N[C@H](C(=O)N)C(C)C)CC(=O)O)CNC(=O)[C@@H]([NH3+])C': 661,\n",
       " 'OP(=O)(C(P(=O)(O)O)(Cc1cccc(c1)c1cccc(c1)NS(=O)(=O)c1ccc2c(c1)cccc2)O)O': 662,\n",
       " 'CC(C[C@@H](C(=O)NCC(=O)N[C@H](C(=O)O)CCC(=O)O)NC(=O)[C@H](CCCNC(=N)N)[NH3+])C': 663,\n",
       " 'OC(=O)CCCC[C@@H]1SC[C@H]2[C@@H]1NC(=O)N2': 664,\n",
       " 'C[C@H]1C(=O)N(C(=O)N1Cc1ccnc2c1cccc2)c1ccc(cc1)S(=O)(=O)C(F)(F)F': 665,\n",
       " '[NH3+]C[C@H](COc1cccc(c1)c1cc(N2CCOCC2)c2c(n1)n(nc2)C(C)C)O': 666,\n",
       " 'O=C(NCC[C@@H](C(=O)O)F)CCNC(=O)[C@@H](C(CO[P@](=O)(O[P@@](=O)(OC[C@H]1O[C@H]([C@@H]([C@@H]1OP(=O)(O)O)O)n1cnc2c1ncnc2N)O)O)(C)C)O': 667,\n",
       " 'CC[C@@H]([C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)NCC(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N1CCC[C@H]1C(=O)O)CC(=O)O)[C@H](O)C)CCC(=O)N)[C@H](O)C)C)CS)NC(=O)[C@H]([C@H](O)C)NC(=O)[C@@H](NC(=O)[C@@H]([NH3+])C)Cc1ccc(cc1)O)C': 668,\n",
       " 'C[NH2+]CCCCc1c(cnc2c1cc(cc2)c1cncc(c1)O)c1cc(C)cc(c1)C': 669,\n",
       " 'N#Cc1cc(Cl)c(c(c1)c1ncnc(c1)n1cccn1)F': 670,\n",
       " 'CC(=O)Nc1nccc(c1)c1[nH]c2c(c1c1ccc(nc1)F)C(=O)NCC2': 671,\n",
       " 'OC[C@@H](C(=O)N1CCC[C@H]1C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)O)C)CO)Cc1ccc(cc1)O)CC1=NC=NC1)CC(C)C)NC(=O)[C@@H](NC(=O)[C@H](C(C)C)NC(=O)[C@@H](NC(=O)[C@H](CO)[NH3+])CC1=CN=C2[C@@H]1C=CC=C2)Cc1ccc(cc1)O': 672,\n",
       " 'Cc1cc(c2c(c1)c(=O)c(c(o2)c1ccc(c(c1)O)O)O)N1CCCS1(=O)=O': 673,\n",
       " 'COc1ccc(c(c1)OC(C)C)C1=N[C@H]([C@H](N1C(=O)N1CCNC(=O)C1)c1ccc(cc1)Cl)c1cc(Br)ccc1F': 674,\n",
       " '[NH3+]CCCC[C@@H](C(=O)N[C@H](C(=O)O)CCC(=O)O)NC(=O)[C@@H](NC(=O)[C@H]([C@H](CC)C)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@H](CCC(=O)O)[NH3+])CC(=O)O)CCSC)Cc1ccc(cc1)O)CCCNC(=N)N)C)C)CC(=O)O)CCC(=O)O)C': 675,\n",
       " 'Cc1nc(nc(c1Cl)NCCc1ccccc1)c1ccccn1': 676,\n",
       " 'Oc1ccc(c(c1)O)c1noc(c1c1ccccc1)C': 677,\n",
       " '[NH3+]CCCC[C@@H](C(=O)N1CCC[C@H]1C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N)C(C)C)CC(=O)N)C(C)C)Cc1ccc(cc1)OP(=O)(O)O)Cc1ccccc1)[NH3+]': 678,\n",
       " 'CC[C@@H]([C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)O)CC(C)C)C(C)C)[C@H](CC)C)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H]([NH3+])C)CCC(=O)O)CCC(=O)O)CCCNC(=N)N)C': 679,\n",
       " 'CCCC[C@@H]1N(C)C(=O)[C@@H](NC(=O)[C@H](CO)NC(=O)[C@@H](NC(=O)[C@H](CO)NC(=O)[C@H](CC(C)C)NC(=O)[C@H](CC2=NC=NC2)NC(=O)[C@@H]2CCCN2C(=O)[C@@H](NC(=O)[C@@H](N(C(=O)[C@@H](NC(=O)CSC[C@H](NC(=O)[C@@H](NC(=O)[C@@H](N(C1=O)C)CCCC)CCCNC(=N)N)C(=O)NCC(=O)N)Cc1ccccc1)C)C)CC(=O)N)CC1=CN=C2[C@@H]1C=CC=C2)CC1=CN=C2[C@@H]1C=CC=C2': 680,\n",
       " 'O=C(Cc1ccccc1)Nc1nnc(s1)CCSCCc1nnc(s1)NC(=O)Cc1ccccc1': 681,\n",
       " 'NC1=N[C@H](OCC2CCCCC2)C2=NC(=NC2=N1)c1cccc(c1C)O': 682,\n",
       " 'OC[C@@H]([C@H]1O[C@H](O)[C@H]([C@H]([C@@H]1O)O)O)O': 683,\n",
       " '[NH3+]CCCC[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)O)[C@H](O)C)CCC(=O)N)NC(=O)[C@H]([C@H](O)C)NC(=O)[C@@H](NC(=O)[C@@H]([NH3+])C)CCCNC(=N)N': 684,\n",
       " 'Clc1cc(Cl)c2c(c1)[C@@H](O)C(=C(O2)c1ccc(cc1)C(=O)O)O': 685,\n",
       " 'C[NH+](CC#Cc1ccc(c(c1)F)OCCCc1sc(nc1C(=O)O)N1CCc2c(C1)c(ccc2)C(=O)Nc1nc2c(s1)cccc2)C': 686,\n",
       " 'O=C(N1CCC[C@@H]1C[NH+]1CCCC1)c1ccc(cc1)c1cnc(c(n1)NCc1c(Cl)cccc1Cl)N': 687,\n",
       " 'O=C(N([C@@H](C(=O)NC(C)(C)C)c1cccnc1)c1ccc(cc1)C(C)(C)C)c1ccco1': 688,\n",
       " 'O=C(N[C@@H](C(S(=O)(=O)C)(C)C)C(=O)N[C@H]([C@@H](C[N@@H+]1C[C@H]2CCCC[C@H]2C[C@H]1C(=O)NC(C)(C)C)O)Cc1ccccc1)COc1cccnc1': 689,\n",
       " 'OC[C@@H]1CCC(=O)N(C)CCCC[C@@H](C(=O)N[C@H](C(=O)N1)Cc1ccc(cc1)F)NC(=O)c1noc(c1)C': 690,\n",
       " 'CCC(CN(S(=O)(=O)c1ccc(cc1)OC)C[C@H]([C@H](Cc1ccccc1)NC(=O)O[C@H]1CO[C@@H]2[C@H]1CCO2)O)CC': 691,\n",
       " 'Nc1nccc(c1)Cn1c(C(=O)O)c(c2c1ccc(c2)C(F)(F)F)n1c(=O)[nH]c2c(c1=O)csc2': 692,\n",
       " 'O=C([C@@H](NC(=O)[C@@H]1CCCN1C(=O)[C@H](Cc1ccc(cc1)O)NC(=O)[C@@H]1CCCN1C(=O)[C@@H]1CCC[NH2+]1)C)N[C@H](C(=O)O)CC1=CN=C2[C@H]1C=CC=C2': 693,\n",
       " 'O=NC(=O)CN(S(=O)(=O)c1ccc(cc1)OC)CC(C)C': 694,\n",
       " 'N#Cc1cnc(nc1c1sc(nc1C)NC)Nc1cccc(c1)C(=O)N1CCOCC1': 695,\n",
       " '[TeH]C[C@@H](COc1ccc(cc1)S(=O)(=O)N)O': 696,\n",
       " 'OCC[NH+]1CCN(CC1)c1nc(nc(c1Cl)C)c1ccccn1': 697,\n",
       " 'O=c1[nH]cnc2c1cccc2': 698,\n",
       " '[NH3+]CCCC[C@@H](C(=O)N[C@H](C(=O)N1CCC[C@H]1C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)O)[C@H](O)C)[C@H](CC)C)Cc1ccc(cc1)O)CCC(=O)O)CCSC)CC(C)C)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@H](CC(=O)O)[NH3+])CC(=O)O)Cc1ccc(cc1)O': 699,\n",
       " '[NH3+]CCCC[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)O)CC(C)C)CO)CC(C)C)CCCNC(=N)N)CCCNC(=N)N)[C@H](CC)C)NC(=O)[C@H](C(C)C)NC(=O)[C@H](C(C)C)NC(=O)[C@@H]1CCCN1C(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@H](C(C)C)NC(=O)[C@@H](NC(=O)[C@H]([C@H](O)C)[NH3+])CC(=O)N)CO)CC(C)C)Cc1ccc(cc1)O': 700,\n",
       " 'N#C[C@@H]1CN(C1)C(=O)[C@H](NC(=O)C1=CN=C2[C@@H]1N=C(C=N2)c1nn(c2c1ccc(c2)F)C)C': 701,\n",
       " 'CC[C@@H]([C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)O)CC(=O)O)Cc1ccc(cc1)O)Cc1ccccc1)CC(=O)O)CO)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@H](C(C)C)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@H]([C@H](O)C)NC(=O)[C@H](CCCNC(=N)N)[NH3+])Cc1ccccc1)CCCNC(=N)N)CCC(=O)N)CCC(=O)N)CO)CO)C': 702,\n",
       " '[NH3+]CCCC[C@@H](C(=O)N[C@H](C(=O)O)C)NC(=O)[C@@H]1CCCN1C(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)C[NH3+])CO)C)CCCNC(=N)N)C)CCC(=O)O': 703,\n",
       " 'CCCN(C(=O)c1cc(C)cc(c1)C(=O)N[C@H]([C@@H](C[NH2+]Cc1cccc(c1)OC)O)Cc1cc(F)cc(c1)F)CCC': 704,\n",
       " 'NC(=N)NCCC[C@@H](C(=O)N1CCC[C@H]1C(=O)N[C@H](C(=O)N1CCC[C@H]1C(=O)O)C)NC(=O)[C@H]([C@H](O)C)NC(=O)[C@H](CC(=O)O)[NH3+]': 705,\n",
       " 'CCCc1scc(c1C[C@@H](c1[nH]ccc(=O)n1)/N=C/1\\\\NC(C)(C)Cc2c1ccc(c2)Cl)C1=CN=NC1': 706,\n",
       " 'CCOc1ccc2c(c1)ccc(c2)c1nn(c2c1c(N)ncn2)C[C@@H]1CC[N@H+](CC1)C': 707,\n",
       " 'OC[C@H]1O[C@@H](NC(=S)/N=N/Cc2ccncc2)[C@@H]([C@H]([C@@H]1O)O)O': 708,\n",
       " 'C[C@@H](C(O)(C)C)NC(=O)C1=CN=C2[C@@H]1N=C(C=N2)C1CC1': 709,\n",
       " 'CC[C@@H]([C@@H](C(=O)N[C@H](C(=O)NCC(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N1CCC[C@H]1C(=O)O)Cc1ccc(cc1)O)[C@H](O)C)[C@H](O)C)C(C)C)NC(=O)[C@@H](NC(=O)[C@H](Cc1ccccc1)[NH3+])CC(=O)N)C': 710,\n",
       " 'CC1=NC2=CC=CCC2=C1C[NH+]1CCN(CC1)c1ccccn1': 711,\n",
       " 'CCn1ccn(c(=O)c1=O)C(=O)N[C@H](c1ccc(cc1)O)C(=O)NCB(O)O': 712,\n",
       " 'C[N@@H+]1CCN(CC1)C(=O)c1ccc(cc1)[C@H]1N=c2c(=C1)c(ncn2)c1cccc(c1C)NC(=O)c1ccc(cc1)C(C)(C)C': 713,\n",
       " 'OC[C@H](C[C@@H]1CCNC1=O)NC(=O)[C@@H](n1ccoc1=O)CC(C)C': 714,\n",
       " 'CO[C@@H]1C[C@@H](CC[C@H]1O)C[C@H]([C@H]1OC(=O)[C@@H]2CCCCN2C(=O)C(=O)[C@]2(O)O[C@@H](CC[C@H]2C)C[C@H](OC)/C(=C/C=C/C=C/[C@H](C[C@H](C(=O)[C@@H]([C@@H](/C(=C/[C@H](C(=O)C1)C)/C)O)OC)C)C)/C)C': 715,\n",
       " 'CC(C[C@@H](C(=O)N[C@H](C(=O)N)C)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H]1CCCN1C(=O)[C@@H]([NH3+])C)CCCNC(=N)N)CCCCNC(=O)C)CCC(=O)N)C': 715,\n",
       " 'COc1cc(CC[C@@H]2N=NC(=C2)NC(=O)c2ccc(cc2)N2C[C@H](C)[NH2+][C@@H](C2)C)cc(c1)OC': 716,\n",
       " 'O[C@@H]1[C@@H](CO[P@@](=O)(O[P@@](=O)(O[P@@](=O)(O[P@@](=O)(OC[C@H]2O[C@H]([C@@H]([C@@H]2O)O)n2cnc3c2ncnc3N)O)O)O)O)O[C@H]([C@@H]1O)n1cnc2c1ncnc2N': 716,\n",
       " 'O[C@@H]1[C@@H](CO[P@](=O)(OP(=O)(O)O)O)O[C@H]([C@@H]1O)n1cnc2c1ncnc2N': 717,\n",
       " 'O=C(c1ccc(cc1)S(=O)(=O)N)CSc1nc(C)cc(n1)C': 717,\n",
       " 'CC[C@@H](C(=O)O)NC(=O)[C@@H](NC(=O)[C@H]([C@H](O)C)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@H]([C@H](O)C)NC(=O)[C@@H](NC(=O)[C@@H]([NH3+])C)CCC[NH3+])C)C)C': 718,\n",
       " 'O=C(c1ccccc1)O[C@@H]1C[C@@H]2CC[C@H](C1)[NH2+]2': 718,\n",
       " 'OC[C@H]1O[C@]2(ON=C(C2)c2ccccc2)[C@@H]([C@H]([C@@H]1O)O)O': 719,\n",
       " 'O=C(c1ccc(cc1)C(C)(C)C)Nc1cn2c(n1)ccc(c2)c1c[nH]nc1': 719,\n",
       " 'Clc1cc(nc(n1)N)N(C1CC1)Cc1cccs1': 720,\n",
       " 'O=CN1C[C@@H](CC[C@H]1C(=O)NC1CC[NH2+]CC1)[NH2+]OS(=O)(=O)O': 720,\n",
       " 'OC(=O)[C@H](CC1=CN=C2[C@@H]1C=CC=C2)[NH3+]': 721,\n",
       " 'CCNP(OCC)O': 721,\n",
       " 'O=NC(=O)[C@H](N(S(=O)(=O)c1ccc(cc1)OC)Cc1cccnc1)C(C)C': 722,\n",
       " 'C[N@@H+]1CC[N@H+](CC1)Cc1[nH]c(=O)c2c(n1)c1cc(Br)ccc1o2': 722,\n",
       " 'N#Cc1ccc(s1)[C@@]12CN(C[C@H]2C(=O)N(C(=N)N1)C)c1nc(C)c(c(n1)NC)F': 723,\n",
       " 'COc1cc(ccc1OC)Cc1nn2c(nc(c2c(=O)[nH]1)C)[C@H]([C@H](O)C)CCCc1ccccc1': 723,\n",
       " 'COc1cc2c(cc1OC)ncn2c1cc(c(s1)C(=O)N)O[C@@H](c1ccccc1Cl)C': 724,\n",
       " 'O[C@@H](C[C@H](CC(=O)O)O)CCc1c(C(C)C)c(nn1c1ccc(cc1)F)C(=O)N([C@@H](c1ccccc1)C)C': 724,\n",
       " 'OCCOCCn1ccc2c1c(ncn2)Nc1ccc(c(c1)Cl)Oc1cccc(c1)NC(=O)NC1CCCCC1': 725,\n",
       " 'CC(c1cc(ccc1O)Sc1ccc(c(c1)C(C)C)O)C': 725,\n",
       " 'ONC(=O)C[C@H](C(=O)N[C@H](C(=O)NCC(=O)NCC(=O)N)CCCNC(=N)N)Cc1ccc2c(c1)cccc2': 726,\n",
       " 'OP(=O)(OCC[N+](C)(C)C)O': 726,\n",
       " 'OC(=O)CC[C@@H](C(=O)O)[NH3+]': 727,\n",
       " 'CNC(=O)[C@H]1Cc2ccc(cc2CN1C(=O)OC(C)(C)C)NS(=O)(=O)O': 727,\n",
       " 'OC[C@@H](C(=O)N[C@H](C(=O)N[C@]1(C)CCC/C=C\\\\CCC[C@@](C)(NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC1=O)CCC(=O)N)CC(C)C)CC(C)C)C(=O)N[C@H](C(=O)N)Cc1ccccc1)CCCNC(=N)N)NC(=O)[C@H](Cc1ccc(cc1)O)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)C)CCCC[NH3+])CCCC[NH3+])CCCNC(=N)N': 728,\n",
       " 'CNC(=O)C([C@@H]([C@@H](NC(=O)[C@H](C(C)C)NC(=O)[C@H](C(C)C)NC(=O)CC(C)C)CC(C)C)O)(F)F': 728,\n",
       " 'CO[C@@H]1O[C@@H]2CO[C@](O[C@H]2[C@@H]([C@@H]1NC(=O)C)O)(C)C(=O)O': 729,\n",
       " 'BCNC(=O)c1c(OC)cccc1OC': 729,\n",
       " 'NC(=N)C1=CC=C2C(=CC(=N2)c2cccc(c2O)c2ccccc2)C1': 730,\n",
       " 'COCCNC(=N)NCCC[C@@H](C(=O)O)[NH3+]': 730,\n",
       " 'OC(=O)CNC(=O)C(=O)O': 731,\n",
       " 'OCCS(=O)(=O)c1c(N[C@H]2CCc3c2cccc3)c(F)c(c(c1F)F)S(=O)(=O)N': 731,\n",
       " 'ONC(=O)[C@H](CC1=CN=C2C1=CCC=C2)NC(=O)[C@@H](N(C(=O)[C@H](Cc1cnc[nH]1)NC(=O)OCc1ccccc1)C)Cc1ccccc1': 732,\n",
       " 'O=C1C/C(=N\\\\c2ccc(cc2)N2CC[N@H+](CC2)C)/C(=NN1c1ccc(cc1)Cl)c1ncn[nH]1': 732,\n",
       " 'NC(=N)c1ccccc1': 733,\n",
       " 'CC[C@@H]([C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)O)Cc1ccc(cc1)O)CCC(=O)N)C)C(C)C)NC(=O)[C@@H]1CCCN1C(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@H](CCC(=O)O)[NH3+])CO)CC(=O)O)C': 733,\n",
       " 'OC(=O)[C@@H](C[C@@H](C(=O)O)[NH3+])CCC(=O)N(O)C': 734,\n",
       " 'Oc1cc(O)c2c(c1)oc(c(c2=O)O)c1cc(O)c(c(c1)O)O': 734,\n",
       " 'CCCCCCCCCCCCCC(=O)CSCCNC(=O)CCNC(=O)[C@@H](C(CO[P@@](=O)(O[P@](=O)(OC[C@H]1O[C@H]([C@@H]([C@@H]1OP(=O)(O)O)O)n1cnc2c1ncnc2N)O)O)(C)C)O': 735,\n",
       " 'OC(=O)c1ccc(cc1)CCCc1c(O)[nH]c(=N)[nH]c1=O': 735,\n",
       " '[NH3+]CCCC[C@@H](C(=O)N1CCC[C@H]1C(=O)N[C@H](C(=O)N[C@H](C(=O)N1CCC[C@H]1C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)O)C)[C@H](CC)C)[C@H](O)C)CC(C)C)C)CC(C)C)C(C)C)NC(=O)[C@@H](NC(=O)[C@@H]([NH3+])C)CCCNC(=N)N': 736,\n",
       " 'O=C(N1CCc2c1cc(Cl)nc2)C[N@@H+]1CC[NH2+][C@@H](C1)C': 736,\n",
       " 'O[C@H]1C[C@H](O)C(=C)/C(=C/C=C\\\\2/CCC[C@]3([C@H]2CC[C@@H]3[C@@H](C[C@H]2C[C@@](C(=O)N2CCc2ccccc2)(C)O)C)C)/C1': 737,\n",
       " 'O=C(Nc1ccc(cc1)C(C)(C)C)NCCC[N@@H+](C(C)C)C[C@H]1O[C@H]([C@@H]([C@@H]1O)O)n1ccc2c1ncnc2N': 737,\n",
       " 'C[C@H]1N=CN=C1c1ccnc(c1)Nc1ccc(cc1)N1CCOCC1': 738,\n",
       " 'OC[C@H]1O[C@@H]2O[C@@H]3[C@@H](CO)O[C@@H]([C@@H]([C@H]3O)O)O[C@@H]3[C@@H](CO)O[C@@H]([C@@H]([C@H]3O)O)O[C@@H]3[C@@H](CO)O[C@@H]([C@@H]([C@H]3O)O)O[C@@H]3[C@H](O[C@H](O[C@@H]4[C@H](O[C@H](O[C@@H]5[C@H](O[C@H](O[C@H]1[C@@H]([C@H]2O)O)[C@H](O)[C@H]5O)Cn1nnc(c1)COCCCCCCCO[C@H]1O[C@H](CO)[C@H]([C@@H]([C@@H]1O)O)O)[C@H](O)[C@H]4O)CO)[C@H](O)[C@H]3O)CO': 738,\n",
       " 'CC/C(=C/1\\\\c2ccccc2OCc2c1c(OC)ccc2)/c1cccc(c1)NS(=O)(=O)C': 739,\n",
       " 'OC1CCN(CC1)S(=O)(=O)c1cc(ccc1F)C(=O)Nc1cc(F)c(c(c1)F)F': 739,\n",
       " 'O[C@@H]1CC[C@H](CC1)Nc1nccc(n1)n1ncc2c1cccc2OCCCS(=O)(=O)C': 740,\n",
       " 'N#Cc1ccc(cc1)O[C@@H]1CC[C@H](CC1)NC(=O)CCSc1nc2ccccc2c(=O)[nH]1': 740,\n",
       " 'SC[C@@H](C(=O)NCC(=O)O)NC(=O)CC[C@@H](C(=O)O)[NH3+]': 741,\n",
       " 'COc1ccc(c(c1)OC(C)C)C1=N[C@@H]([C@@H](N1)c1ccc(cc1)Cl)c1ccc(cc1)Cl': 741,\n",
       " 'N#Cc1cc(cc(c1)Cl)O[C@@H]1C(=CCN(C1=O)CC1=NN=C2[C@H]1C=CC=N2)C': 742,\n",
       " 'O=C(C[C@@H]1N=C(c2ccc(cc2)Cl)c2c(-n3c1nnc3C)sc(c2C)C)NCCCCCCCCCCNC(=O)C[C@@H]1N=C(c2ccc(cc2)Cl)c2c(-n3c1nnc3C)sc(c2C)C': 742,\n",
       " 'CC(CN1C(=O)N(C)C(=O)C2=NC=N[C@H]12)C': 743,\n",
       " 'COc1cc2c(cc1O)CC[C@@H]1[C@@H]2CC[C@]2([C@H]1CC[C@@H]2OS(=O)(=O)N)C': 743,\n",
       " 'OC[C@@H](C(=O)N[C@@H]([C@H](OP(=O)(O)O)C)C(=O)N)NC(=O)[C@H]1NC(=O)[C@H](CC(C)C)NC(=O)[C@@H]2C[C@H](CN2C(=O)C)OCCCCCCN2C=C(C1)[NH+](CCCCCCCCc1ccccc1)C2': 744,\n",
       " 'CC(C[C@@H]1NC(=O)[C@H](CSSCCNC(=O)CNC(=O)[C@@H](NC(=O)C[C@@H]1O)CC(C)C)NC(=O)[C@H](Cc1ccccc1)NC(=O)[C@@H]1CCC[NH2+]1)C': 744,\n",
       " 'CO/C=C(\\\\c1ccccc1/C=C/c1ccccc1)/C(=O)OC': 745,\n",
       " 'N#Cc1ccc(cc1)CSc1[nH]c2c(n1)c(=O)[nH]c(=N)[nH]2': 745,\n",
       " 'CNC(=O)[C@H]([C@H]1C(=O)/C(=C(/C=C/C(=C/[C@H]([C@H]2O[C@@]3(C)O[C@@H]([C@@H]2C)C=C[C@@]23OC2)C)/C)\\\\O)/C(=O)N1[C@@H]1CC[C@@H]([C@@H](O1)C)O)C': 746,\n",
       " 'O=C1CCCN1CCCNC(=O)c1cnc(nc1NC1CCCC1)NCCc1ccncc1': 746,\n",
       " 'COc1c(OC)cc(cc1OC)[C@@H](C(=O)N1CCCC[C@H]1C(=O)NC1(CCCC1)C(=O)N)C1CCCCC1': 747,\n",
       " 'C[C@H](c1ccc(cc1)C(=O)NS(=O)(=O)C)NC(=O)c1cc(nn1C)c1ccccc1': 747,\n",
       " 'CCc1ccccc1C1=CC(=O)[C@@H](C(=N1)C(=O)O)O': 748,\n",
       " 'CCCCCCCCCCCC(=O)NCC(=O)N[C@H](C(=O)NCC(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)O)CCC(=O)O)C)CO)C)C': 748,\n",
       " 'O=C(Cn1c(nc2c1cccc2)c1nccs1)Nc1ccc(cc1)Br': 749,\n",
       " 'O=C(c1ccccc1)c1ccc(cc1)c1nc2c(n1[C@H](C(=O)NC1CCCCC1)C1CCCCC1)cccc2': 749,\n",
       " 'CCCC(=O)O': 750,\n",
       " 'O=C1Nc2c(C)ccnc2N(c2c1cccn2)C1CC1': 750,\n",
       " 'O=C([C@@H](C[P@@](=O)(c1ccccc1)O)Cc1onc(c1)c1ccccc1)N[C@H](C(=O)N)CC1=CN=C2C1=CCC=C2': 751,\n",
       " 'CN(c1cccc(c1)c1cccc(c1)C(=O)O)C': 751,\n",
       " 'CC(=O)N[C@H](C(=O)N1CCC[C@@]21C=C[C@H]1N(C2=O)[C@@H](CC1)C(=O)N1CC[C@H]2[C@H]1C(=O)N1[C@@H](C=C2)CC[C@H]1C(=O)O)Cc1ccccc1Cl': 752,\n",
       " 'COc1cccc(c1)C1=CCC2=C(N=NC2=C1)NC(=O)c1ccc(cc1)N1CC[N@H+](CC1)CC': 752,\n",
       " 'CC1=C(/C=C/n2cnc3c2ncnc3Nc2ccc(cc2)P(=O)(C)C)[C@H]2C(=NN=C2)C=C1': 753,\n",
       " 'ONC(=O)c1ccc(cc1)CN(C(=O)c1ccc(cc1)N(C)C)CC(=O)NCc1ccccc1': 753,\n",
       " 'ONC(=O)c1ccc(cc1)CN1c2ccccc2Sc2c1nccc2': 754,\n",
       " 'O[C@H]1C[C@@H](O[C@@H]1COP(=O)(O)O)n1cc(C)c(=O)[nH]c1=O': 754,\n",
       " 'CCN(c1ccc2c(c1)oc(=O)cc2COC(=O)N[C@H](C(=O)N[C@@H](Cc1ccccc1)C[C@@H]([C@H](Cc1ccccc1)NC(=O)OCc1cncs1)O)C(C)C)CC': 755,\n",
       " 'O=C(c1ccco1)Nc1cccc(c1)C1=NN=C2C1=C[C@H](C=C2)c1ncn[nH]1': 755,\n",
       " 'CCCC[C@@H](C(=O)C(=O)NN1CCOC1=O)NC(=O)O[C@@H](C(C)(C)C)Cc1nnc(o1)c1ccc(cc1)C(F)(F)F': 756,\n",
       " '[NH3+]CCCC[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)NCC(=O)O)CCC(=O)O)CCC(=O)N)CC(C)C)CC(C)C)CCCNC(=N)N)CC1=NC=NC1)CC(C)C)[C@H](CC)C)NC(=O)[C@H](CC1=NC=NC1)[NH3+]': 756,\n",
       " 'OC[C@@H](C(=O)NCC(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)NCC(=O)O)Cc1ccccc1)[C@H](O)C)Cc1ccccc1)C(C)C)NC(=O)[C@@H]1CCCN1C(=O)[C@H](CO)[NH3+]': 757,\n",
       " 'NC(=O)c1cc2c(s1)c(cnc2N)C(=O)N': 757,\n",
       " 'CCN(c1ccc(cc1)C=O)CC': 758,\n",
       " 'CC(C[C@@H]1NC(=O)[C@H]([C@@H](C)OC(=O)CNC(=O)[C@@H](CC(C)C)N(C)C(=O)[C@H]2N(C(=O)[C@@H](N(C(=O)[C@@H](NC(=O)[C@H]3N(C1=O)C[C@@H](C3)C)CC(C)C)C)C(C)C)CCC2)N(C(=O)[C@@H]1C[C@H](CN1C(=O)[C@@H](N(C(=O)C)C)C(C)C)C)C)C': 758,\n",
       " 'ONC(=O)[C@@H](c1ccc(cc1)Br)NC(=O)C(C)(C)C': 759,\n",
       " 'Clc1ccc(c(c1)C(=O)c1ccn(=O)cc1)NS(=O)(=O)c1ccc(cc1)C(C)(C)C': 759,\n",
       " 'CC(=O)Nc1nc2c(s1)cc(cc2)c1cnc(c(c1)NS(=O)(=O)c1ccc(cc1)F)Cl': 760,\n",
       " 'CCCCC[C@@H](C(=O)N[C@H](C(=O)N1CCC[C@H]1CO)C(C)C)CC(=O)NO': 760,\n",
       " 'O[C@@H]1[C@H](COP(=O)(O)O)[NH2+][C@H]([C@@H]1O)COP(=O)(O)O': 761,\n",
       " 'Nc1ccc(cn1)S(=O)(=O)N1CCN(CC1)c1ccc(cc1)C(C(F)(F)F)(C(F)(F)F)O': 761,\n",
       " 'CC(=O)Nc1ccc(cc1)Oc1ccccc1c1sc2c(n1)ccnc2': 762,\n",
       " 'Nc1nc(c2ccccc2)c(c(=O)[nH]1)Br': 762,\n",
       " 'Cc1nc(=N)[nH]c(=S)[nH]1': 763,\n",
       " 'N=c1[nH]c(=O)c2c([nH]1)n(cn2)[C@@H]1O[C@H]2[C@H]([C@H]1O[P@](=O)(O)OC[C@@H]1[C@@H](O[P@@](=O)(OC2)O)[C@H]([C@@H](O1)n1cnc2c1ncnc2N)O)O': 763,\n",
       " 'CCCC[C@H](NC(=O)[C@@H]1CCCC[C@H]1C(=O)c1ccc(cc1)Cl)CCS(=O)(=O)c1ccccc1': 764,\n",
       " 'CCCCCCCCCCCCCC(=O)O[C@@H]1[C@H](CO[C@@H]([C@H]1O)CO)CC(=O)NO': 764,\n",
       " 'OBc1cc(Br)cc(c1)[N+]1=CCC=C1': 765,\n",
       " 'OC[C@H]1O[C@H]([C@@H]([C@@H]1O)O)n1cnc2c1ncnc2SC': 765,\n",
       " 'Oc1ccc(cc1)c1ccc2c(c1)ccc(c2Cl)O': 766,\n",
       " 'CCO[P@H](=O)C': 766,\n",
       " 'CCOc1nc(ccc1C#N)C(=O)NCc1cc(OC)ccc1OC': 767,\n",
       " '[NH3+]CCCC[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)NCC(=O)N[C@H](C(=O)O)C(C)C)Cc1ccc(cc1)O)CCC(=O)O)NC(=O)[C@@H](NC(=O)[C@H](C(C)C)NC(=O)CNC(=O)c1ccccc1)CC(C)C': 767,\n",
       " 'OC[C@H]1O[C@H](C[C@@H]1OP(=O)(O)O)n1cnc2c1ncnc2N': 768,\n",
       " 'FC(Sc1ccc(cc1)Nc1ncnc2c1cccc2)(F)F': 768,\n",
       " '[NH3+]CC(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)NCC(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N1CCC[C@H]1C(=O)N1CCC[C@H]1C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@@H]([C@H](CC)C)C(=O)O)Cc1ccc(cc1)O)CC(C)C)CC(C)C)CCC(=O)O)CO)C(C)C)CCCNC(=N)N)CCCNC(=N)N)CCC(=O)O)CC(C)C)CC(C)C': 769,\n",
       " 'O[C@@H]([C@H]([C@@H]([C@H](C(=O)O)O)O)O)COP(=O)(O)O': 769,\n",
       " 'Fc1cc(CNC(=O)[C@@]2(O)CCN(C2=O)c2ccccc2)cc(c1)Cl': 770,\n",
       " 'CCOc1ccc2c(c1)sc(n2)S(=O)(=O)N': 770,\n",
       " 'OC[C@H]1O[C@H](C[P@](=O)(O[P@@](=O)(OC/C=C(\\\\CC/C=C(\\\\CC/C=C(\\\\CCC=C(C)C)/C)/C)/C)O)O)[C@@H]([C@H]([C@@H]1O)O)NC(=O)C': 771,\n",
       " 'CNC(=O)c1cccc(c1)c1ccc2c(c1)N(C[C@@H](N2C(=O)C)C)C(=O)c1ccco1': 771,\n",
       " '[NH3+]CCCC[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)NCC(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N1CCC[C@H]1C(=O)N[C@H](C(=O)O)CCCNC(=N)N)[C@H](CC)C)CCC(=O)O)CC1=NC=NC1)[C@H](CC)C)CO)CCCNC(=N)N)C)CCC(=O)O)NC(=O)[C@H](C(C)C)NC(=O)[C@H]([C@H](O)C)NC(=O)[C@@H](NC(=O)[C@H](C(C)C)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)C[NH3+])CO)CC(C)C)C': 772,\n",
       " 'CCN(S(=O)(=O)c1cc(ccc1Br)C(=O)Nc1ccccc1C(=O)O)CC': 772,\n",
       " 'CC[C@@H]([C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)NCC(=O)NCC(=O)N[C@H](C(=O)O)Cc1ccccc1)CC(C)C)CCCC[NH3+])NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H]1CCCN1C(=O)[C@@H](NC(=O)[C@H](C(C)C)NC(=O)[C@H](CCCC[NH3+])[NH3+])C)CC(C)C)CCCC[NH3+])C': 773,\n",
       " 'CCCCCCCCCCCCO[P@@](=O)(OC[C@H]1O[C@H]([C@@H]([C@@H]1O)O)n1cnc2c1ncnc2N)O': 773,\n",
       " 'N=c1[nH]c2cc3=N[C@H](N=c3cc2c(=O)[nH]1)NCC[NH+]1CCOCC1': 774,\n",
       " 'CC(=O)N[C@H](C(=O)N1CCC[C@H]1C(=O)N[C@H]([C@@H](C(C(=O)NCCc1ccccc1)(F)F)O)C(C)C)C': 774,\n",
       " '[NH3+]CCCC[C@@H]1NC(=O)[C@H](Cc2ccc(cc2)O)NC(=O)[C@@H]2CSSC[C@H](NC(=O)[C@@H](NC(=O)[C@H]3N(C(=O)[C@H]4N(C(=O)[C@@H](NC(=O)[C@@H](NC1=O)CO)CCCC[NH3+])CCC4)CCC3)[C@H](CC)C)C(=O)N[C@@H](Cc1ccccc1)C(=O)N1CCC[C@H]1C(=O)N[C@H](C(=O)N1[C@H](C(=O)N[C@H](C(=O)N2)CCCNC(=N)N)O1)CC(=O)O': 775,\n",
       " 'O=C1Nc2c(C1)cc(cc2)Nc1ncncc1Cl': 775,\n",
       " '[NH3+]CC#CCOc1ccc(cc1)S(=O)(=O)N1CCSC([C@@H]1C(=O)NO)(C)C': 776,\n",
       " 'CC(C[C@@H](C(=O)NCCCCNC(=N)N)NC(=O)[C@H](CC(=O)O)O)C': 776,\n",
       " 'CCc1nc2n(c1N(c1sc(c(n1)c1ccc(cc1)F)C#N)C)cc(cc2C)N1CC[NH+](CC1)CC(=O)N1CC(C1)O': 777,\n",
       " 'OC[C@H]1O[C@H]([C@@H]([C@@H]1O[C@H]1O[C@@H](C[NH3+])[C@H]([C@@H]([C@H]1[NH3+])O)O)O)O[C@@H]1[C@@H](O)[C@H]([NH3+])C[C@@H]([C@@H]1O[C@@H]1O[C@H](C[NH3+])[C@H]([C@@H]([C@H]1[NH3+])O)O)[NH3+]': 777,\n",
       " 'COc1ccc2c(c1CN1c3ccccc3OC3([C@@H](C1=O)NC(=O)[C@@H]([NH2+]C)C)CCOCC3)ccc(c2)C(=O)O': 778,\n",
       " 'Clc1ccc(cc1Cl)OCc1nnn[nH]1': 778,\n",
       " 'COc1ccc2c(c1)nc(c(n2)C)O[C@@H]1C[C@H](N(C1)C(=O)[C@H](C(C)(C)C)NC(=O)OC1CCCC1)C(=O)N[C@@]1(C[C@H]1C=C)C(=O)NS(=O)(=O)C1(C)CC1': 779,\n",
       " 'NC(=N)c1ccc(c(c1)C[NH2+]CC[NH2+]Cc1cc(ccc1O)C(=N)N)O': 779,\n",
       " 'CO[C@@H]1[C@@H]2O[P@@](=O)(O)OC[C@H]3O[C@H]([C@@H]([C@@H]3O[P@@](=O)(O)O[NH2+]OCCCCCCO[P@](=O)(OC[C@@H]3[C@@H](O[P@@](=O)(OC[C@@H]4[C@@H](O[P@@](=O)(OC[C@H]2O[C@H]1n1cnc2c1[nH]c(=N)[nH]c2=O)O)C[N@@H+](C4)CC1=CN=C2C1=NC=N[C@H]2N)O)[C@@H](OC)[C@@H](O3)n1cnc2c1[nH]c(=N)[nH]c2=O)O)OC)n1cnc2c1ncnc2N': 780,\n",
       " 'O=C(Nc1ccccc1C(=O)O)NCc1ccccc1': 780,\n",
       " 'C[C@@H](C(=O)N[C@H](C(=O)N1CCC[C@H]1C(=O)N[C@H](B(O)O)C(C)C)C)[NH3+]': 781,\n",
       " 'Fc1ccc(c(c1)F)c1cc(cc2c1CCC(=O)N2c1c(Cl)cccc1Cl)[C@@H]1C=CNC=C1': 781,\n",
       " 'O[C@@H]1[C@@H]2O[P@@](=O)(O)OC[C@H]2O[C@H]1n1cnc2c1ncnc2N': 782,\n",
       " 'CC(=O)N[C@H](C(=O)N[C@H](C(=O)N1CCC[C@H]1C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N)C)[C@H](O)C)CSCCCO[P@](=O)(O[P@@](=O)(OC[C@H]1O[C@H]([C@@H]([C@@H]1O)O)n1ccc(=O)[nH]c1=O)O)O)C(C)C)[C@H](O)C)C(C)C': 782,\n",
       " 'OC[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N1CCC[C@H]1C(=O)N[C@H](C(=O)NCC(=O)O)Cc1ccccc1)CCC(=O)O)COP(=O)(O)O)Cc1ccccc1)NC(=O)[C@H](CCCNC(=N)N)[NH3+]': 783,\n",
       " 'COc1ccc(cc1OC1CCCC1)[C@@H]1CNC(=O)C1': 783,\n",
       " 'CO[P@](=O)(c1ccc(cc1C)C)Nc1nn(cc1C(=O)O)c1ccc(cc1)c1cc2n(n1)c(N)cc(n2)C': 784,\n",
       " 'CCC[NH+](Cc1cc(ccc1N1C(=O)CCC1(CO)CO)C(=O)O)CCC': 784,\n",
       " '[NH3+]CCCC[C@@H](C(=O)O)NC(=O)[C@@H](NC(=O)[C@H]([C@H](O)C)NC(=O)[C@@H](NC(=O)[C@@H]1CCCN1C(=O)[C@@H](NC(=O)[C@H]([C@H](O)C)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@H](Cc1ccccc1)[NH3+])CC(=O)N)Cc1ccc(cc1)O)CCC(=O)O)CO)CC(=O)N)Cc1ccccc1)C': 785,\n",
       " 'Oc1ccc(cc1)n1nnc(c1)c1ccc2c(n1)cccc2': 785,\n",
       " 'O=c1[nH]c(=O)c2c(n1CCCC[N@H+]1CCC(=CC1)c1ccccc1)cccc2F': 786,\n",
       " 'COc1ccc2c(c1)c(CC(=O)O)c(n2C(=O)c1ccc(cc1)Cl)C': 786,\n",
       " 'CCCc1sc(nc1CSc1nc(N)cc(n1)N)c1ccc(c(c1)OCCF)OC': 787,\n",
       " 'O=C(Nc1cccnc1Oc1ccccc1C(C)(C)C)Nc1ccc(cc1)OC(F)(F)F': 787,\n",
       " 'O=C([C@H](NS(=O)(=O)c1ccc2c(c1)cccc2)Cc1ccccc1)NC[C@@H]1CCCN(C1)C(=N)N': 788,\n",
       " '[NH3+]CCC1=CN=C2[C@@H]1C=CC=C2': 788,\n",
       " 'OC[C@@H](c1ccccc1)[NH2+]C[C@H]1[NH2+][C@@H]([C@@H]([C@@H]1O)O)C': 789,\n",
       " 'O=C1N(C[C@H]2c3c1cccc3CCC2)[C@@H]1C[N@@H+]2CC[C@H]1CC2': 789,\n",
       " 'COC(=O)[C@H](C(C)C)NC(=O)[C@H](C(C)C)NC(=O)CC[C@@H]([C@@H](NC(=O)[C@@H](NC(=O)[C@@H]([NH3+])C)C)Cc1ccccc1)O': 790,\n",
       " 'C[NH2+][C@@H]1C[C@H]2O[C@]([C@@H]1OC)(C)n1c3ccccc3c3c1c1n2c2ccccc2c1c1c3CNC1=O': 790,\n",
       " 'Clc1ccc(c(c1)Cl)OCCCCCn1cncc1': 791,\n",
       " 'CCc1ccc(cc1)c1n(C)ncc1c1nn(c2c1c(ncn2)N1CC[C@H](C1)[NH+](C)C)C': 791,\n",
       " 'C[NH+]1CCN(CC1)c1ccc(cc1)Nc1nccc(n1)c1cc2c([nH]1)CCNC2=O': 792,\n",
       " 'CCc1cccc(c1NC(=O)n1cc2c(c1)[C@@H]([NH2+]N2)NC(=O)c1ccc(cc1)N1CC[N@H+](CC1)C)CC': 792,\n",
       " 'COc1ccc2c(c1C[NH+]1CC[NH2+]CC1)O/C(=C\\\\C1=NN=C3[C@H]1C=CC=C3)/C2=O': 793,\n",
       " 'CC(C[C@@H](C(=O)N[C@H](C(=O)O)Cc1ccccc1)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)C)CCC(=O)N)CC(C)C)CC(=O)O)C': 793,\n",
       " '[NH3+]CCCC[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)O)CCC(=O)O)CCSC)CCC(=O)N)CCC(=O)O)Cc1ccc(cc1)O)CO)CCCNC(=N)N)NC(=O)[C@H]([C@H](O)C)NC(=O)CNC(=O)[C@@H](NC(=O)[C@H](CO)[NH3+])CCC(=O)N': 794,\n",
       " 'O[C@@H]1[C@@H](CO[P@@](=O)(OP(=O)(O)O)O)O[C@H]([C@@H]1O)n1cnc2c1nc(N)[nH]c2=O': 794,\n",
       " 'C[N@@H+]1CC[C@H](CC1)NC(=O)c1c(C)n[nH]c1C1=N[C@H]2C(=N1)C=CC(=C2)C(C)C': 795,\n",
       " 'OC(=O)CCNc1cc(nc(n1)c1ccccn1)N1CCc2c(CC1)cccc2': 795,\n",
       " 'O=C(Nc1ccc(cc1Cl)S(=O)(=O)N)CSC(=O)N1CCCc2c1ccc(c2)C': 796,\n",
       " 'OCCn1ncc(c1)CC[C@H]1N=Nc2c1cc(cc2)O[C@@H](c1c(Cl)cncc1Cl)C': 796,\n",
       " 'OC[C@@H](C(=O)NCC(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)O)CC1=NC=NC1)CC1=NC=NC1)C(C)C)CCC(=O)O)Cc1ccc(cc1)O)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H]1CCC(=O)N1)Cc1ccccc1)CCCNC(=N)N)CC1=NC=NC1)CC(=O)O': 797,\n",
       " 'CC1=C[C@]2(O[C@@H](C1)[C@@H](/C=C/[C@H]1CC[C@]3(O1)CC[C@@H]1[C@@H](O3)[C@H](O)C(=C)[C@H](O1)[C@H](C[C@@H]([C@H]1O[C@@]3(CCCCO3)CC[C@H]1C)C)O)C)O[C@@H](CC[C@H]2O)C[C@](C(=O)O)(O)C': 797,\n",
       " 'OC[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N)C)CO)[C@H](O)C)[C@H](O)C)NC(=O)[C@H]([C@H](O)C)NC(=O)[C@H](C(C)C)NC(=O)[C@@H](NC(=O)[C@@H]([NH3+])C)C': 798,\n",
       " '[NH3+]CCCC[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)O)CCCNC(=N)N)[C@H](O)C)CC(C)C)CCCNC(=N)N)[C@H](CC)C)NC(=O)[C@@H](NC(=O)[C@@H]1CCCN1C(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@H](C(C)C)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@H]([C@H](O)C)[NH3+])CC1=CN=C2[C@H]1C=CC=C2)CCCNC(=N)N)CCC(=O)N)CCCNC(=N)N)CO)CCC(=O)N)CC(=O)N)CC(C)C': 798,\n",
       " 'Cn1cnc2c1c(=O)n(C)c(=O)n2C': 799,\n",
       " 'Fc1cccc(c1NC(=S)n1nc(nc1N)Nc1ccc(cc1)S(=O)(=O)N)F': 799,\n",
       " 'C[N@@H+]1CC[C@H](CC1)c1cnn(c1)c1nccc2c1nc[nH]c2=O': 800,\n",
       " 'COc1ccccc1[C@@H]1C=Nc2c1cc(cn2)c1cncc(c1)C(=O)N(C)C': 800,\n",
       " 'COC(=O)N[C@@H](C(C)(C)C)C(=O)N[N@@H+](Cc1ccc(cc1)[C@@H]1SCC=C1)CCC[C@@]1(O)Cc2ccc(cc2)C/C=C\\\\CNC(=O)[C@@H](NC1=O)C(C)C': 801,\n",
       " 'Cc1cnccc1C1=C2C=NN=C2C=C(C1)C(F)(F)F': 801,\n",
       " 'CSCC[C@@H](C(=O)O)NC(=O)[C@H]([C@H](CC)C)NC(=O)[C@@H]([NH3+])C': 802,\n",
       " 'O[C@H]1CC[C@]2([C@@H](C1)CC[C@@H]1[C@@H]2C[C@@H](O)[C@]2([C@]1(O)CC[C@@H]2C1=CC(=O)OC1)C)C': 802,\n",
       " 'CC[C@@H]([C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)O)CC(=O)O)CCC(=O)O)[C@H](CC)C)NC(=O)[C@@H]1CCCN1C(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@H](CC)[NH3+])CC(C)C)COP(=O)(O)O)C': 803,\n",
       " 'COc1cccc(c1)C(=O)N[C@H]1C[C@@]([C@H]2[C@@H]1[C@@H]2C(=O)O)([NH3+])C(=O)O': 803,\n",
       " 'CCCS(=O)(=O)Nc1ccc(c(c1F)C(=O)NC1=CC2=C(OC)N=NC2=NC1)F': 804,\n",
       " 'CCCN(C(=O)c1cccc(c1)C(=O)N[C@H]([C@@H](C[NH2+]Cc1cccc(c1)OC)O)Cc1ccccc1)CCC': 804,\n",
       " '[NH3+]CCCC[C@@H](C(=O)NCCC1CCCCC1)NC(=O)[C@@H](NC(=O)CCCCCCCCC[NH3+])CO': 805,\n",
       " 'Nc1[nH]c(=O)c2c(n1)n(C)cn2': 805,\n",
       " '[NH3+]CCCC[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)O)CO)CCCC[NH+](C)C)CCCNC(=N)N)C)[C@H](O)C)CCC(=O)N)NC(=O)[C@H]([C@H](O)C)[NH3+]': 806,\n",
       " 'CCn1cnc2c1nc(nc2Nc1ccc(cc1)[P@@](=O)(CP(=O)(O)O)O)[C@@H]1CC[C@H](CC1)[NH3+]': 806,\n",
       " 'OCC[C@@H](c1ccc(cc1)Cl)NC(=O)[C@]1([NH3+])CCN(CC1)C1=NC=N[C@@H]2C1=CC=N2': 807,\n",
       " 'CC(C[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N1CCC[C@H]1C(=O)N[C@H](C(=O)N[C@H](C(=O)O)C)CC(C)C)CC(C)C)CCC(=O)N)NC(=O)[C@H]([C@H](O)C)NC(=O)[C@@H]1CCCN1C(=O)[C@@H](NC(=O)[C@H]1[NH2+]CCC1)CCCNC(=N)N)C': 807,\n",
       " 'CC(CCn1c(C[N@@H+]2CC[C@@H](CC2)C(=O)N)nc2c1c(=O)n(c(=O)n2C)C)C': 808,\n",
       " 'OC[C@@H](C(=O)O)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@H]([C@H](O)C)NC(=O)[C@@H](NC(=O)[C@H](CC(C)C)[NH3+])C)CCCC[NH3+])C)C)CCCNC(=N)N)CCCC[N+](C)(C)C': 808,\n",
       " 'C[NH+](CCCN1c2ccccc2Sc2c1cc(Cl)cc2)C': 809,\n",
       " 'COc1cc(ccc1C(=O)Nc1ccc2c(c1)CC[NH2+]CC2)C1=CN=NC1': 809,\n",
       " 'COc1ccc(cc1)C1(SC[C@@H](C(=O)O)[NH3+])c2ccccc2CCc2c1cccc2': 810,\n",
       " 'O=C(NCCc1ccc(cc1)CC(C(=O)O)C(=O)O)NCCC(=O)Nc1ccc2c(c1)sc(n2)C': 810,\n",
       " 'OC[C@H]1O[C@@H](NC(=O)NC(=O)c2ccccc2)[C@@H]([C@H]([C@@H]1O)O)O': 811,\n",
       " 'CO[C@@H]1O[C@@H](C)[C@H]([C@H]([C@@H]1O)O)O': 811,\n",
       " 'Oc1ccc(cc1)CCc1cc(O)cc(c1)O': 812,\n",
       " 'OC[C@@H](C(=O)N[C@H](C(=O)O)C(C)C)NC(=O)[C@H]([C@H](O)C)NC(=O)[C@H](C(C)C)NC(=O)[C@@H](NC(=O)[C@H](Cc1ccc(cc1)O)[NH3+])CC(C)C': 812,\n",
       " '[NH3+]Cc1ccc(cc1)c1ncc(nc1c1ccc(cc1)C1=CN=NC1)OCC1CC[NH2+]CC1': 813,\n",
       " 'OC(=O)COc1cc(C)c(c(c1)C)Cc1ccc(c(c1)C(C)C)O': 813,\n",
       " '[NH3+]CCCC[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N1CCC[C@H]1C(=O)N[C@H](C(=O)N[C@H](C(=O)O)CCC(=O)N)C)CCCC[NH3+])CCCC[NH3+])C)CCCC[NH3+])CCCNC(=N)N)CO)CO)NC(=O)[C@H]([C@H](O)C)NC(=O)[C@@H]1CCCN1C(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@H]([C@H](CC)C)NC(=O)[C@H]([C@H](CC)C)NC(=O)[C@H](CS)[NH3+])CO)C)CCSC': 814,\n",
       " 'OC[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)O)CC(=O)O)CC1=CN=C2[C@H]1C=CC=C2)CCCNC(=N)N)[NH3+]': 814,\n",
       " '[NH3+]CCCC[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)O)CCCNC(=N)N)CC(C)C)C(C)C)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)CNC(=O)CNC(=O)[C@@H](NC(=O)CNC(=O)[C@@H](NC(=O)C[NH3+])CC(C)C)CCCC[NH3+])C)CCCCNC(=O)C)CCCNC(=N)N)CC1=NC=NC1)CCCNC(=N)N': 815,\n",
       " 'ONC(=O)/C=C/c1ccccc1OCc1c(Cl)cccc1Cl': 815,\n",
       " 'OC[C@@H](C(=O)N[C@H](C(=O)N1CCC[C@H]1C(=O)N[C@H](C(=O)N[C@H](C(=O)O)CO)Cc1ccc(cc1)O)C(C)C)NC(=O)[C@@H](NC(=O)[C@@H]1CCCN1C(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@H](CC1=CN=C2[C@H]1C=CC=C2)[NH3+])CC(C)C)C)Cc1ccc(cc1)O)CC(=O)O': 816,\n",
       " 'COc1ccc2c(c1)ccnc2O[C@@H]1C[C@H](N(C1)C(=O)[C@H](C(C)(C)C)NC(=O)OC(C)(C)C)C(=O)N[C@@]1(C[C@H]1C=C)C(=O)NS(=O)(=O)C1CC1': 816,\n",
       " 'N=C1SCC[C@@](N1)(C)c1ccc(cc1F)F': 817,\n",
       " 'CCC(CN(S(=O)(=O)[C@@H]1CC[C@@H](CC1)[NH3+])C[C@H]([C@@H](NC(=O)O[C@H]1CO[C@@H]2[C@H]1CCO2)CC1CCCCC1)O)CC': 817,\n",
       " 'CC([C@H]1[C@@H](SC2=N[C@@]([C@H](N12)c1ccc(cc1)Cl)(C)c1ccc(cc1)Cl)C(=O)N1CCC[C@H]1C(=O)N(C)C)C': 818,\n",
       " 'O=C(N[C@@H](C(=O)O)CC1=NC2=CC=CCC2=N1)CCc1ccccc1': 818,\n",
       " 'CCCCCCCCCCC1=C(O)C(=O)NC1=O': 819,\n",
       " 'Oc1ccc(cc1)CC1=NC(=C2[C@@H]1Oc1ccccc1C2=O)c1cccs1': 819,\n",
       " 'CC[NH+](CCNS(=O)(=O)Cc1ccc(cc1)F)CC': 820,\n",
       " '[NH3+]CCCC[C@@H](C(=O)N[C@H](C(=O)N1CCC[C@H]1C(=O)N[C@H](C(=O)O)C)[C@H](CC)C)NC(=O)[C@@H](NC(=O)CNC(=O)[C@@H]1CCCN1C(=O)[C@H](CC(=O)N)[NH3+])CC(C)C': 820,\n",
       " 'NC(=O)CC[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)O)C)[C@H](O)C)NC(=O)[C@@H](NC(=O)[C@H]([C@H](O)C)NC(=O)[C@@H](NC(=O)[C@@H]([NH3+])C)CCCNC(=N)N(C)C)CCCC[NH+](C)C': 821,\n",
       " 'OS(=O)(=O)OC[C@H]1O[C@H](O[C@@H]2[C@@H](O[C@H]([C@@H]([C@H]2O)OS(=O)(=O)O)O[C@@H]2[C@@H](COS(=O)(=O)O)O[C@@H]([C@@H]([C@H]2O)NS(=O)(=O)O)O[C@@H]2[C@@H](O[C@H]([C@@H]([C@H]2O)OS(=O)(=O)O)O)C(=O)O)C(=O)O)[C@@H]([C@H]([C@@H]1O[C@@H]1O[C@@H](C(=O)O)[C@H]([C@@H]([C@H]1OS(=O)(=O)O)O)O[C@H]1O[C@H](COS(=O)(=O)O)[C@H]([C@@H]([C@H]1NS(=O)(=O)O)O)O)O)NS(=O)(=O)O': 821,\n",
       " 'Clc1cc(cc(c1OCCCCCCCCCCCOc1c(Cl)cc(cc1Cl)Nc1ccccc1C(=O)O)Cl)Nc1ccccc1C(=O)O': 822,\n",
       " 'OC[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)OCc1ccccc1)CC(C)C)CC(C)C)CC(C)C': 822,\n",
       " 'OC(=O)c1cncc(c1)C#Cc1ccccc1': 823,\n",
       " 'OC(=O)Cc1c(=O)[nH]c2c(c1c1ccccc1)cc(cc2)Cl': 823,\n",
       " 'CC[C@@H]([C@@H](C(=O)N1CCC[C@H]1C(=O)N[C@H](C(=O)O)Cc1ccccc1)NC(=O)[C@H](CC(=O)O)[NH3+])C': 824,\n",
       " 'CCNC(=O)Nc1nc(c(s1)C1=NC=CC1)/C=C/c1cccnc1': 824,\n",
       " 'CCCC[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N)CCCNC(=N)N)CCC(=O)N)[NH2+]C[C@@H](NC(=O)[C@H]([C@H](CC)C)NC(=O)[C@H]([C@H](O)C)NC(=O)C)CCCC': 825,\n",
       " 'N=C1SC[C@H]2[C@@](N1)(COC2)c1cc(ccc1F)NC(=O)c1ccc(cn1)F': 825,\n",
       " 'OC[C@@H](C(=O)N[C@H](C(=O)O)CC(C)C)NC(=O)[C@H]([C@H](O)C)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H]([NH3+])C)C)CC(=O)N)CCC(=O)O': 826,\n",
       " 'CCS[C@@H]1O[C@H](CO)[C@@H]([C@@H]([C@H]1O)O)O': 826,\n",
       " 'COc1ccc(cc1C[C@@H]1CN/C(=N\\\\N(C)C)/CN(C1=O)CC(=O)Nc1cccc(c1)C(=O)O)Cl': 827,\n",
       " 'OC(=O)Cn1c(cc2c1c1ccccc1cc2)C(=O)NCP(=O)(O)O': 827,\n",
       " 'Cc1cc(ccc1Nc1nc2-c(c([nH]1)NC1CCCCC1)ncn2)N1CCOCC1': 828,\n",
       " 'OC(=O)c1ccnc2c1cccc2O': 828,\n",
       " 'OC[C@@H](C(=O)N1CCC[C@H]1C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)NCC(=O)O)CO)CCCNC(=N)N)CCCNC(=N)N)CO)NC(=O)[C@@H](NC(=O)[C@@H]1CCCN1C(=O)[C@@H]1CCCN1C(=O)[C@H]([C@H](O)C)NC(=O)[C@@H]1CCCN1C(=O)[C@@H](NC(=O)[C@H](CC(C)C)[NH3+])CC(C)C)CC(C)C': 829,\n",
       " 'O=C(C[C@H](C(=O)O)O)CCC1=C[C@H](C=N1)C(=O)c1cccc(c1)F': 829,\n",
       " 'Cc1ncn(c1)c1cc(NC(=O)c2ccc(c(c2)/C=C/n2cnc3c2ncnc3NC2CC2)C)cc(c1)C(F)(F)F': 830,\n",
       " 'COc1cc2c(ccnc2cc1OC)Oc1ccc(cc1F)NC(=O)[C@H]1C(=[N+](N(C1=O)c1ccccc1)C)C': 830,\n",
       " 'NC(=O)c1ccc(cc1)Nc1ncc2c(n1)N(C1CCCC1)[C@@H]1CCC[C@@H]1C(=O)N2C': 831,\n",
       " 'O=C1Cc2c(N1)ccc(c2)Nc1ncc(c(n1)NCc1ccccc1S(=O)(=O)N1CCCC1)C(F)(F)F': 831,\n",
       " 'Clc1ccc(c(c1)Cl)[C@@H]1NC(=O)c2c(N1C)nccc2': 832,\n",
       " 'OCC#Cc1nn(c2c1c(N)ncn2)Cc1nc2cccc(c2c(=O)n1c1ccccc1C)C': 832,\n",
       " 'O[C@H]([C@H](C(=O)O)O)C(=O)O': 833,\n",
       " 'OC(=O)[C@@H]1CCC[NH2+]1': 833,\n",
       " 'OC[C@H]([C@H]([C@@H]1O[C@](OCCn2nnc(c2)C[N@H+](Cc2nnn(c2)CCO[C@@]2(C[C@H](O)[C@H]([C@@H](O2)[C@@H]([C@@H](CO)O)O)NC(=O)CC)C(=O)O)Cc2nnn(c2)CCO[C@@]2(C[C@H](O)[C@H]([C@@H](O2)[C@@H]([C@@H](CO)O)O)NC(=O)CC)C(=O)O)(C[C@@H]([C@H]1NC(=O)CC)O)C(=O)O)O)O': 834,\n",
       " 'CNC(=O)c1cc(OCC(=O)Nc2ccccc2)ccc1OC1CCOCC1': 834,\n",
       " 'C[NH+](CCCC(=O)Nc1ccc2c(c1)c(ncn2)Nc1cccc(c1)Br)C': 835,\n",
       " 'Nc1ncc2c(n1)ccc(c2)c1c(C)ccn(c1=O)c1cccc(c1)C(F)(F)F': 835,\n",
       " 'O=C(Nc1cc(nn1c1ccc2c(c1)C[C@H]([NH2+]C2)C(=O)O)C(C)(C)C)Nc1cccc(c1Cl)Cl': 836,\n",
       " 'OC[C@H]1O[C@@H](NC(=O)[C@@H](Cc2ccc(cc2)C(C)C)C)[C@@H]([C@H]([C@@H]1O)O)O': 836,\n",
       " 'OC[C@H]1O[C@H]([C@@H]([C@H]([C@@H]1O)O)O)n1cc(Cl)c(=O)[nH]c1=O': 837,\n",
       " 'CN(c1ccc(cc1)C(c1ccc(cc1)N(C)C)c1ccccc1)C': 837,\n",
       " 'O=C(Nc1ccccc1Cl)NCc1cccnc1': 838,\n",
       " 'Fc1cc(cc(c1)N1CCOCC1)C(=O)Nc1ccc2c(c1)n(CCc1ccncc1)cc2': 838,\n",
       " 'OC[C@H](C[C@@H]1CCNC1=O)NC(=O)[C@@H](NC(=O)OCCc1ccccc1)CC(C)C': 839,\n",
       " 'Fc1ccc(cc1)C(=O)Nc1ccc2c(c1)C[NH2+]CC2': 839,\n",
       " 'CCN(C(=O)c1c(NC(=O)c2cccs2)sc2c1CC(C)(C)CC2)CC': 840,\n",
       " 'O=CN1CC(=N)CC[C@H]1C(=O)NNC(=O)[C@H]1C[NH2+]CC1': 840,\n",
       " 'O=C1NCCc2c1cc([nH]2)c1ccnc(c1)c1cnc2c(c1)cccc2': 841,\n",
       " 'CCCCC#CC1=CC=CN(C1)CC(P(=O)(O)O)P(=O)(O)O': 841,\n",
       " 'O=C(N1CCC[C@H]1C(=O)NCc1ccc(cc1)C(=N)N)[C@H](NS(=O)(=O)Cc1ccccc1)Cc1ccccc1': 842,\n",
       " 'OC[C@@H]1CCCCN1C(=O)[C@@H]1CCCN1C(=O)OCc1ccccc1': 842,\n",
       " 'O=[N+]([C@@H](C#Cc1cccc(c1)O[C@]12C[C@H]3C[C@@H](C2)C[C@@H](C1)C3)C)C(=O)N': 843,\n",
       " 'CNc1ccnc(n1)Nc1ccc2c(c1)n(c(c2)C)c1cc(cnc1Cl)c1c(C)ccn(c1=O)C': 843,\n",
       " '[NH3+][C@@H]1CCCC[C@H]1Nc1ccn2c(n1)c(cn2)c1ccc2c(c1)cccc2': 844,\n",
       " 'CSCC[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)O)C)[C@H](CC)C)C(C)C)NC(=O)[C@@H]1CCCN1C(=O)[C@H](C(C)C)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)C[NH3+])Cc1ccccc1)CO)CC(=O)O)CC(=O)O': 844,\n",
       " 'C[C@@H]([C@H]1CC[C@@H]2[C@]1(C)CCC/C/2=C\\\\C=C\\\\1/C[C@@H](O)C(=C)[C@@H](C1)O)CC#C[C@H]([C@]12C[C@H]3C[C@@H](C2)C[C@@H](C1)C3)O': 845,\n",
       " 'N#Cc1ccc(o1)C(=O)Nc1ccc(cc1N1CCCCC1)N1CC[NH+](CC1)C': 845,\n",
       " 'N[C@@H]1N=NC(=C1/N=N/c1ccccc1C(F)(F)F)N': 846,\n",
       " 'Brc1nc(Cl)c2c(n1)nc[nH]2': 846,\n",
       " 'CCCCC[C@@H](C(=O)N[C@H](C(=O)N1CCC[C@H]1C(=O)O)C(C)C)CC(=O)NO': 847,\n",
       " 'Clc1ccc(cc1)C1(CC[NH2+]CC1)c1ccc(cc1)C1=CN=NC1': 847,\n",
       " 'ONC(=O)Cc1ccccc1S(=O)(=O)c1ccc(cc1)c1ccc(cc1)OC': 848,\n",
       " 'Clc1ccc2-c3nc(ncc3CN=C(c2c1)c1c(F)cccc1F)Nc1ccc(cc1)C(=O)O': 848,\n",
       " 'COc1cccc(c1)CC(=O)Nc1scc(n1)c1ccnc(c1)N': 849,\n",
       " 'Oc1ccc2c(c1)OCO2': 849,\n",
       " 'CCC(Oc1cc(cc(c1NC(=O)C)N)C(=O)O)CC': 850,\n",
       " 'CN(c1nccc(n1)c1cnc2n1cccc2)C1CCCCC1': 850,\n",
       " 'COc1ccc(cc1Br)Cc1nc(O)c(c(n1)C(=O)O)O': 851,\n",
       " 'Fc1cc(cc(c1)F)[C@@H]1CC[NH2+]N1C(=O)C1CCN(CC1)c1nnc(o1)C': 851,\n",
       " 'OC[C@H]1O[C@@H](O[C@H]2[C@H](O)C[NH2+]C[C@@H]2CO)[C@@H]([C@H]([C@@H]1O[C@@H]1O[C@H](CO)[C@H]([C@@H]([C@H]1O)O)O[C@@H]1O[C@H](CO)[C@H]([C@@H]([C@H]1O)O)O)O)O': 852,\n",
       " 'CC(COc1ccc(c(c1)c1nnc2n1c1cc(ccc1nc2C)C(=O)N[C@]12C[C@H]3C[C@@H](C2)C[C@@H](C1)C3)Cl)C': 852,\n",
       " '[NH3+][C@@H]1OC[C@]2([NH2+]1)c1cc(ccc1Oc1c2cc(nc1F)[C@H]1COCC=C1)c1cccnc1F': 853,\n",
       " 'C[C@H]1C[NH2+]CCCN1S(=O)(=O)c1cccc2c1c(C)cnc2': 853,\n",
       " 'Nc1ccccc1SCCCCP(=O)(O)O': 854,\n",
       " 'C[NH+](CC(=O)N[C@@H]1CC[C@@H]2[C@H](C1)[C@H]1CCCC[C@@H]1[C@H]([NH2+]2)O)C': 854,\n",
       " '[NH3+]CCCC[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)O)Cc1ccc(cc1)O)C)CCCNC(=N)N)CC(=O)O)CCCC[NH+](C)C)CO)NC(=O)[C@@H](NC(=O)C)CO': 855,\n",
       " 'OC[C@@H](C(=O)N[C@H](C(=O)N1CCC[C@H]1C(=O)NCC(=O)N[C@H](C(=O)N[C@H](C(=O)N1CCC[C@H]1C(=O)N1C[C@H](C[C@H]1C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)O)CC(=O)N)CC1=NC=NC1)CCCNC(=N)N)O)CC1=NC=NC1)CC1=NC=NC1)CC(=O)N)NC(=O)[C@@H](NC(=O)[C@H](CC(=O)O)[NH3+])Cc1ccc(cc1)OP(=O)(O)O': 855,\n",
       " 'CCCc1cc2c(c(c1O[C@@H](c1ccc(cc1)C(C)C)C(=O)O)CCC)on(c2=O)C': 856,\n",
       " '[NH3+]CCCC[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N)CC(=O)N)CS)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)CNC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@H](CC1=CN=C2[C@@H]1C=CC=C2)NC(=O)[C@@H](NC(=O)[C@H](CCC(=O)N)[NH3+])CS)CC(=O)O)CCCNC(=N)N)CS)CCC(=O)O)CC(=O)N)CCCNC(=N)N': 856,\n",
       " '[NH3+]CCCC[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)O)C)CO)CCCC[N+](C)(C)C)CCCNC(=N)N)C)C)NC(=O)[C@H]([C@H](O)C)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H]([NH3+])C)CC(C)C)C': 857,\n",
       " ...}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parquet_file = 'data/binding_data_200k.parquet'\n",
    "output_dir = \"processed_data/ligands_multiscale_etkdg\"\n",
    "\n",
    "generate_multiscale_ligand_representations(\n",
    "    parquet_file=parquet_file,\n",
    "    output_dir=output_dir,\n",
    "    batch_size=5,\n",
    "    n_jobs=10,\n",
    "    chunkify=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c43e76-0305-4018-ba21-f3aee3738a53",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Combined Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8f020de-8d76-4106-b539-966ae8cb644a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multiscale_combined_dataset(parquet_file, protein_embedding_file, protein_map_file, \n",
    "                                       ligand_data_file, output_file, use_full_sequence=True,\n",
    "                                       max_protein_length=1000):\n",
    "    \"\"\"\n",
    "    Create a combined dataset linking proteins and multi-scale ligand representations\n",
    "    with binding affinities for the graph-based diffusion model\n",
    "    \n",
    "    Args:\n",
    "        parquet_file: Path to original data with protein-ligand pairs and affinity values\n",
    "        protein_embedding_file: Path to protein embeddings .npz file\n",
    "        protein_map_file: Path to protein sequence to embedding index mapping\n",
    "        ligand_data_file: Path to multi-scale ligand representations file\n",
    "        output_file: Path to save the combined dataset\n",
    "        use_full_sequence: Whether to use full sequence embeddings (True) or CLS token embeddings (False)\n",
    "        max_protein_length: Maximum protein sequence length to include (for memory efficiency)\n",
    "    \"\"\"\n",
    "    print(\"Loading data sources...\")\n",
    "    \n",
    "    # Load original data with protein-ligand pairs\n",
    "    df = pd.read_parquet(parquet_file)\n",
    "    print(f\"Loaded {len(df)} protein-ligand pairs from original dataset\")\n",
    "    \n",
    "    # Load protein embeddings\n",
    "    print(f\"Loading protein embeddings from {protein_embedding_file}...\")\n",
    "    if use_full_sequence:\n",
    "        protein_data = np.load(protein_embedding_file, allow_pickle=True)\n",
    "        protein_embeddings = protein_data['embeddings'].item()  # Dictionary for full sequence\n",
    "        print(f\"Loaded full sequence embeddings for {len(protein_embeddings)} proteins\")\n",
    "    else:\n",
    "        protein_data = np.load(protein_embedding_file)\n",
    "        protein_embeddings = protein_data['embeddings']  # Array for CLS token\n",
    "        print(f\"Loaded CLS token embeddings with shape {protein_embeddings.shape}\")\n",
    "    \n",
    "    # Load protein mapping\n",
    "    print(f\"Loading protein mapping from {protein_map_file}...\")\n",
    "    with open(protein_map_file, 'rb') as f:\n",
    "        protein_to_idx = pickle.load(f)\n",
    "    print(f\"Loaded mapping for {len(protein_to_idx)} proteins\")\n",
    "    \n",
    "    # Load ligand multi-scale representations\n",
    "    print(f\"Loading ligand representations from {ligand_data_file}...\")\n",
    "    with open(ligand_data_file, 'rb') as f:\n",
    "        ligand_data = pickle.load(f)\n",
    "    print(f\"Loaded multi-scale representations for {len(ligand_data)} ligands\")\n",
    "    \n",
    "    # Create combined dataset\n",
    "    print(\"Creating combined multi-scale dataset...\")\n",
    "    combined_data = []\n",
    "    \n",
    "    # Counters for stats\n",
    "    missing_protein = 0\n",
    "    missing_ligand = 0\n",
    "    filtered_ligand = 0\n",
    "    successful_pairs = 0\n",
    "    truncated_proteins = 0\n",
    "    \n",
    "    # Process each protein-ligand pair\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        protein_seq = row['seq']\n",
    "        ligand_smiles = row['smiles']\n",
    "        \n",
    "        # Get affinity values - use normalized affinity if available\n",
    "        if 'neg_log10_affinity_M' in row:\n",
    "            affinity = row['neg_log10_affinity_M']\n",
    "        elif 'affinity' in row:\n",
    "            affinity = row['affinity']\n",
    "        else:\n",
    "            affinity = 0.0\n",
    "        \n",
    "        # Skip if protein not in our processed data\n",
    "        if protein_seq not in protein_to_idx:\n",
    "            missing_protein += 1\n",
    "            continue\n",
    "        \n",
    "        # Skip if ligand not in our processed data\n",
    "        if ligand_smiles not in ligand_data:\n",
    "            # Check if it was filtered due to size or just missing\n",
    "            mol = Chem.MolFromSmiles(ligand_smiles) if 'Chem' in globals() else None\n",
    "            if mol and mol.GetNumAtoms() > 150:\n",
    "                filtered_ligand += 1\n",
    "            else:\n",
    "                missing_ligand += 1\n",
    "            continue\n",
    "        \n",
    "        # Get protein embedding\n",
    "        if use_full_sequence:\n",
    "            # For full sequence embeddings\n",
    "            protein_idx = protein_to_idx[protein_seq]\n",
    "            protein_embedding = protein_embeddings[protein_idx]\n",
    "            \n",
    "            # Truncate if too long (for memory efficiency)\n",
    "            if len(protein_embedding) > max_protein_length:\n",
    "                protein_embedding = protein_embedding[:max_protein_length]\n",
    "                truncated_proteins += 1\n",
    "        else:\n",
    "            # For CLS token embeddings\n",
    "            protein_idx = protein_to_idx[protein_seq]\n",
    "            protein_embedding = protein_embeddings[protein_idx]\n",
    "        \n",
    "        # Get ligand multi-scale representation\n",
    "        ligand_repr = ligand_data[ligand_smiles]\n",
    "        \n",
    "        # Create entry linking everything\n",
    "        entry = {\n",
    "            # Protein data\n",
    "            'protein_embedding': protein_embedding,\n",
    "            'protein_seq': protein_seq,\n",
    "            'protein_length': len(protein_embedding) if use_full_sequence else protein_embeddings.shape[1],\n",
    "            \n",
    "            # Binding data\n",
    "            'affinity': affinity,\n",
    "            \n",
    "            # Ligand data\n",
    "            'ligand_smiles': ligand_smiles,\n",
    "            'multi_scale': ligand_repr['multi_scale'],\n",
    "            'properties': ligand_repr['properties'],\n",
    "            'functional_groups': ligand_repr['functional_groups'],\n",
    "            'physics_features': ligand_repr['physics_features'],\n",
    "        }\n",
    "        \n",
    "        combined_data.append(entry)\n",
    "        successful_pairs += 1\n",
    "    \n",
    "    print(f\"Created combined multi-scale dataset with {len(combined_data)} entries\")\n",
    "    print(f\"Stats: {successful_pairs} successful pairs\")\n",
    "    print(f\"       {missing_protein} missing proteins\")\n",
    "    print(f\"       {missing_ligand} missing ligands\")\n",
    "    print(f\"       {filtered_ligand} filtered ligands (>150 atoms)\")\n",
    "    if use_full_sequence:\n",
    "        print(f\"       {truncated_proteins} truncated proteins (>{max_protein_length} residues)\")\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    \n",
    "    # Save the combined dataset\n",
    "    print(f\"Saving combined dataset to {output_file}...\")\n",
    "    with open(output_file, 'wb') as f:\n",
    "        pickle.dump(combined_data, f)\n",
    "    \n",
    "    print(f\"Saved combined multi-scale dataset with {len(combined_data)} entries\")\n",
    "    \n",
    "    # Calculate some statistics about the dataset\n",
    "    if len(combined_data) > 0:\n",
    "        print(\"\\nDataset Statistics:\")\n",
    "        \n",
    "        # Affinity distribution\n",
    "        affinities = [entry['affinity'] for entry in combined_data]\n",
    "        print(f\"Affinity range: {min(affinities):.2f} to {max(affinities):.2f}\")\n",
    "        print(f\"Mean affinity: {np.mean(affinities):.2f}\")\n",
    "        \n",
    "        # Protein sequence lengths\n",
    "        if use_full_sequence:\n",
    "            protein_lengths = [len(entry['protein_embedding']) for entry in combined_data]\n",
    "            print(f\"Protein length range: {min(protein_lengths)} to {max(protein_lengths)} residues\")\n",
    "            print(f\"Mean protein length: {np.mean(protein_lengths):.2f} residues\")\n",
    "        \n",
    "        # Ligand statistics\n",
    "        mol_weights = [entry['properties']['molecular_weight'] for entry in combined_data]\n",
    "        print(f\"Molecular weight range: {min(mol_weights):.2f} to {max(mol_weights):.2f}\")\n",
    "        print(f\"Mean molecular weight: {np.mean(mol_weights):.2f}\")\n",
    "        \n",
    "        # Graph statistics\n",
    "        atom_counts = [len(entry['multi_scale']['level3_complete']['graph']['atoms']) for entry in combined_data]\n",
    "        print(f\"Atom count range: {min(atom_counts)} to {max(atom_counts)} atoms\")\n",
    "        print(f\"Mean atom count: {np.mean(atom_counts):.2f} atoms\")\n",
    "    \n",
    "    return combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9df0327-88f5-4574-bae4-0317b6cd6b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data sources...\n",
      "Loaded 20000 protein-ligand pairs from original dataset\n",
      "Loading protein embeddings from processed_data/protein_full_embeddings_all.npz...\n",
      "Loaded full sequence embeddings for 5000 proteins\n",
      "Loading protein mapping from processed_data/protein_to_fullembedding_map.pkl...\n",
      "Loaded mapping for 11795 proteins\n",
      "Loading ligand representations from processed_data/ligands_multiscale_etkdg/ligand_multiscale_etkdg_representations.pkl...\n",
      "Loaded multi-scale representations for 16525 ligands\n",
      "Creating combined multi-scale dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:02<00:00, 8550.89it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created combined multi-scale dataset with 19989 entries\n",
      "Stats: 19989 successful pairs\n",
      "       0 missing proteins\n",
      "       0 missing ligands\n",
      "       11 filtered ligands (>150 atoms)\n",
      "       85 truncated proteins (>1000 residues)\n",
      "Saving combined dataset to processed_data/combined_multiscale_dataset.pkl...\n",
      "Saved combined multi-scale dataset with 19989 entries\n",
      "\n",
      "Dataset Statistics:\n",
      "Affinity range: 0.40 to 15.00\n",
      "Mean affinity: 6.43\n",
      "Protein length range: 24 to 1000 residues\n",
      "Mean protein length: 280.66 residues\n",
      "Molecular weight range: 57.89 to 2498.03\n",
      "Mean molecular weight: 483.83\n",
      "Atom count range: 4 to 177 atoms\n",
      "Mean atom count: 33.62 atoms\n",
      "Combined multi-scale dataset created with 19989 protein-ligand pairs\n"
     ]
    }
   ],
   "source": [
    "parquet_file = 'data/binding_data_200k.parquet'\n",
    "\n",
    "# Protein embedding files (use full sequence embeddings)\n",
    "protein_embedding_file = 'processed_data/protein_full_embeddings_all.npz'\n",
    "protein_map_file = 'processed_data/protein_to_fullembedding_map.pkl'\n",
    "\n",
    "# Multi-scale ligand representation file\n",
    "ligand_data_file = 'processed_data/ligands_multiscale_etkdg/ligand_multiscale_etkdg_representations.pkl'\n",
    "\n",
    "# Output path for the combined dataset\n",
    "output_file = 'processed_data/combined_multiscale_dataset.pkl'\n",
    "\n",
    "# Create the combined dataset\n",
    "combined_data = create_multiscale_combined_dataset(\n",
    "    parquet_file=parquet_file,\n",
    "    protein_embedding_file=protein_embedding_file,\n",
    "    protein_map_file=protein_map_file,\n",
    "    ligand_data_file=ligand_data_file,\n",
    "    output_file=output_file,\n",
    "    use_full_sequence=True,  # Use full sequence embeddings\n",
    "    max_protein_length=1000  # Limit protein length for memory efficiency\n",
    ")\n",
    "\n",
    "print(f\"Combined multi-scale dataset created with {len(combined_data)} protein-ligand pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0184731-2218-4229-9e38-53e8a50d793d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a953a1d-04cc-4ad3-bb2d-5f0ceca4989b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading multi-scale combined dataset...\n",
      "Dataset size: 19989 protein-ligand pairs\n",
      "\n",
      "Keys in each data entry:\n",
      "- protein_embedding\n",
      "- protein_seq\n",
      "- protein_length\n",
      "- affinity\n",
      "- ligand_smiles\n",
      "- multi_scale\n",
      "- properties\n",
      "- functional_groups\n",
      "- physics_features\n",
      "\n",
      "Multi-scale representation levels:\n",
      "- level1_scaffold\n",
      "- level2_fragments\n",
      "- level3_complete\n",
      "\n",
      "Data shapes:\n",
      "Protein embedding shape: (284, 1280)\n",
      "Atoms in ligand graph: 22\n",
      "Bonds in ligand graph: 21\n",
      "Number of fragments: 1\n",
      "\n",
      "Analyzing physics-informed features...\n",
      "\n",
      "Analyzing fragment diversity...\n",
      "Total unique scaffolds: 9683\n",
      "Top 5 most common scaffolds: [('c1ccccc1', 1068), ('c1ncc2ncn([C@H]3CCCO3)c2n1', 430), ('C1CCOCC1', 225), ('C1CCNC1', 172), ('O=c1ccn([C@H]2CCCO2)c(=O)[nH]1', 116)]\n",
      "Average fragments per molecule: 5.51\n",
      "\n",
      "Analyzing protein embedding space...\n",
      "\n",
      "Multi-Scale Dataset Statistics:\n",
      "Total protein-ligand pairs: 19989\n",
      "Affinity range: 0.40 to 15.00\n",
      "Average number of atoms per molecule: 33.62\n",
      "Average number of bonds per molecule: 35.64\n",
      "Average number of fragments per molecule: 5.51\n",
      "\n",
      "All analyses complete! Visualizations saved to 'visualizations/multiscale/' directory.\n"
     ]
    }
   ],
   "source": [
    "# Create output directory for visualizations\n",
    "os.makedirs('visualizations/multiscale', exist_ok=True)\n",
    "\n",
    "# Load the multi-scale combined dataset\n",
    "print(\"Loading multi-scale combined dataset...\")\n",
    "with open('processed_data/combined_multiscale_dataset.pkl', 'rb') as f:\n",
    "    combined_data = pickle.load(f)\n",
    "\n",
    "# Basic information about the dataset\n",
    "print(f\"Dataset size: {len(combined_data)} protein-ligand pairs\")\n",
    "\n",
    "# Examine one sample in detail\n",
    "sample = combined_data[0]\n",
    "print(\"\\nKeys in each data entry:\")\n",
    "for key in sample.keys():\n",
    "    print(f\"- {key}\")\n",
    "\n",
    "print(\"\\nMulti-scale representation levels:\")\n",
    "for key in sample['multi_scale'].keys():\n",
    "    print(f\"- {key}\")\n",
    "\n",
    "# Check shapes of important data elements\n",
    "print(\"\\nData shapes:\")\n",
    "print(f\"Protein embedding shape: {np.array(sample['protein_embedding']).shape}\")\n",
    "print(f\"Atoms in ligand graph: {len(sample['multi_scale']['level3_complete']['graph']['atoms'])}\")\n",
    "print(f\"Bonds in ligand graph: {len(sample['multi_scale']['level3_complete']['graph']['bonds'])}\")\n",
    "print(f\"Number of fragments: {len(sample['multi_scale']['level2_fragments'])}\")\n",
    "\n",
    "# Distribution of affinity values\n",
    "affinities = [item['affinity'] for item in combined_data]\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(affinities, bins=50, kde=True)\n",
    "plt.title('Distribution of Binding Affinity Values')\n",
    "plt.xlabel('Affinity (-log10[M])')\n",
    "plt.ylabel('Count')\n",
    "plt.savefig('visualizations/multiscale/affinity_distribution.png')\n",
    "plt.close()\n",
    "\n",
    "# Analyze protein sequences\n",
    "protein_lengths = [len(item['protein_seq']) for item in combined_data]\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(protein_lengths, bins=50, kde=True)\n",
    "plt.title('Distribution of Protein Sequence Lengths')\n",
    "plt.xlabel('Length (amino acids)')\n",
    "plt.ylabel('Count')\n",
    "plt.savefig('visualizations/multiscale/protein_length_distribution.png')\n",
    "plt.close()\n",
    "\n",
    "# Analyze ligand properties\n",
    "mol_weights = [item['properties']['molecular_weight'] for item in combined_data]\n",
    "logp_values = [item['properties']['logp'] for item in combined_data]\n",
    "tpsa_values = [item['properties']['tpsa'] for item in combined_data]\n",
    "qed_values = [item['properties']['qed'] for item in combined_data]\n",
    "\n",
    "# Create a multi-panel figure for ligand properties\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "sns.histplot(mol_weights, bins=50, kde=True, ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Molecular Weight Distribution')\n",
    "axes[0, 0].set_xlabel('Molecular Weight (Da)')\n",
    "\n",
    "sns.histplot(logp_values, bins=50, kde=True, ax=axes[0, 1])\n",
    "axes[0, 1].set_title('LogP Distribution')\n",
    "axes[0, 1].set_xlabel('LogP')\n",
    "\n",
    "sns.histplot(tpsa_values, bins=50, kde=True, ax=axes[1, 0])\n",
    "axes[1, 0].set_title('TPSA Distribution')\n",
    "axes[1, 0].set_xlabel('Topological Polar Surface Area')\n",
    "\n",
    "sns.histplot(qed_values, bins=50, kde=True, ax=axes[1, 1])\n",
    "axes[1, 1].set_title('QED Distribution')\n",
    "axes[1, 1].set_xlabel('Quantitative Estimate of Drug-likeness')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('visualizations/multiscale/ligand_properties.png')\n",
    "plt.close()\n",
    "\n",
    "# Analyze correlations between properties and affinity\n",
    "properties_df = pd.DataFrame({\n",
    "    'affinity': affinities,\n",
    "    'molecular_weight': mol_weights,\n",
    "    'logp': logp_values,\n",
    "    'tpsa': tpsa_values,\n",
    "    'qed': qed_values\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(properties_df.corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Between Ligand Properties and Binding Affinity')\n",
    "plt.savefig('visualizations/multiscale/property_correlations.png')\n",
    "plt.close()\n",
    "\n",
    "# Analyze physics-informed features\n",
    "print(\"\\nAnalyzing physics-informed features...\")\n",
    "# Extract physics features that are numerical\n",
    "physics_features = []\n",
    "for item in combined_data:\n",
    "    if 'physics_features' in item:\n",
    "        features = {}\n",
    "        for key, value in item['physics_features'].items():\n",
    "            if isinstance(value, (int, float)) and key not in ['ring_count']:\n",
    "                features[key] = value\n",
    "        if features:\n",
    "            physics_features.append(features)\n",
    "\n",
    "if physics_features:\n",
    "    # Convert to DataFrame\n",
    "    physics_df = pd.DataFrame(physics_features)\n",
    "    \n",
    "    # Plot distributions of physics features\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle('Distribution of Physics-Informed Features', fontsize=16)\n",
    "    \n",
    "    if 'total_energy' in physics_df.columns:\n",
    "        sns.histplot(physics_df['total_energy'].clip(-100, 100), bins=50, kde=True, ax=axes[0, 0])\n",
    "        axes[0, 0].set_title('Total Energy Distribution')\n",
    "        axes[0, 0].set_xlabel('Total Energy')\n",
    "    \n",
    "    if 'avg_ring_strain' in physics_df.columns:\n",
    "        sns.histplot(physics_df['avg_ring_strain'], bins=50, kde=True, ax=axes[0, 1])\n",
    "        axes[0, 1].set_title('Average Ring Strain Distribution')\n",
    "        axes[0, 1].set_xlabel('Average Ring Strain')\n",
    "    \n",
    "    if 'total_ring_strain' in physics_df.columns:\n",
    "        sns.histplot(physics_df['total_ring_strain'], bins=50, kde=True, ax=axes[1, 0])\n",
    "        axes[1, 0].set_title('Total Ring Strain Distribution')\n",
    "        axes[1, 0].set_xlabel('Total Ring Strain')\n",
    "    \n",
    "    if 'total_bond_strain' in physics_df.columns:\n",
    "        sns.histplot(physics_df['total_bond_strain'].clip(0, 100), bins=50, kde=True, ax=axes[1, 1])\n",
    "        axes[1, 1].set_title('Total Bond Strain Distribution')\n",
    "        axes[1, 1].set_xlabel('Total Bond Strain')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('visualizations/multiscale/physics_features.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Count force field types\n",
    "    if 'force_field_type' in physics_df.columns:\n",
    "        force_field_counts = Counter([item['physics_features'].get('force_field_type', 'unknown') \n",
    "                                     for item in combined_data if 'physics_features' in item])\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(force_field_counts.keys(), force_field_counts.values())\n",
    "        plt.title('Force Field Types Used in Physics-Informed Features')\n",
    "        plt.xlabel('Force Field Type')\n",
    "        plt.ylabel('Count')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('visualizations/multiscale/force_field_types.png')\n",
    "        plt.close()\n",
    "\n",
    "# Function to visualize multi-scale representation of a ligand\n",
    "def visualize_multiscale(idx=0):\n",
    "    \"\"\"\n",
    "    Visualize the multi-scale representation of a ligand\n",
    "    \"\"\"\n",
    "    example = combined_data[idx]\n",
    "    \n",
    "    # Get smiles strings for each level\n",
    "    l1_smiles = example['multi_scale']['level1_scaffold']['smiles']\n",
    "    l3_smiles = example['multi_scale']['level3_complete']['smiles']\n",
    "    \n",
    "    # Get fragment smiles\n",
    "    fragment_smiles = [frag['smiles'] for frag in example['multi_scale']['level2_fragments']]\n",
    "    \n",
    "    # For visualization, limit the number of fragments\n",
    "    if len(fragment_smiles) > 4:\n",
    "        fragment_smiles = fragment_smiles[:4]\n",
    "    \n",
    "    # Create RDKit molecules\n",
    "    l1_mol = Chem.MolFromSmiles(l1_smiles) if l1_smiles else None\n",
    "    l3_mol = Chem.MolFromSmiles(l3_smiles)\n",
    "    fragment_mols = [Chem.MolFromSmiles(smiles) for smiles in fragment_smiles if smiles]\n",
    "    \n",
    "    # Setup the plot based on number of fragments\n",
    "    n_fragments = len(fragment_mols)\n",
    "    if n_fragments > 0:\n",
    "        fig, axes = plt.subplots(1, 2 + n_fragments, figsize=(15, 5))\n",
    "    else:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    \n",
    "    # Plot scaffold (Level 1)\n",
    "    if l1_mol:\n",
    "        img1 = Draw.MolToImage(l1_mol, size=(300, 300))\n",
    "        axes[0].imshow(img1)\n",
    "        axes[0].set_title(f\"Level 1: Scaffold\\n{l1_smiles[:20]}...\")\n",
    "    else:\n",
    "        axes[0].text(0.5, 0.5, \"No scaffold available\", ha='center', va='center')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Plot fragments (Level 2)\n",
    "    for i, mol in enumerate(fragment_mols):\n",
    "        if mol:\n",
    "            img = Draw.MolToImage(mol, size=(300, 300))\n",
    "            axes[i+1].imshow(img)\n",
    "            axes[i+1].set_title(f\"Level 2: Fragment {i+1}\")\n",
    "        else:\n",
    "            axes[i+1].text(0.5, 0.5, \"Invalid fragment\", ha='center', va='center')\n",
    "        axes[i+1].axis('off')\n",
    "    \n",
    "    # Plot complete molecule (Level 3)\n",
    "    if l3_mol:\n",
    "        img3 = Draw.MolToImage(l3_mol, size=(300, 300))\n",
    "        axes[-1].imshow(img3)\n",
    "        axes[-1].set_title(f\"Level 3: Complete\\n{l3_smiles[:20]}...\")\n",
    "    else:\n",
    "        axes[-1].text(0.5, 0.5, \"Invalid molecule\", ha='center', va='center')\n",
    "    axes[-1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Function to visualize the molecular graph\n",
    "def visualize_molecular_graph(idx=0):\n",
    "    \"\"\"\n",
    "    Visualize the molecular graph from the graph representation\n",
    "    \"\"\"\n",
    "    example = combined_data[idx]\n",
    "    graph_data = example['multi_scale']['level3_complete']['graph']\n",
    "    \n",
    "    atoms = graph_data['atoms']\n",
    "    bonds = graph_data['bonds']\n",
    "    \n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add nodes\n",
    "    for i, atom in enumerate(atoms):\n",
    "        G.add_node(i, \n",
    "                  atomic_num=atom['atomic_num'],\n",
    "                  charge=atom['formal_charge'],\n",
    "                  aromatic=atom['aromatic'])\n",
    "    \n",
    "    # Add edges\n",
    "    for bond in bonds:\n",
    "        G.add_edge(bond['start_atom'], \n",
    "                  bond['end_atom'], \n",
    "                  bond_type=bond['bond_type'],\n",
    "                  aromatic=bond['is_aromatic'])\n",
    "    \n",
    "    # Setup plot\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    \n",
    "    # Create position layout\n",
    "    pos = nx.spring_layout(G, seed=42)\n",
    "    \n",
    "    # Define node colors based on atom type\n",
    "    atom_types = [data['atomic_num'] for _, data in G.nodes(data=True)]\n",
    "    unique_atoms = list(set(atom_types))\n",
    "    atom_colors = {atom: i for i, atom in enumerate(unique_atoms)}\n",
    "    \n",
    "    # Map atom numbers to element symbols\n",
    "    element_map = {\n",
    "        1: 'H', 6: 'C', 7: 'N', 8: 'O', 9: 'F', \n",
    "        15: 'P', 16: 'S', 17: 'Cl', 35: 'Br', 53: 'I'\n",
    "    }\n",
    "    \n",
    "    # Get node colors\n",
    "    node_colors = [atom_colors[atom] for atom in atom_types]\n",
    "    \n",
    "    # Draw nodes\n",
    "    nx.draw_networkx_nodes(G, pos, node_color=node_colors, \n",
    "                          node_size=500, alpha=0.8,\n",
    "                          cmap=plt.cm.tab10)\n",
    "    \n",
    "    # Draw edges\n",
    "    edge_types = [G[u][v]['bond_type'] for u, v in G.edges()]\n",
    "    edge_colors = ['red' if t == 2 else 'blue' if t == 3 else 'black' for t in edge_types]\n",
    "    edge_styles = ['--' if G[u][v].get('aromatic', 0) else '-' for u, v in G.edges()]\n",
    "    \n",
    "    for (u, v), color, style in zip(G.edges(), edge_colors, edge_styles):\n",
    "        nx.draw_networkx_edges(G, pos, edgelist=[(u, v)], width=2, \n",
    "                              edge_color=color, style=style)\n",
    "    \n",
    "    # Draw labels\n",
    "    labels = {i: element_map.get(atom['atomic_num'], str(atom['atomic_num'])) \n",
    "              for i, atom in enumerate(atoms)}\n",
    "    nx.draw_networkx_labels(G, pos, labels, font_size=12)\n",
    "    \n",
    "    plt.title(f\"Molecular Graph Representation\\n{example['ligand_smiles'][:30]}...\")\n",
    "    plt.axis('off')\n",
    "    return plt.gcf()\n",
    "    \n",
    "# Visualize multi-scale representations and molecular graphs for examples\n",
    "# Select examples based on affinity ranges\n",
    "affinities_array = np.array(affinities)\n",
    "low_idx = np.where(affinities_array < np.percentile(affinities_array, 10))[0][0]\n",
    "med_idx = np.where(abs(affinities_array - np.median(affinities_array)) < 0.1)[0][0]\n",
    "high_idx = np.where(affinities_array > np.percentile(affinities_array, 90))[0][0]\n",
    "\n",
    "example_indices = [low_idx, med_idx, high_idx]\n",
    "affinity_labels = ['Low Affinity', 'Medium Affinity', 'High Affinity']\n",
    "\n",
    "# Visualize multi-scale representations\n",
    "for idx, label in zip(example_indices, affinity_labels):\n",
    "    fig = visualize_multiscale(idx)\n",
    "    plt.suptitle(f\"{label} Example (Affinity: {combined_data[idx]['affinity']:.2f})\", fontsize=16)\n",
    "    plt.savefig(f'visualizations/multiscale/multiscale_example_{label.lower().replace(\" \", \"_\")}.png')\n",
    "    plt.close()\n",
    "\n",
    "# Visualize molecular graphs\n",
    "for idx, label in zip(example_indices, affinity_labels):\n",
    "    fig = visualize_molecular_graph(idx)\n",
    "    plt.suptitle(f\"{label} Example - Molecular Graph (Affinity: {combined_data[idx]['affinity']:.2f})\", fontsize=16)\n",
    "    plt.savefig(f'visualizations/multiscale/molecular_graph_{label.lower().replace(\" \", \"_\")}.png')\n",
    "    plt.close()\n",
    "\n",
    "# Analyze fragment diversity\n",
    "print(\"\\nAnalyzing fragment diversity...\")\n",
    "# Extract scaffold data\n",
    "l1_scaffolds = [item['multi_scale']['level1_scaffold']['smiles'] for item in combined_data \n",
    "               if item['multi_scale']['level1_scaffold']['smiles']]\n",
    "l1_scaffold_counts = Counter(l1_scaffolds)\n",
    "\n",
    "# Get fragment counts\n",
    "fragment_counts = [len(item['multi_scale']['level2_fragments']) for item in combined_data]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(fragment_counts, bins=20, kde=True)\n",
    "plt.title('Distribution of Fragment Counts per Molecule')\n",
    "plt.xlabel('Number of Fragments')\n",
    "plt.ylabel('Count')\n",
    "plt.savefig('visualizations/multiscale/fragment_counts.png')\n",
    "plt.close()\n",
    "\n",
    "print(f\"Total unique scaffolds: {len(l1_scaffold_counts)}\")\n",
    "print(f\"Top 5 most common scaffolds: {l1_scaffold_counts.most_common(5)}\")\n",
    "print(f\"Average fragments per molecule: {np.mean(fragment_counts):.2f}\")\n",
    "\n",
    "# Analyze protein embedding space by using the mean embedding vector per protein\n",
    "print(\"\\nAnalyzing protein embedding space...\")\n",
    "# For full sequence embeddings, create fixed-length representations\n",
    "import random\n",
    "sample_indices = random.sample(range(len(combined_data)), min(1000, len(combined_data)))\n",
    "\n",
    "# Create fixed-length representations by taking the mean across sequence length\n",
    "sample_embeddings = np.array([\n",
    "    np.mean(combined_data[i]['protein_embedding'], axis=0) if isinstance(combined_data[i]['protein_embedding'], np.ndarray) and combined_data[i]['protein_embedding'].ndim > 1\n",
    "    else combined_data[i]['protein_embedding']\n",
    "    for i in sample_indices\n",
    "])\n",
    "sample_affinities = np.array([combined_data[i]['affinity'] for i in sample_indices])\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(sample_embeddings)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(pca_result[:, 0], pca_result[:, 1], c=sample_affinities, cmap='viridis', alpha=0.7)\n",
    "plt.colorbar(scatter, label='Binding Affinity')\n",
    "plt.title('PCA of Mean Protein Embeddings Colored by Binding Affinity')\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
    "plt.savefig('visualizations/multiscale/protein_embedding_pca.png')\n",
    "plt.close()\n",
    "\n",
    "# Analyze graph complexity vs affinity\n",
    "atom_counts = [len(item['multi_scale']['level3_complete']['graph']['atoms']) for item in combined_data]\n",
    "bond_counts = [len(item['multi_scale']['level3_complete']['graph']['bonds']) for item in combined_data]\n",
    "\n",
    "# Create a scatter plot of atom counts vs affinity\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(atom_counts, affinities, alpha=0.5)\n",
    "plt.title('Relationship Between Molecule Size and Binding Affinity')\n",
    "plt.xlabel('Number of Atoms')\n",
    "plt.ylabel('Binding Affinity')\n",
    "plt.savefig('visualizations/multiscale/atom_count_vs_affinity.png')\n",
    "plt.close()\n",
    "\n",
    "# Print overall statistics\n",
    "print(\"\\nMulti-Scale Dataset Statistics:\")\n",
    "print(f\"Total protein-ligand pairs: {len(combined_data)}\")\n",
    "print(f\"Affinity range: {min(affinities):.2f} to {max(affinities):.2f}\")\n",
    "print(f\"Average number of atoms per molecule: {np.mean(atom_counts):.2f}\")\n",
    "print(f\"Average number of bonds per molecule: {np.mean(bond_counts):.2f}\")\n",
    "print(f\"Average number of fragments per molecule: {np.mean(fragment_counts):.2f}\")\n",
    "\n",
    "print(\"\\nAll analyses complete! Visualizations saved to 'visualizations/multiscale/' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8086523-1c92-4cc3-94df-6614c8d398cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 19989 protein-ligand pairs\n",
      "\n",
      "Keys in each data entry:\n",
      "- protein_embedding\n",
      "- protein_seq\n",
      "- protein_length\n",
      "- affinity\n",
      "- ligand_smiles\n",
      "- multi_scale\n",
      "- properties\n",
      "- functional_groups\n",
      "- physics_features\n"
     ]
    }
   ],
   "source": [
    "with open('processed_data/combined_multiscale_dataset.pkl', 'rb') as f:\n",
    "    df = pickle.load(f)\n",
    "\n",
    "# Basic information about the dataset\n",
    "print(f\"Dataset size: {len(df)} protein-ligand pairs\")\n",
    "\n",
    "# Examine one sample in detail\n",
    "sample = df[1]\n",
    "print(\"\\nKeys in each data entry:\")\n",
    "for key in sample.keys():\n",
    "    print(f\"- {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5822e5a5-3ceb-485d-8225-2d480518f681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((377, 1280),\n",
       " 'APQTITELCSEYRNTQIYTINDKILSYTESMAGKREMVIITFKSGETFQVEVPGSQHIDSQKKAIERMKDTLRITYLTETKIDKLCVWNNKTPNSIAAISMKN',\n",
       " 103)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[\"protein_embedding\"].shape, sample[\"protein_seq\"], len(sample[\"protein_seq\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "559d6a48-3af3-4a26-a8ec-b4adda752cf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'total_energy': 143.39453268750978,\n",
       "  'force_field_type': 'MMFF',\n",
       "  'avg_ring_strain': 0.1,\n",
       "  'total_ring_strain': 0.2,\n",
       "  'ring_count': 2,\n",
       "  'ring_sizes': [6, 6]},\n",
       " 3.301029920578003)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['physics_features'], sample['affinity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "faa0c59a-ea11-4091-93a5-184647595725",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'level1_scaffold': {'smiles': 'c1ccc(O[C@@H]2CCCCO2)cc1',\n",
       "  'graph': {'atoms': [{'atomic_num': 6,\n",
       "     'formal_charge': 0,\n",
       "     'hybridization': 4,\n",
       "     'aromatic': 0,\n",
       "     'num_h': 2,\n",
       "     'degree': 2,\n",
       "     'chirality': 0,\n",
       "     'is_in_ring': 1,\n",
       "     'ring_sizes': [6],\n",
       "     'valence': 4},\n",
       "    {'atomic_num': 8,\n",
       "     'formal_charge': 0,\n",
       "     'hybridization': 4,\n",
       "     'aromatic': 0,\n",
       "     'num_h': 0,\n",
       "     'degree': 2,\n",
       "     'chirality': 0,\n",
       "     'is_in_ring': 1,\n",
       "     'ring_sizes': [6],\n",
       "     'valence': 2},\n",
       "    {'atomic_num': 6,\n",
       "     'formal_charge': 0,\n",
       "     'hybridization': 4,\n",
       "     'aromatic': 0,\n",
       "     'num_h': 1,\n",
       "     'degree': 3,\n",
       "     'chirality': 2,\n",
       "     'is_in_ring': 1,\n",
       "     'ring_sizes': [6],\n",
       "     'valence': 4},\n",
       "    {'atomic_num': 8,\n",
       "     'formal_charge': 0,\n",
       "     'hybridization': 3,\n",
       "     'aromatic': 0,\n",
       "     'num_h': 0,\n",
       "     'degree': 2,\n",
       "     'chirality': 0,\n",
       "     'is_in_ring': 0,\n",
       "     'ring_sizes': [],\n",
       "     'valence': 2},\n",
       "    {'atomic_num': 6,\n",
       "     'formal_charge': 0,\n",
       "     'hybridization': 3,\n",
       "     'aromatic': 1,\n",
       "     'num_h': 0,\n",
       "     'degree': 3,\n",
       "     'chirality': 0,\n",
       "     'is_in_ring': 1,\n",
       "     'ring_sizes': [6],\n",
       "     'valence': 4},\n",
       "    {'atomic_num': 6,\n",
       "     'formal_charge': 0,\n",
       "     'hybridization': 3,\n",
       "     'aromatic': 1,\n",
       "     'num_h': 1,\n",
       "     'degree': 2,\n",
       "     'chirality': 0,\n",
       "     'is_in_ring': 1,\n",
       "     'ring_sizes': [6],\n",
       "     'valence': 4},\n",
       "    {'atomic_num': 6,\n",
       "     'formal_charge': 0,\n",
       "     'hybridization': 3,\n",
       "     'aromatic': 1,\n",
       "     'num_h': 1,\n",
       "     'degree': 2,\n",
       "     'chirality': 0,\n",
       "     'is_in_ring': 1,\n",
       "     'ring_sizes': [6],\n",
       "     'valence': 4},\n",
       "    {'atomic_num': 6,\n",
       "     'formal_charge': 0,\n",
       "     'hybridization': 3,\n",
       "     'aromatic': 1,\n",
       "     'num_h': 1,\n",
       "     'degree': 2,\n",
       "     'chirality': 0,\n",
       "     'is_in_ring': 1,\n",
       "     'ring_sizes': [6],\n",
       "     'valence': 4},\n",
       "    {'atomic_num': 6,\n",
       "     'formal_charge': 0,\n",
       "     'hybridization': 3,\n",
       "     'aromatic': 1,\n",
       "     'num_h': 1,\n",
       "     'degree': 2,\n",
       "     'chirality': 0,\n",
       "     'is_in_ring': 1,\n",
       "     'ring_sizes': [6],\n",
       "     'valence': 4},\n",
       "    {'atomic_num': 6,\n",
       "     'formal_charge': 0,\n",
       "     'hybridization': 3,\n",
       "     'aromatic': 1,\n",
       "     'num_h': 1,\n",
       "     'degree': 2,\n",
       "     'chirality': 0,\n",
       "     'is_in_ring': 1,\n",
       "     'ring_sizes': [6],\n",
       "     'valence': 4},\n",
       "    {'atomic_num': 6,\n",
       "     'formal_charge': 0,\n",
       "     'hybridization': 4,\n",
       "     'aromatic': 0,\n",
       "     'num_h': 2,\n",
       "     'degree': 2,\n",
       "     'chirality': 0,\n",
       "     'is_in_ring': 1,\n",
       "     'ring_sizes': [6],\n",
       "     'valence': 4},\n",
       "    {'atomic_num': 6,\n",
       "     'formal_charge': 0,\n",
       "     'hybridization': 4,\n",
       "     'aromatic': 0,\n",
       "     'num_h': 2,\n",
       "     'degree': 2,\n",
       "     'chirality': 0,\n",
       "     'is_in_ring': 1,\n",
       "     'ring_sizes': [6],\n",
       "     'valence': 4},\n",
       "    {'atomic_num': 6,\n",
       "     'formal_charge': 0,\n",
       "     'hybridization': 4,\n",
       "     'aromatic': 0,\n",
       "     'num_h': 2,\n",
       "     'degree': 2,\n",
       "     'chirality': 0,\n",
       "     'is_in_ring': 1,\n",
       "     'ring_sizes': [6],\n",
       "     'valence': 4}],\n",
       "   'bonds': [{'start_atom': 0,\n",
       "     'end_atom': 1,\n",
       "     'bond_type': 1,\n",
       "     'is_conjugated': 0,\n",
       "     'is_aromatic': 0,\n",
       "     'is_in_ring': 1,\n",
       "     'stereo': 0,\n",
       "     'ring_sizes': [6]},\n",
       "    {'start_atom': 1,\n",
       "     'end_atom': 2,\n",
       "     'bond_type': 1,\n",
       "     'is_conjugated': 0,\n",
       "     'is_aromatic': 0,\n",
       "     'is_in_ring': 1,\n",
       "     'stereo': 0,\n",
       "     'ring_sizes': [6]},\n",
       "    {'start_atom': 2,\n",
       "     'end_atom': 3,\n",
       "     'bond_type': 1,\n",
       "     'is_conjugated': 0,\n",
       "     'is_aromatic': 0,\n",
       "     'is_in_ring': 0,\n",
       "     'stereo': 0,\n",
       "     'ring_sizes': []},\n",
       "    {'start_atom': 3,\n",
       "     'end_atom': 4,\n",
       "     'bond_type': 1,\n",
       "     'is_conjugated': 1,\n",
       "     'is_aromatic': 0,\n",
       "     'is_in_ring': 0,\n",
       "     'stereo': 0,\n",
       "     'ring_sizes': []},\n",
       "    {'start_atom': 4,\n",
       "     'end_atom': 5,\n",
       "     'bond_type': 12,\n",
       "     'is_conjugated': 1,\n",
       "     'is_aromatic': 1,\n",
       "     'is_in_ring': 1,\n",
       "     'stereo': 0,\n",
       "     'ring_sizes': [6]},\n",
       "    {'start_atom': 5,\n",
       "     'end_atom': 6,\n",
       "     'bond_type': 12,\n",
       "     'is_conjugated': 1,\n",
       "     'is_aromatic': 1,\n",
       "     'is_in_ring': 1,\n",
       "     'stereo': 0,\n",
       "     'ring_sizes': [6]},\n",
       "    {'start_atom': 6,\n",
       "     'end_atom': 7,\n",
       "     'bond_type': 12,\n",
       "     'is_conjugated': 1,\n",
       "     'is_aromatic': 1,\n",
       "     'is_in_ring': 1,\n",
       "     'stereo': 0,\n",
       "     'ring_sizes': [6]},\n",
       "    {'start_atom': 7,\n",
       "     'end_atom': 8,\n",
       "     'bond_type': 12,\n",
       "     'is_conjugated': 1,\n",
       "     'is_aromatic': 1,\n",
       "     'is_in_ring': 1,\n",
       "     'stereo': 0,\n",
       "     'ring_sizes': [6]},\n",
       "    {'start_atom': 8,\n",
       "     'end_atom': 9,\n",
       "     'bond_type': 12,\n",
       "     'is_conjugated': 1,\n",
       "     'is_aromatic': 1,\n",
       "     'is_in_ring': 1,\n",
       "     'stereo': 0,\n",
       "     'ring_sizes': [6]},\n",
       "    {'start_atom': 2,\n",
       "     'end_atom': 10,\n",
       "     'bond_type': 1,\n",
       "     'is_conjugated': 0,\n",
       "     'is_aromatic': 0,\n",
       "     'is_in_ring': 1,\n",
       "     'stereo': 0,\n",
       "     'ring_sizes': [6]},\n",
       "    {'start_atom': 10,\n",
       "     'end_atom': 11,\n",
       "     'bond_type': 1,\n",
       "     'is_conjugated': 0,\n",
       "     'is_aromatic': 0,\n",
       "     'is_in_ring': 1,\n",
       "     'stereo': 0,\n",
       "     'ring_sizes': [6]},\n",
       "    {'start_atom': 11,\n",
       "     'end_atom': 12,\n",
       "     'bond_type': 1,\n",
       "     'is_conjugated': 0,\n",
       "     'is_aromatic': 0,\n",
       "     'is_in_ring': 1,\n",
       "     'stereo': 0,\n",
       "     'ring_sizes': [6]},\n",
       "    {'start_atom': 12,\n",
       "     'end_atom': 0,\n",
       "     'bond_type': 1,\n",
       "     'is_conjugated': 0,\n",
       "     'is_aromatic': 0,\n",
       "     'is_in_ring': 1,\n",
       "     'stereo': 0,\n",
       "     'ring_sizes': [6]},\n",
       "    {'start_atom': 9,\n",
       "     'end_atom': 4,\n",
       "     'bond_type': 12,\n",
       "     'is_conjugated': 1,\n",
       "     'is_aromatic': 1,\n",
       "     'is_in_ring': 1,\n",
       "     'stereo': 0,\n",
       "     'ring_sizes': [6]}]}},\n",
       " 'level2_fragments': [{'smiles': '[16*]c1cccc([N+](=O)[O-])c1',\n",
       "   'graph': {'atoms': [{'atomic_num': 0,\n",
       "      'formal_charge': 0,\n",
       "      'hybridization': 0,\n",
       "      'aromatic': 0,\n",
       "      'num_h': 0,\n",
       "      'degree': 1,\n",
       "      'chirality': 0,\n",
       "      'is_in_ring': 0,\n",
       "      'ring_sizes': [],\n",
       "      'valence': 1},\n",
       "     {'atomic_num': 6,\n",
       "      'formal_charge': 0,\n",
       "      'hybridization': 3,\n",
       "      'aromatic': 1,\n",
       "      'num_h': 0,\n",
       "      'degree': 3,\n",
       "      'chirality': 0,\n",
       "      'is_in_ring': 1,\n",
       "      'ring_sizes': [6],\n",
       "      'valence': 4},\n",
       "     {'atomic_num': 6,\n",
       "      'formal_charge': 0,\n",
       "      'hybridization': 3,\n",
       "      'aromatic': 1,\n",
       "      'num_h': 1,\n",
       "      'degree': 2,\n",
       "      'chirality': 0,\n",
       "      'is_in_ring': 1,\n",
       "      'ring_sizes': [6],\n",
       "      'valence': 4},\n",
       "     {'atomic_num': 6,\n",
       "      'formal_charge': 0,\n",
       "      'hybridization': 3,\n",
       "      'aromatic': 1,\n",
       "      'num_h': 1,\n",
       "      'degree': 2,\n",
       "      'chirality': 0,\n",
       "      'is_in_ring': 1,\n",
       "      'ring_sizes': [6],\n",
       "      'valence': 4},\n",
       "     {'atomic_num': 6,\n",
       "      'formal_charge': 0,\n",
       "      'hybridization': 3,\n",
       "      'aromatic': 1,\n",
       "      'num_h': 1,\n",
       "      'degree': 2,\n",
       "      'chirality': 0,\n",
       "      'is_in_ring': 1,\n",
       "      'ring_sizes': [6],\n",
       "      'valence': 4},\n",
       "     {'atomic_num': 6,\n",
       "      'formal_charge': 0,\n",
       "      'hybridization': 3,\n",
       "      'aromatic': 1,\n",
       "      'num_h': 0,\n",
       "      'degree': 3,\n",
       "      'chirality': 0,\n",
       "      'is_in_ring': 1,\n",
       "      'ring_sizes': [6],\n",
       "      'valence': 4},\n",
       "     {'atomic_num': 7,\n",
       "      'formal_charge': 1,\n",
       "      'hybridization': 3,\n",
       "      'aromatic': 0,\n",
       "      'num_h': 0,\n",
       "      'degree': 3,\n",
       "      'chirality': 0,\n",
       "      'is_in_ring': 0,\n",
       "      'ring_sizes': [],\n",
       "      'valence': 4},\n",
       "     {'atomic_num': 8,\n",
       "      'formal_charge': 0,\n",
       "      'hybridization': 3,\n",
       "      'aromatic': 0,\n",
       "      'num_h': 0,\n",
       "      'degree': 1,\n",
       "      'chirality': 0,\n",
       "      'is_in_ring': 0,\n",
       "      'ring_sizes': [],\n",
       "      'valence': 2},\n",
       "     {'atomic_num': 8,\n",
       "      'formal_charge': -1,\n",
       "      'hybridization': 3,\n",
       "      'aromatic': 0,\n",
       "      'num_h': 0,\n",
       "      'degree': 1,\n",
       "      'chirality': 0,\n",
       "      'is_in_ring': 0,\n",
       "      'ring_sizes': [],\n",
       "      'valence': 1},\n",
       "     {'atomic_num': 6,\n",
       "      'formal_charge': 0,\n",
       "      'hybridization': 3,\n",
       "      'aromatic': 1,\n",
       "      'num_h': 1,\n",
       "      'degree': 2,\n",
       "      'chirality': 0,\n",
       "      'is_in_ring': 1,\n",
       "      'ring_sizes': [6],\n",
       "      'valence': 4}],\n",
       "    'bonds': [{'start_atom': 0,\n",
       "      'end_atom': 1,\n",
       "      'bond_type': 1,\n",
       "      'is_conjugated': 0,\n",
       "      'is_aromatic': 0,\n",
       "      'is_in_ring': 0,\n",
       "      'stereo': 0,\n",
       "      'ring_sizes': []},\n",
       "     {'start_atom': 1,\n",
       "      'end_atom': 2,\n",
       "      'bond_type': 12,\n",
       "      'is_conjugated': 1,\n",
       "      'is_aromatic': 1,\n",
       "      'is_in_ring': 1,\n",
       "      'stereo': 0,\n",
       "      'ring_sizes': [6]},\n",
       "     {'start_atom': 2,\n",
       "      'end_atom': 3,\n",
       "      'bond_type': 12,\n",
       "      'is_conjugated': 1,\n",
       "      'is_aromatic': 1,\n",
       "      'is_in_ring': 1,\n",
       "      'stereo': 0,\n",
       "      'ring_sizes': [6]},\n",
       "     {'start_atom': 3,\n",
       "      'end_atom': 4,\n",
       "      'bond_type': 12,\n",
       "      'is_conjugated': 1,\n",
       "      'is_aromatic': 1,\n",
       "      'is_in_ring': 1,\n",
       "      'stereo': 0,\n",
       "      'ring_sizes': [6]},\n",
       "     {'start_atom': 4,\n",
       "      'end_atom': 5,\n",
       "      'bond_type': 12,\n",
       "      'is_conjugated': 1,\n",
       "      'is_aromatic': 1,\n",
       "      'is_in_ring': 1,\n",
       "      'stereo': 0,\n",
       "      'ring_sizes': [6]},\n",
       "     {'start_atom': 5,\n",
       "      'end_atom': 6,\n",
       "      'bond_type': 1,\n",
       "      'is_conjugated': 1,\n",
       "      'is_aromatic': 0,\n",
       "      'is_in_ring': 0,\n",
       "      'stereo': 0,\n",
       "      'ring_sizes': []},\n",
       "     {'start_atom': 6,\n",
       "      'end_atom': 7,\n",
       "      'bond_type': 2,\n",
       "      'is_conjugated': 1,\n",
       "      'is_aromatic': 0,\n",
       "      'is_in_ring': 0,\n",
       "      'stereo': 0,\n",
       "      'ring_sizes': []},\n",
       "     {'start_atom': 6,\n",
       "      'end_atom': 8,\n",
       "      'bond_type': 1,\n",
       "      'is_conjugated': 1,\n",
       "      'is_aromatic': 0,\n",
       "      'is_in_ring': 0,\n",
       "      'stereo': 0,\n",
       "      'ring_sizes': []},\n",
       "     {'start_atom': 5,\n",
       "      'end_atom': 9,\n",
       "      'bond_type': 12,\n",
       "      'is_conjugated': 1,\n",
       "      'is_aromatic': 1,\n",
       "      'is_in_ring': 1,\n",
       "      'stereo': 0,\n",
       "      'ring_sizes': [6]},\n",
       "     {'start_atom': 9,\n",
       "      'end_atom': 1,\n",
       "      'bond_type': 12,\n",
       "      'is_conjugated': 1,\n",
       "      'is_aromatic': 1,\n",
       "      'is_in_ring': 1,\n",
       "      'stereo': 0,\n",
       "      'ring_sizes': [6]}]}},\n",
       "  {'smiles': '[13*][C@H]1O[C@H]([13*])[C@H](O)C(O)[C@H]1O',\n",
       "   'graph': {'atoms': [{'atomic_num': 0,\n",
       "      'formal_charge': 0,\n",
       "      'hybridization': 0,\n",
       "      'aromatic': 0,\n",
       "      'num_h': 0,\n",
       "      'degree': 1,\n",
       "      'chirality': 0,\n",
       "      'is_in_ring': 0,\n",
       "      'ring_sizes': [],\n",
       "      'valence': 1},\n",
       "     {'atomic_num': 6,\n",
       "      'formal_charge': 0,\n",
       "      'hybridization': 4,\n",
       "      'aromatic': 0,\n",
       "      'num_h': 1,\n",
       "      'degree': 3,\n",
       "      'chirality': 1,\n",
       "      'is_in_ring': 1,\n",
       "      'ring_sizes': [6],\n",
       "      'valence': 4},\n",
       "     {'atomic_num': 8,\n",
       "      'formal_charge': 0,\n",
       "      'hybridization': 4,\n",
       "      'aromatic': 0,\n",
       "      'num_h': 0,\n",
       "      'degree': 2,\n",
       "      'chirality': 0,\n",
       "      'is_in_ring': 1,\n",
       "      'ring_sizes': [6],\n",
       "      'valence': 2},\n",
       "     {'atomic_num': 6,\n",
       "      'formal_charge': 0,\n",
       "      'hybridization': 4,\n",
       "      'aromatic': 0,\n",
       "      'num_h': 1,\n",
       "      'degree': 3,\n",
       "      'chirality': 2,\n",
       "      'is_in_ring': 1,\n",
       "      'ring_sizes': [6],\n",
       "      'valence': 4},\n",
       "     {'atomic_num': 0,\n",
       "      'formal_charge': 0,\n",
       "      'hybridization': 0,\n",
       "      'aromatic': 0,\n",
       "      'num_h': 0,\n",
       "      'degree': 1,\n",
       "      'chirality': 0,\n",
       "      'is_in_ring': 0,\n",
       "      'ring_sizes': [],\n",
       "      'valence': 1},\n",
       "     {'atomic_num': 6,\n",
       "      'formal_charge': 0,\n",
       "      'hybridization': 4,\n",
       "      'aromatic': 0,\n",
       "      'num_h': 1,\n",
       "      'degree': 3,\n",
       "      'chirality': 2,\n",
       "      'is_in_ring': 1,\n",
       "      'ring_sizes': [6],\n",
       "      'valence': 4},\n",
       "     {'atomic_num': 8,\n",
       "      'formal_charge': 0,\n",
       "      'hybridization': 4,\n",
       "      'aromatic': 0,\n",
       "      'num_h': 1,\n",
       "      'degree': 1,\n",
       "      'chirality': 0,\n",
       "      'is_in_ring': 0,\n",
       "      'ring_sizes': [],\n",
       "      'valence': 2},\n",
       "     {'atomic_num': 6,\n",
       "      'formal_charge': 0,\n",
       "      'hybridization': 4,\n",
       "      'aromatic': 0,\n",
       "      'num_h': 1,\n",
       "      'degree': 3,\n",
       "      'chirality': 0,\n",
       "      'is_in_ring': 1,\n",
       "      'ring_sizes': [6],\n",
       "      'valence': 4},\n",
       "     {'atomic_num': 8,\n",
       "      'formal_charge': 0,\n",
       "      'hybridization': 4,\n",
       "      'aromatic': 0,\n",
       "      'num_h': 1,\n",
       "      'degree': 1,\n",
       "      'chirality': 0,\n",
       "      'is_in_ring': 0,\n",
       "      'ring_sizes': [],\n",
       "      'valence': 2},\n",
       "     {'atomic_num': 6,\n",
       "      'formal_charge': 0,\n",
       "      'hybridization': 4,\n",
       "      'aromatic': 0,\n",
       "      'num_h': 1,\n",
       "      'degree': 3,\n",
       "      'chirality': 1,\n",
       "      'is_in_ring': 1,\n",
       "      'ring_sizes': [6],\n",
       "      'valence': 4},\n",
       "     {'atomic_num': 8,\n",
       "      'formal_charge': 0,\n",
       "      'hybridization': 4,\n",
       "      'aromatic': 0,\n",
       "      'num_h': 1,\n",
       "      'degree': 1,\n",
       "      'chirality': 0,\n",
       "      'is_in_ring': 0,\n",
       "      'ring_sizes': [],\n",
       "      'valence': 2}],\n",
       "    'bonds': [{'start_atom': 0,\n",
       "      'end_atom': 1,\n",
       "      'bond_type': 1,\n",
       "      'is_conjugated': 0,\n",
       "      'is_aromatic': 0,\n",
       "      'is_in_ring': 0,\n",
       "      'stereo': 0,\n",
       "      'ring_sizes': []},\n",
       "     {'start_atom': 1,\n",
       "      'end_atom': 2,\n",
       "      'bond_type': 1,\n",
       "      'is_conjugated': 0,\n",
       "      'is_aromatic': 0,\n",
       "      'is_in_ring': 1,\n",
       "      'stereo': 0,\n",
       "      'ring_sizes': [6]},\n",
       "     {'start_atom': 2,\n",
       "      'end_atom': 3,\n",
       "      'bond_type': 1,\n",
       "      'is_conjugated': 0,\n",
       "      'is_aromatic': 0,\n",
       "      'is_in_ring': 1,\n",
       "      'stereo': 0,\n",
       "      'ring_sizes': [6]},\n",
       "     {'start_atom': 3,\n",
       "      'end_atom': 4,\n",
       "      'bond_type': 1,\n",
       "      'is_conjugated': 0,\n",
       "      'is_aromatic': 0,\n",
       "      'is_in_ring': 0,\n",
       "      'stereo': 0,\n",
       "      'ring_sizes': []},\n",
       "     {'start_atom': 3,\n",
       "      'end_atom': 5,\n",
       "      'bond_type': 1,\n",
       "      'is_conjugated': 0,\n",
       "      'is_aromatic': 0,\n",
       "      'is_in_ring': 1,\n",
       "      'stereo': 0,\n",
       "      'ring_sizes': [6]},\n",
       "     {'start_atom': 5,\n",
       "      'end_atom': 6,\n",
       "      'bond_type': 1,\n",
       "      'is_conjugated': 0,\n",
       "      'is_aromatic': 0,\n",
       "      'is_in_ring': 0,\n",
       "      'stereo': 0,\n",
       "      'ring_sizes': []},\n",
       "     {'start_atom': 5,\n",
       "      'end_atom': 7,\n",
       "      'bond_type': 1,\n",
       "      'is_conjugated': 0,\n",
       "      'is_aromatic': 0,\n",
       "      'is_in_ring': 1,\n",
       "      'stereo': 0,\n",
       "      'ring_sizes': [6]},\n",
       "     {'start_atom': 7,\n",
       "      'end_atom': 8,\n",
       "      'bond_type': 1,\n",
       "      'is_conjugated': 0,\n",
       "      'is_aromatic': 0,\n",
       "      'is_in_ring': 0,\n",
       "      'stereo': 0,\n",
       "      'ring_sizes': []},\n",
       "     {'start_atom': 7,\n",
       "      'end_atom': 9,\n",
       "      'bond_type': 1,\n",
       "      'is_conjugated': 0,\n",
       "      'is_aromatic': 0,\n",
       "      'is_in_ring': 1,\n",
       "      'stereo': 0,\n",
       "      'ring_sizes': [6]},\n",
       "     {'start_atom': 9,\n",
       "      'end_atom': 10,\n",
       "      'bond_type': 1,\n",
       "      'is_conjugated': 0,\n",
       "      'is_aromatic': 0,\n",
       "      'is_in_ring': 0,\n",
       "      'stereo': 0,\n",
       "      'ring_sizes': []},\n",
       "     {'start_atom': 9,\n",
       "      'end_atom': 1,\n",
       "      'bond_type': 1,\n",
       "      'is_conjugated': 0,\n",
       "      'is_aromatic': 0,\n",
       "      'is_in_ring': 1,\n",
       "      'stereo': 0,\n",
       "      'ring_sizes': [6]}]}},\n",
       "  {'smiles': '[8*]CO',\n",
       "   'graph': {'atoms': [{'atomic_num': 0,\n",
       "      'formal_charge': 0,\n",
       "      'hybridization': 0,\n",
       "      'aromatic': 0,\n",
       "      'num_h': 0,\n",
       "      'degree': 1,\n",
       "      'chirality': 0,\n",
       "      'is_in_ring': 0,\n",
       "      'ring_sizes': [],\n",
       "      'valence': 1},\n",
       "     {'atomic_num': 6,\n",
       "      'formal_charge': 0,\n",
       "      'hybridization': 4,\n",
       "      'aromatic': 0,\n",
       "      'num_h': 2,\n",
       "      'degree': 2,\n",
       "      'chirality': 0,\n",
       "      'is_in_ring': 0,\n",
       "      'ring_sizes': [],\n",
       "      'valence': 4},\n",
       "     {'atomic_num': 8,\n",
       "      'formal_charge': 0,\n",
       "      'hybridization': 4,\n",
       "      'aromatic': 0,\n",
       "      'num_h': 1,\n",
       "      'degree': 1,\n",
       "      'chirality': 0,\n",
       "      'is_in_ring': 0,\n",
       "      'ring_sizes': [],\n",
       "      'valence': 2}],\n",
       "    'bonds': [{'start_atom': 0,\n",
       "      'end_atom': 1,\n",
       "      'bond_type': 1,\n",
       "      'is_conjugated': 0,\n",
       "      'is_aromatic': 0,\n",
       "      'is_in_ring': 0,\n",
       "      'stereo': 0,\n",
       "      'ring_sizes': []},\n",
       "     {'start_atom': 1,\n",
       "      'end_atom': 2,\n",
       "      'bond_type': 1,\n",
       "      'is_conjugated': 0,\n",
       "      'is_aromatic': 0,\n",
       "      'is_in_ring': 0,\n",
       "      'stereo': 0,\n",
       "      'ring_sizes': []}]}},\n",
       "  {'smiles': '[3*]O[3*]',\n",
       "   'graph': {'atoms': [{'atomic_num': 0,\n",
       "      'formal_charge': 0,\n",
       "      'hybridization': 0,\n",
       "      'aromatic': 0,\n",
       "      'num_h': 0,\n",
       "      'degree': 1,\n",
       "      'chirality': 0,\n",
       "      'is_in_ring': 0,\n",
       "      'ring_sizes': [],\n",
       "      'valence': 1},\n",
       "     {'atomic_num': 8,\n",
       "      'formal_charge': 0,\n",
       "      'hybridization': 4,\n",
       "      'aromatic': 0,\n",
       "      'num_h': 0,\n",
       "      'degree': 2,\n",
       "      'chirality': 0,\n",
       "      'is_in_ring': 0,\n",
       "      'ring_sizes': [],\n",
       "      'valence': 2},\n",
       "     {'atomic_num': 0,\n",
       "      'formal_charge': 0,\n",
       "      'hybridization': 0,\n",
       "      'aromatic': 0,\n",
       "      'num_h': 0,\n",
       "      'degree': 1,\n",
       "      'chirality': 0,\n",
       "      'is_in_ring': 0,\n",
       "      'ring_sizes': [],\n",
       "      'valence': 1}],\n",
       "    'bonds': [{'start_atom': 0,\n",
       "      'end_atom': 1,\n",
       "      'bond_type': 1,\n",
       "      'is_conjugated': 0,\n",
       "      'is_aromatic': 0,\n",
       "      'is_in_ring': 0,\n",
       "      'stereo': 0,\n",
       "      'ring_sizes': []},\n",
       "     {'start_atom': 1,\n",
       "      'end_atom': 2,\n",
       "      'bond_type': 1,\n",
       "      'is_conjugated': 0,\n",
       "      'is_aromatic': 0,\n",
       "      'is_in_ring': 0,\n",
       "      'stereo': 0,\n",
       "      'ring_sizes': []}]}}],\n",
       " 'level3_complete': {'smiles': 'OC[C@H]1O[C@H](Oc2cccc(c2)N(=O)=O)[C@@H]([C@H]([C@H]1O)O)O',\n",
       "  'graph': {'atoms': [{'atomic_num': 8,\n",
       "     'formal_charge': 0,\n",
       "     'hybridization': 4,\n",
       "     'aromatic': 0,\n",
       "     'num_h': 1,\n",
       "     'degree': 1,\n",
       "     'chirality': 0,\n",
       "     'is_in_ring': 0,\n",
       "     'ring_sizes': [],\n",
       "     'valence': 2},\n",
       "    {'atomic_num': 6,\n",
       "     'formal_charge': 0,\n",
       "     'hybridization': 4,\n",
       "     'aromatic': 0,\n",
       "     'num_h': 2,\n",
       "     'degree': 2,\n",
       "     'chirality': 0,\n",
       "     'is_in_ring': 0,\n",
       "     'ring_sizes': [],\n",
       "     'valence': 4},\n",
       "    {'atomic_num': 6,\n",
       "     'formal_charge': 0,\n",
       "     'hybridization': 4,\n",
       "     'aromatic': 0,\n",
       "     'num_h': 1,\n",
       "     'degree': 3,\n",
       "     'chirality': 1,\n",
       "     'is_in_ring': 1,\n",
       "     'ring_sizes': [6],\n",
       "     'valence': 4},\n",
       "    {'atomic_num': 8,\n",
       "     'formal_charge': 0,\n",
       "     'hybridization': 4,\n",
       "     'aromatic': 0,\n",
       "     'num_h': 0,\n",
       "     'degree': 2,\n",
       "     'chirality': 0,\n",
       "     'is_in_ring': 1,\n",
       "     'ring_sizes': [6],\n",
       "     'valence': 2},\n",
       "    {'atomic_num': 6,\n",
       "     'formal_charge': 0,\n",
       "     'hybridization': 4,\n",
       "     'aromatic': 0,\n",
       "     'num_h': 1,\n",
       "     'degree': 3,\n",
       "     'chirality': 2,\n",
       "     'is_in_ring': 1,\n",
       "     'ring_sizes': [6],\n",
       "     'valence': 4},\n",
       "    {'atomic_num': 8,\n",
       "     'formal_charge': 0,\n",
       "     'hybridization': 3,\n",
       "     'aromatic': 0,\n",
       "     'num_h': 0,\n",
       "     'degree': 2,\n",
       "     'chirality': 0,\n",
       "     'is_in_ring': 0,\n",
       "     'ring_sizes': [],\n",
       "     'valence': 2},\n",
       "    {'atomic_num': 6,\n",
       "     'formal_charge': 0,\n",
       "     'hybridization': 3,\n",
       "     'aromatic': 1,\n",
       "     'num_h': 0,\n",
       "     'degree': 3,\n",
       "     'chirality': 0,\n",
       "     'is_in_ring': 1,\n",
       "     'ring_sizes': [6],\n",
       "     'valence': 4},\n",
       "    {'atomic_num': 6,\n",
       "     'formal_charge': 0,\n",
       "     'hybridization': 3,\n",
       "     'aromatic': 1,\n",
       "     'num_h': 1,\n",
       "     'degree': 2,\n",
       "     'chirality': 0,\n",
       "     'is_in_ring': 1,\n",
       "     'ring_sizes': [6],\n",
       "     'valence': 4},\n",
       "    {'atomic_num': 6,\n",
       "     'formal_charge': 0,\n",
       "     'hybridization': 3,\n",
       "     'aromatic': 1,\n",
       "     'num_h': 1,\n",
       "     'degree': 2,\n",
       "     'chirality': 0,\n",
       "     'is_in_ring': 1,\n",
       "     'ring_sizes': [6],\n",
       "     'valence': 4},\n",
       "    {'atomic_num': 6,\n",
       "     'formal_charge': 0,\n",
       "     'hybridization': 3,\n",
       "     'aromatic': 1,\n",
       "     'num_h': 1,\n",
       "     'degree': 2,\n",
       "     'chirality': 0,\n",
       "     'is_in_ring': 1,\n",
       "     'ring_sizes': [6],\n",
       "     'valence': 4},\n",
       "    {'atomic_num': 6,\n",
       "     'formal_charge': 0,\n",
       "     'hybridization': 3,\n",
       "     'aromatic': 1,\n",
       "     'num_h': 0,\n",
       "     'degree': 3,\n",
       "     'chirality': 0,\n",
       "     'is_in_ring': 1,\n",
       "     'ring_sizes': [6],\n",
       "     'valence': 4},\n",
       "    {'atomic_num': 6,\n",
       "     'formal_charge': 0,\n",
       "     'hybridization': 3,\n",
       "     'aromatic': 1,\n",
       "     'num_h': 1,\n",
       "     'degree': 2,\n",
       "     'chirality': 0,\n",
       "     'is_in_ring': 1,\n",
       "     'ring_sizes': [6],\n",
       "     'valence': 4},\n",
       "    {'atomic_num': 7,\n",
       "     'formal_charge': 1,\n",
       "     'hybridization': 3,\n",
       "     'aromatic': 0,\n",
       "     'num_h': 0,\n",
       "     'degree': 3,\n",
       "     'chirality': 0,\n",
       "     'is_in_ring': 0,\n",
       "     'ring_sizes': [],\n",
       "     'valence': 4},\n",
       "    {'atomic_num': 8,\n",
       "     'formal_charge': -1,\n",
       "     'hybridization': 3,\n",
       "     'aromatic': 0,\n",
       "     'num_h': 0,\n",
       "     'degree': 1,\n",
       "     'chirality': 0,\n",
       "     'is_in_ring': 0,\n",
       "     'ring_sizes': [],\n",
       "     'valence': 1},\n",
       "    {'atomic_num': 8,\n",
       "     'formal_charge': 0,\n",
       "     'hybridization': 3,\n",
       "     'aromatic': 0,\n",
       "     'num_h': 0,\n",
       "     'degree': 1,\n",
       "     'chirality': 0,\n",
       "     'is_in_ring': 0,\n",
       "     'ring_sizes': [],\n",
       "     'valence': 2},\n",
       "    {'atomic_num': 6,\n",
       "     'formal_charge': 0,\n",
       "     'hybridization': 4,\n",
       "     'aromatic': 0,\n",
       "     'num_h': 1,\n",
       "     'degree': 3,\n",
       "     'chirality': 1,\n",
       "     'is_in_ring': 1,\n",
       "     'ring_sizes': [6],\n",
       "     'valence': 4},\n",
       "    {'atomic_num': 6,\n",
       "     'formal_charge': 0,\n",
       "     'hybridization': 4,\n",
       "     'aromatic': 0,\n",
       "     'num_h': 1,\n",
       "     'degree': 3,\n",
       "     'chirality': 2,\n",
       "     'is_in_ring': 1,\n",
       "     'ring_sizes': [6],\n",
       "     'valence': 4},\n",
       "    {'atomic_num': 6,\n",
       "     'formal_charge': 0,\n",
       "     'hybridization': 4,\n",
       "     'aromatic': 0,\n",
       "     'num_h': 1,\n",
       "     'degree': 3,\n",
       "     'chirality': 1,\n",
       "     'is_in_ring': 1,\n",
       "     'ring_sizes': [6],\n",
       "     'valence': 4},\n",
       "    {'atomic_num': 8,\n",
       "     'formal_charge': 0,\n",
       "     'hybridization': 4,\n",
       "     'aromatic': 0,\n",
       "     'num_h': 1,\n",
       "     'degree': 1,\n",
       "     'chirality': 0,\n",
       "     'is_in_ring': 0,\n",
       "     'ring_sizes': [],\n",
       "     'valence': 2},\n",
       "    {'atomic_num': 8,\n",
       "     'formal_charge': 0,\n",
       "     'hybridization': 4,\n",
       "     'aromatic': 0,\n",
       "     'num_h': 1,\n",
       "     'degree': 1,\n",
       "     'chirality': 0,\n",
       "     'is_in_ring': 0,\n",
       "     'ring_sizes': [],\n",
       "     'valence': 2},\n",
       "    {'atomic_num': 8,\n",
       "     'formal_charge': 0,\n",
       "     'hybridization': 4,\n",
       "     'aromatic': 0,\n",
       "     'num_h': 1,\n",
       "     'degree': 1,\n",
       "     'chirality': 0,\n",
       "     'is_in_ring': 0,\n",
       "     'ring_sizes': [],\n",
       "     'valence': 2}],\n",
       "   'bonds': [{'start_atom': 0,\n",
       "     'end_atom': 1,\n",
       "     'bond_type': 1,\n",
       "     'is_conjugated': 0,\n",
       "     'is_aromatic': 0,\n",
       "     'is_in_ring': 0,\n",
       "     'stereo': 0,\n",
       "     'ring_sizes': []},\n",
       "    {'start_atom': 1,\n",
       "     'end_atom': 2,\n",
       "     'bond_type': 1,\n",
       "     'is_conjugated': 0,\n",
       "     'is_aromatic': 0,\n",
       "     'is_in_ring': 0,\n",
       "     'stereo': 0,\n",
       "     'ring_sizes': []},\n",
       "    {'start_atom': 2,\n",
       "     'end_atom': 3,\n",
       "     'bond_type': 1,\n",
       "     'is_conjugated': 0,\n",
       "     'is_aromatic': 0,\n",
       "     'is_in_ring': 1,\n",
       "     'stereo': 0,\n",
       "     'ring_sizes': [6]},\n",
       "    {'start_atom': 3,\n",
       "     'end_atom': 4,\n",
       "     'bond_type': 1,\n",
       "     'is_conjugated': 0,\n",
       "     'is_aromatic': 0,\n",
       "     'is_in_ring': 1,\n",
       "     'stereo': 0,\n",
       "     'ring_sizes': [6]},\n",
       "    {'start_atom': 4,\n",
       "     'end_atom': 5,\n",
       "     'bond_type': 1,\n",
       "     'is_conjugated': 0,\n",
       "     'is_aromatic': 0,\n",
       "     'is_in_ring': 0,\n",
       "     'stereo': 0,\n",
       "     'ring_sizes': []},\n",
       "    {'start_atom': 5,\n",
       "     'end_atom': 6,\n",
       "     'bond_type': 1,\n",
       "     'is_conjugated': 1,\n",
       "     'is_aromatic': 0,\n",
       "     'is_in_ring': 0,\n",
       "     'stereo': 0,\n",
       "     'ring_sizes': []},\n",
       "    {'start_atom': 6,\n",
       "     'end_atom': 7,\n",
       "     'bond_type': 12,\n",
       "     'is_conjugated': 1,\n",
       "     'is_aromatic': 1,\n",
       "     'is_in_ring': 1,\n",
       "     'stereo': 0,\n",
       "     'ring_sizes': [6]},\n",
       "    {'start_atom': 7,\n",
       "     'end_atom': 8,\n",
       "     'bond_type': 12,\n",
       "     'is_conjugated': 1,\n",
       "     'is_aromatic': 1,\n",
       "     'is_in_ring': 1,\n",
       "     'stereo': 0,\n",
       "     'ring_sizes': [6]},\n",
       "    {'start_atom': 8,\n",
       "     'end_atom': 9,\n",
       "     'bond_type': 12,\n",
       "     'is_conjugated': 1,\n",
       "     'is_aromatic': 1,\n",
       "     'is_in_ring': 1,\n",
       "     'stereo': 0,\n",
       "     'ring_sizes': [6]},\n",
       "    {'start_atom': 9,\n",
       "     'end_atom': 10,\n",
       "     'bond_type': 12,\n",
       "     'is_conjugated': 1,\n",
       "     'is_aromatic': 1,\n",
       "     'is_in_ring': 1,\n",
       "     'stereo': 0,\n",
       "     'ring_sizes': [6]},\n",
       "    {'start_atom': 10,\n",
       "     'end_atom': 11,\n",
       "     'bond_type': 12,\n",
       "     'is_conjugated': 1,\n",
       "     'is_aromatic': 1,\n",
       "     'is_in_ring': 1,\n",
       "     'stereo': 0,\n",
       "     'ring_sizes': [6]},\n",
       "    {'start_atom': 10,\n",
       "     'end_atom': 12,\n",
       "     'bond_type': 1,\n",
       "     'is_conjugated': 1,\n",
       "     'is_aromatic': 0,\n",
       "     'is_in_ring': 0,\n",
       "     'stereo': 0,\n",
       "     'ring_sizes': []},\n",
       "    {'start_atom': 12,\n",
       "     'end_atom': 13,\n",
       "     'bond_type': 1,\n",
       "     'is_conjugated': 1,\n",
       "     'is_aromatic': 0,\n",
       "     'is_in_ring': 0,\n",
       "     'stereo': 0,\n",
       "     'ring_sizes': []},\n",
       "    {'start_atom': 12,\n",
       "     'end_atom': 14,\n",
       "     'bond_type': 2,\n",
       "     'is_conjugated': 1,\n",
       "     'is_aromatic': 0,\n",
       "     'is_in_ring': 0,\n",
       "     'stereo': 0,\n",
       "     'ring_sizes': []},\n",
       "    {'start_atom': 4,\n",
       "     'end_atom': 15,\n",
       "     'bond_type': 1,\n",
       "     'is_conjugated': 0,\n",
       "     'is_aromatic': 0,\n",
       "     'is_in_ring': 1,\n",
       "     'stereo': 0,\n",
       "     'ring_sizes': [6]},\n",
       "    {'start_atom': 15,\n",
       "     'end_atom': 16,\n",
       "     'bond_type': 1,\n",
       "     'is_conjugated': 0,\n",
       "     'is_aromatic': 0,\n",
       "     'is_in_ring': 1,\n",
       "     'stereo': 0,\n",
       "     'ring_sizes': [6]},\n",
       "    {'start_atom': 16,\n",
       "     'end_atom': 17,\n",
       "     'bond_type': 1,\n",
       "     'is_conjugated': 0,\n",
       "     'is_aromatic': 0,\n",
       "     'is_in_ring': 1,\n",
       "     'stereo': 0,\n",
       "     'ring_sizes': [6]},\n",
       "    {'start_atom': 17,\n",
       "     'end_atom': 18,\n",
       "     'bond_type': 1,\n",
       "     'is_conjugated': 0,\n",
       "     'is_aromatic': 0,\n",
       "     'is_in_ring': 0,\n",
       "     'stereo': 0,\n",
       "     'ring_sizes': []},\n",
       "    {'start_atom': 16,\n",
       "     'end_atom': 19,\n",
       "     'bond_type': 1,\n",
       "     'is_conjugated': 0,\n",
       "     'is_aromatic': 0,\n",
       "     'is_in_ring': 0,\n",
       "     'stereo': 0,\n",
       "     'ring_sizes': []},\n",
       "    {'start_atom': 15,\n",
       "     'end_atom': 20,\n",
       "     'bond_type': 1,\n",
       "     'is_conjugated': 0,\n",
       "     'is_aromatic': 0,\n",
       "     'is_in_ring': 0,\n",
       "     'stereo': 0,\n",
       "     'ring_sizes': []},\n",
       "    {'start_atom': 17,\n",
       "     'end_atom': 2,\n",
       "     'bond_type': 1,\n",
       "     'is_conjugated': 0,\n",
       "     'is_aromatic': 0,\n",
       "     'is_in_ring': 1,\n",
       "     'stereo': 0,\n",
       "     'ring_sizes': [6]},\n",
       "    {'start_atom': 11,\n",
       "     'end_atom': 6,\n",
       "     'bond_type': 12,\n",
       "     'is_conjugated': 1,\n",
       "     'is_aromatic': 1,\n",
       "     'is_in_ring': 1,\n",
       "     'stereo': 0,\n",
       "     'ring_sizes': [6]}]}}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['multi_scale']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def9ea45-c1ae-4beb-80a2-89a676375ae6",
   "metadata": {},
   "source": [
    "## Modelling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c585c18-2fec-4b94-a0e7-69304bb21e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_to_device(data, device):\n",
    "    \"\"\"\n",
    "    Recursively move all tensors in a PyTorch Geometric Data object to the device.\n",
    "    \"\"\"\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        return data.to(device)\n",
    "    elif isinstance(data, Data):\n",
    "        for key, value in data:\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                data[key] = value.to(device)\n",
    "        return data\n",
    "    elif isinstance(data, dict):\n",
    "        return {k: deep_to_device(v, device) for k, v in data.items()}\n",
    "    elif isinstance(data, list):\n",
    "        return [deep_to_device(v, device) for v in data]\n",
    "    else:\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982bed51-265e-4c1b-a614-c983e6cc4eb1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b04914a9-ae1e-4d0b-8791-84998a07144e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiScaleProteinLigandDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for protein-ligand pairs with multi-scale ligand representation\n",
    "    \"\"\"\n",
    "    def __init__(self, data_path, split='train', test_size=0.1, val_size=0.1, max_atoms=150, max_samples=None):\n",
    "        \"\"\"\n",
    "        Initialize the dataset\n",
    "        \n",
    "        Args:\n",
    "            data_path: Path to the combined dataset pickle file\n",
    "            split: One of 'train', 'val', or 'test'\n",
    "            test_size: Fraction of data to use for testing\n",
    "            val_size: Fraction of data to use for validation\n",
    "            max_atoms: Maximum number of atoms in a molecule to include\n",
    "        \"\"\"\n",
    "        self.max_atoms = max_atoms\n",
    "        \n",
    "        # Load the combined dataset\n",
    "        print(f\"Loading dataset from {data_path}\")\n",
    "        with open(data_path, 'rb') as f:\n",
    "            self.data = pickle.load(f)\n",
    "        \n",
    "        print(f\"Loaded {len(self.data)} protein-ligand pairs\")\n",
    "        \n",
    "        # Filter out molecules that are too large\n",
    "        filtered_data = []\n",
    "        for item in self.data:\n",
    "            if len(item['multi_scale']['level3_complete']['graph']['atoms']) <= max_atoms:\n",
    "                filtered_data.append(item)\n",
    "        \n",
    "        print(f\"Filtered to {len(filtered_data)} pairs with <= {max_atoms} atoms\")\n",
    "        self.data = filtered_data\n",
    "        \n",
    "        # Split the data\n",
    "        train_data, test_data = train_test_split(self.data, test_size=test_size, random_state=42)\n",
    "        train_data, val_data = train_test_split(train_data, test_size=val_size/(1-test_size), random_state=42)\n",
    "        \n",
    "        if split == 'train':\n",
    "            self.data = train_data\n",
    "        elif split == 'val':\n",
    "            self.data = val_data\n",
    "        elif split == 'test':\n",
    "            self.data = test_data\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid split: {split}\")\n",
    "        \n",
    "        print(f\"Using {len(self.data)} samples for {split}\")\n",
    "\n",
    "        if max_samples is not None and split == 'train':\n",
    "            self.data = self.data[:max_samples]\n",
    "            print(f\"Limited to {len(self.data)} samples per epoch\")\n",
    "\n",
    "        if max_samples is not None and split == 'val':\n",
    "            self.data = self.data[:int(max_samples)]\n",
    "            print(f\"Limited to {len(self.data)} samples per epoch\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a protein-ligand pair with multi-scale representation\n",
    "        \n",
    "        Returns a dictionary with:\n",
    "        - protein_embedding: Tensor of shape (L, 1280) for sequence or (1280,) for pooled\n",
    "        - affinities: Tensor of shape (1,) with binding affinity\n",
    "        - level1_graph: Graph data for scaffold (level 1)\n",
    "        - level2_graphs: List of graph data for fragments (level 2)\n",
    "        - level3_graph: Graph data for complete molecule (level 3)\n",
    "        - ligand_properties: Dictionary of molecular properties\n",
    "        - physics_features: Dictionary of physics-informed features\n",
    "        \"\"\"\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # Process protein embedding - handle both sequence and pooled embeddings\n",
    "        protein_emb = torch.tensor(item['protein_embedding'], dtype=torch.float32)\n",
    "        \n",
    "        # Process affinity\n",
    "        affinity = torch.tensor([item['affinity']], dtype=torch.float32)\n",
    "        \n",
    "        # Process multi-scale graph representation\n",
    "        level1_graph = self._process_graph_data(item['multi_scale']['level1_scaffold']['graph'])\n",
    "        \n",
    "        # Process level 2 fragments\n",
    "        level2_graphs = []\n",
    "        for frag in item['multi_scale']['level2_fragments']:\n",
    "            level2_graphs.append(self._process_graph_data(frag['graph']))\n",
    "        \n",
    "        # Process level 3 complete molecule\n",
    "        level3_graph = self._process_graph_data(item['multi_scale']['level3_complete']['graph'])\n",
    "        \n",
    "        # Package properties and physics features\n",
    "        ligand_properties = {k: torch.tensor([v], dtype=torch.float32) \n",
    "                           for k, v in item['properties'].items() \n",
    "                           if isinstance(v, (int, float))}\n",
    "        \n",
    "        physics_features = {k: torch.tensor([v], dtype=torch.float32) \n",
    "                          for k, v in item['physics_features'].items() \n",
    "                          if isinstance(v, (int, float))}\n",
    "        \n",
    "        assert level1_graph.x.size(1) == 8, f\"Level 1 graph has {level1_graph.x.size(1)} features instead of 8\"\n",
    "        assert level3_graph.x.size(1) == 8, f\"Level 3 graph has {level3_graph.x.size(1)} features instead of 8\"\n",
    "        \n",
    "        for i, frag in enumerate(level2_graphs):\n",
    "            assert frag.x.size(1) == 8, f\"Level 2 fragment {i} has {frag.x.size(1)} features instead of 8\"\n",
    "        \n",
    "        return {\n",
    "            'protein_embedding': protein_emb,\n",
    "            'affinity': affinity,\n",
    "            'level1_graph': level1_graph,\n",
    "            'level2_graphs': level2_graphs,\n",
    "            'level3_graph': level3_graph,\n",
    "            'ligand_properties': ligand_properties,\n",
    "            'physics_features': physics_features,\n",
    "            'smiles': item['ligand_smiles']\n",
    "        }\n",
    "\n",
    "    def _process_graph_data(self, graph_dict):\n",
    "        \"\"\"Convert the graph dictionary to PyTorch Geometric Data object with atom indices\"\"\"\n",
    "        # Define standard node feature dimension\n",
    "        NODE_FEATURE_DIM = 8\n",
    "        \n",
    "        if not graph_dict['atoms'] or not graph_dict['bonds']:\n",
    "            # Handle empty graphs with standard dimensions\n",
    "            return Data(\n",
    "                x=torch.zeros((1, NODE_FEATURE_DIM), dtype=torch.float32),\n",
    "                atom_types=torch.zeros(1, dtype=torch.long),\n",
    "                edge_index=torch.zeros((2, 1), dtype=torch.long),\n",
    "                edge_attr=torch.zeros((1, 5), dtype=torch.float32)\n",
    "            )\n",
    "\n",
    "        atoms = graph_dict['atoms']\n",
    "        bonds = graph_dict['bonds']\n",
    "\n",
    "        # Node features - strictly limited to 8 features\n",
    "        node_features = []\n",
    "        atom_types = []\n",
    "\n",
    "        for atom in atoms:\n",
    "            # Store atomic number separately\n",
    "            atom_types.append(min(atom['atomic_num'], 118))\n",
    "            \n",
    "            # IMPORTANT: Exactly 8 features, no more, no less\n",
    "            features = [\n",
    "                atom['formal_charge'],\n",
    "                atom['hybridization'],\n",
    "                atom['aromatic'],\n",
    "                atom['num_h'],\n",
    "                atom['degree'],\n",
    "                atom['chirality'],\n",
    "                atom['is_in_ring'],\n",
    "                atom['valence']\n",
    "            ]\n",
    "            node_features.append(features)\n",
    "\n",
    "    \n",
    "        # Process edges (unchanged)\n",
    "        edge_indices = []\n",
    "        edge_features = []\n",
    "    \n",
    "        for bond in bonds:\n",
    "            start, end = bond['start_atom'], bond['end_atom']\n",
    "            edge_indices.append([start, end])\n",
    "            edge_indices.append([end, start])\n",
    "            \n",
    "            edge_feat = [\n",
    "                bond['bond_type'],\n",
    "                bond['is_conjugated'],\n",
    "                bond['is_aromatic'],\n",
    "                bond['is_in_ring'],\n",
    "                bond['stereo']\n",
    "            ]\n",
    "            edge_features.append(edge_feat)\n",
    "            edge_features.append(edge_feat)\n",
    "    \n",
    "        x = torch.tensor(node_features, dtype=torch.float32)\n",
    "        atom_types = torch.tensor(atom_types, dtype=torch.long)\n",
    "        edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n",
    "        edge_attr = torch.tensor(edge_features, dtype=torch.float32)\n",
    "\n",
    "        # Double-check feature dimensions\n",
    "        assert x.size(1) == NODE_FEATURE_DIM, f\"Node features should have dimension {NODE_FEATURE_DIM}, got {x.size(1)}\"\n",
    "\n",
    "        return Data(x=x, atom_types=atom_types, edge_index=edge_index, edge_attr=edge_attr)\n",
    "    \n",
    "    \n",
    "# Custom collate function for batching the data with variable-sized level2 graphs\n",
    "def collate_multi_scale_data(batch):\n",
    "    \"\"\"Collate function for the multi-scale dataset\"\"\"\n",
    "    protein_embs = []\n",
    "    affinities = []\n",
    "    level1_graphs = []\n",
    "    level2_graphs_lists = []\n",
    "    level3_graphs = []\n",
    "    ligand_properties_list = []\n",
    "    physics_features_list = []\n",
    "    smiles_list = []\n",
    "\n",
    "    for item in batch:\n",
    "        protein_embs.append(item['protein_embedding'])\n",
    "        affinities.append(item['affinity'])\n",
    "        level1_graphs.append(item['level1_graph'])\n",
    "        level2_graphs_lists.append(item['level2_graphs'])\n",
    "        level3_graphs.append(item['level3_graph'])\n",
    "        ligand_properties_list.append(item['ligand_properties'])\n",
    "        physics_features_list.append(item['physics_features'])\n",
    "        smiles_list.append(item['smiles'])\n",
    "\n",
    "    # Function to ensure tensor is independent and properly sized\n",
    "    def ensure_proper_tensor(graph, target_dim=8):\n",
    "        # Clone all tensors to ensure independent storage\n",
    "        if hasattr(graph, 'x') and graph.x is not None:\n",
    "            # Fix dimensions if needed\n",
    "            if graph.x.dim() > 1 and graph.x.size(1) != target_dim:\n",
    "                if graph.x.size(1) > target_dim:\n",
    "                    graph.x = graph.x[:, :target_dim].clone()  # Truncate and clone\n",
    "                else:\n",
    "                    # Pad with zeros and clone\n",
    "                    pad = torch.zeros(graph.x.size(0), target_dim - graph.x.size(1), \n",
    "                                     dtype=graph.x.dtype, device=graph.x.device)\n",
    "                    graph.x = torch.cat([graph.x, pad], dim=1).clone()\n",
    "            else:\n",
    "                graph.x = graph.x.clone()  # Just clone\n",
    "                \n",
    "        if hasattr(graph, 'atom_types') and graph.atom_types is not None:\n",
    "            graph.atom_types = graph.atom_types.clone()\n",
    "            \n",
    "        if hasattr(graph, 'edge_index') and graph.edge_index is not None:\n",
    "            graph.edge_index = graph.edge_index.clone()\n",
    "            \n",
    "        if hasattr(graph, 'edge_attr') and graph.edge_attr is not None:\n",
    "            graph.edge_attr = graph.edge_attr.clone()\n",
    "            \n",
    "        return graph\n",
    "\n",
    "    # Process level1 graphs\n",
    "    for i in range(len(level1_graphs)):\n",
    "        level1_graphs[i] = ensure_proper_tensor(level1_graphs[i])\n",
    "        \n",
    "    # Process level3 graphs\n",
    "    for i in range(len(level3_graphs)):\n",
    "        level3_graphs[i] = ensure_proper_tensor(level3_graphs[i])\n",
    "        \n",
    "    # Process level2 fragments\n",
    "    for i in range(len(level2_graphs_lists)):\n",
    "        for j in range(len(level2_graphs_lists[i])):\n",
    "            level2_graphs_lists[i][j] = ensure_proper_tensor(level2_graphs_lists[i][j])\n",
    "\n",
    "    # Rest of your collation function...\n",
    "    # Batch the protein embeddings - handle both sequence and pooled\n",
    "    if protein_embs[0].dim() == 1:\n",
    "        # Pooled embeddings\n",
    "        protein_embs_batch = torch.stack(protein_embs)\n",
    "    else:\n",
    "        # Sequence embeddings - pad to the same length\n",
    "        max_len = max(emb.size(0) for emb in protein_embs)\n",
    "        padded_embs = []\n",
    "        for emb in protein_embs:\n",
    "            pad_len = max_len - emb.size(0)\n",
    "            if pad_len > 0:\n",
    "                padded_emb = F.pad(emb, (0, 0, 0, pad_len))\n",
    "            else:\n",
    "                padded_emb = emb\n",
    "            padded_embs.append(padded_emb)\n",
    "        protein_embs_batch = torch.stack(padded_embs)\n",
    "\n",
    "    # Batch the affinities\n",
    "    affinities_batch = torch.cat(affinities)\n",
    "\n",
    "    # Create batches with proper error handling\n",
    "    try:\n",
    "        level1_batch = Batch.from_data_list(level1_graphs)\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating level1 batch: {e}\")\n",
    "        raise  # Re-raise to see the full error\n",
    "        \n",
    "    try:\n",
    "        level3_batch = Batch.from_data_list(level3_graphs)\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating level3 batch: {e}\")\n",
    "        raise  # Re-raise to see the full error\n",
    "\n",
    "    # For level2, we have varying numbers of fragments per molecule\n",
    "    # We'll keep them as lists of batches for each fragment position\n",
    "    max_fragments = max(len(frags) for frags in level2_graphs_lists)\n",
    "    level2_batches = []\n",
    "\n",
    "    for frag_idx in range(max_fragments):\n",
    "        frag_graphs = []\n",
    "        for mol_frags in level2_graphs_lists:\n",
    "            if frag_idx < len(mol_frags):\n",
    "                frag_graphs.append(mol_frags[frag_idx])\n",
    "            else:\n",
    "                # Create completely new tensor for dummy graph\n",
    "                dummy_graph = Data(\n",
    "                    x=torch.zeros((1, 8), dtype=torch.float32),\n",
    "                    atom_types=torch.zeros(1, dtype=torch.long),\n",
    "                    edge_index=torch.zeros((2, 1), dtype=torch.long),\n",
    "                    edge_attr=torch.zeros((1, 5), dtype=torch.float32)\n",
    "                )\n",
    "                frag_graphs.append(dummy_graph)\n",
    "                \n",
    "        # Verbose error checking for level2 batching\n",
    "        try:\n",
    "            level2_batches.append(Batch.from_data_list(frag_graphs))\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating batch for fragment {frag_idx}:\")\n",
    "            for i, fg in enumerate(frag_graphs):\n",
    "                print(f\"  Graph {i} x shape: {fg.x.shape if hasattr(fg, 'x') else 'None'}\")\n",
    "            raise  # Re-raise to see the full error\n",
    "\n",
    "    # Combine the properties and physics features\n",
    "    combined_properties = {}\n",
    "    for props in ligand_properties_list:\n",
    "        for k, v in props.items():\n",
    "            if k not in combined_properties:\n",
    "                combined_properties[k] = []\n",
    "            combined_properties[k].append(v)\n",
    "\n",
    "    batched_properties = {k: torch.cat(v) for k, v in combined_properties.items()}\n",
    "\n",
    "    combined_physics = {}\n",
    "    for props in physics_features_list:\n",
    "        for k, v in props.items():\n",
    "            if k not in combined_physics:\n",
    "                combined_physics[k] = []\n",
    "            combined_physics[k].append(v)\n",
    "\n",
    "    batched_physics = {k: torch.cat(v) for k, v in combined_physics.items()}\n",
    "\n",
    "    return_dict = {\n",
    "        'protein_embedding': protein_embs_batch,\n",
    "        'affinity': affinities_batch,\n",
    "        'level1_graph': level1_batch,\n",
    "        'level2_graphs': level2_batches,\n",
    "        'level3_graph': level3_batch,\n",
    "        'ligand_properties': batched_properties,\n",
    "        'physics_features': batched_physics,\n",
    "        'smiles': smiles_list\n",
    "    }\n",
    "    \n",
    "    # Make sure all tensors are contiguous for better CUDA performance\n",
    "    for key, value in return_dict.items():\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            return_dict[key] = value.contiguous()\n",
    "        elif isinstance(value, list) and all(isinstance(g, Data) for g in value):\n",
    "            for i, g in enumerate(value):\n",
    "                for k, v in g:\n",
    "                    if isinstance(v, torch.Tensor):\n",
    "                        g[k] = v.contiguous()\n",
    "                value[i] = g\n",
    "    \n",
    "    return return_dict\n",
    "# Create data loaders\n",
    "def create_data_loaders(data_path, batch_size=32, num_workers=4, max_atoms=150, max_samples_per_epoch= None):\n",
    "    \"\"\"Create train, validation, and test data loaders\"\"\"\n",
    "    train_dataset = MultiScaleProteinLigandDataset(\n",
    "        data_path=data_path, split='train', max_atoms=max_atoms, max_samples= max_samples_per_epoch)\n",
    "    \n",
    "    val_dataset = MultiScaleProteinLigandDataset(\n",
    "        data_path=data_path, split='val', max_atoms=max_atoms, max_samples= max_samples_per_epoch*0.1)\n",
    "    \n",
    "    test_dataset = MultiScaleProteinLigandDataset(\n",
    "        data_path=data_path, split='test', max_atoms=max_atoms)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, \n",
    "        num_workers=num_workers, collate_fn=collate_multi_scale_data)\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False, \n",
    "        num_workers=num_workers, collate_fn=collate_multi_scale_data)\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=batch_size, shuffle=False,\n",
    "        num_workers=num_workers, collate_fn=collate_multi_scale_data)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5566a73b-44d1-400e-b44b-0bb891552e0a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Protein Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "11c32152-09ff-4997-9604-d0887bfd7154",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder for protein sequences to create conditioning vectors for the diffusion model\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=1280, hidden_dim=512, output_dim=256, n_layers=3, \n",
    "                 dropout=0.1, use_attention=True):\n",
    "        super(ProteinEncoder, self).__init__()\n",
    "        \n",
    "        self.use_attention = use_attention\n",
    "        \n",
    "        # Initial projection\n",
    "        self.input_proj = nn.Linear(input_dim, hidden_dim)\n",
    "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "        # Sequence processing\n",
    "        if use_attention:\n",
    "            self.attention_layers = nn.ModuleList([\n",
    "                nn.MultiheadAttention(hidden_dim, num_heads=8, dropout=dropout)\n",
    "                for _ in range(n_layers)\n",
    "            ])\n",
    "            self.attention_norms = nn.ModuleList([\n",
    "                nn.LayerNorm(hidden_dim)\n",
    "                for _ in range(n_layers)\n",
    "            ])\n",
    "        else:\n",
    "            self.lstm = nn.LSTM(\n",
    "                hidden_dim, hidden_dim, n_layers, \n",
    "                dropout=dropout, bidirectional=True, batch_first=True\n",
    "            )\n",
    "        \n",
    "        # Output projections for each level in the multi-scale representation\n",
    "        self.level1_output = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * (2 if not use_attention else 1), output_dim),\n",
    "            nn.LayerNorm(output_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(output_dim, output_dim)\n",
    "        )\n",
    "        \n",
    "        self.level2_output = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * (2 if not use_attention else 1), output_dim),\n",
    "            nn.LayerNorm(output_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(output_dim, output_dim)\n",
    "        )\n",
    "        \n",
    "        self.level3_output = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * (2 if not use_attention else 1), output_dim),\n",
    "            nn.LayerNorm(output_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(output_dim, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, padding_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass through the protein encoder\n",
    "        \n",
    "        Args:\n",
    "            x: Protein embeddings of shape (batch_size, seq_len, input_dim)\n",
    "               or (batch_size, input_dim) for pooled embeddings\n",
    "            padding_mask: Mask for padded sequence positions\n",
    "        \n",
    "        Returns:\n",
    "            level1_cond, level2_cond, level3_cond: Conditioning vectors for each level\n",
    "        \"\"\"\n",
    "        # Handle pooled embeddings (single vector per protein)\n",
    "        if x.dim() == 2:\n",
    "            # If we have pooled embeddings, expand dims to create a length-1 sequence\n",
    "            x = x.unsqueeze(1)\n",
    "            is_pooled = True\n",
    "        else:\n",
    "            is_pooled = False\n",
    "        \n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Initial projection\n",
    "        x = self.input_proj(x)\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        # Sequence processing\n",
    "        if self.use_attention:\n",
    "            # Create attention mask from padding mask\n",
    "            if padding_mask is not None:\n",
    "                attn_mask = padding_mask.repeat(8, 1, 1)  # For 8 attention heads\n",
    "            else:\n",
    "                attn_mask = None\n",
    "            \n",
    "            # Process with self-attention layers\n",
    "            x_t = x.transpose(0, 1)  # (seq_len, batch_size, hidden_dim)\n",
    "            \n",
    "            for attn_layer, norm_layer in zip(self.attention_layers, self.attention_norms):\n",
    "                residual = x_t\n",
    "                x_t, _ = attn_layer(x_t, x_t, x_t, key_padding_mask=padding_mask)\n",
    "                x_t = norm_layer(x_t + residual)\n",
    "            \n",
    "            x = x_t.transpose(0, 1)  # (batch_size, seq_len, hidden_dim)\n",
    "            \n",
    "            # Global pooling for sequence-level representation\n",
    "            if padding_mask is not None:\n",
    "                # Masked mean pooling\n",
    "                mask_expanded = padding_mask.unsqueeze(-1).float()\n",
    "                sum_embeddings = torch.sum(x * (1 - mask_expanded), dim=1)\n",
    "                sum_mask = torch.sum(1 - mask_expanded, dim=1)\n",
    "                pooled_output = sum_embeddings / (sum_mask + 1e-10)\n",
    "            else:\n",
    "                # Regular mean pooling\n",
    "                pooled_output = torch.mean(x, dim=1)\n",
    "        else:\n",
    "            Print(\"Define encoding method first\")\n",
    "        return level1_cond, level2_cond, level3_cond"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945f2a76-7d89-4f08-9972-faa2de289f2b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Graph Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba28aeed-8651-45bb-b326-27d8640c5212",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphEncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic building block for graph encoding with message passing and attention\n",
    "    \"\"\"\n",
    "    def __init__(self, node_dim, edge_dim, hidden_dim, out_dim, heads=4, dropout=0.1, device= None):\n",
    "        super(GraphEncoderBlock, self).__init__()\n",
    "\n",
    "        self.device = device if device is not None else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        # Node and edge feature projections\n",
    "        self.node_proj = nn.Sequential(\n",
    "            nn.Linear(node_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        self.edge_proj = nn.Sequential(\n",
    "            nn.Linear(edge_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Message passing layers\n",
    "        self.conv1 = GATv2Conv(hidden_dim, hidden_dim // heads, heads=heads, \n",
    "                             edge_dim=hidden_dim, dropout=dropout)\n",
    "        self.conv2 = GATv2Conv(hidden_dim, hidden_dim // heads, heads=heads, \n",
    "                             edge_dim=hidden_dim, dropout=dropout)\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_proj = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, out_dim),\n",
    "            nn.LayerNorm(out_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Residual connections\n",
    "        self.residual1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.residual2 = nn.Linear(hidden_dim, out_dim)\n",
    "        \n",
    "        # Layer norms\n",
    "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
    "        self.norm2 = nn.LayerNorm(out_dim)\n",
    "    \n",
    "    def forward(self, x, edge_index, edge_attr, batch=None):\n",
    "        \"\"\"\n",
    "        Forward pass through graph encoder block\n",
    "        \n",
    "        Args:\n",
    "            x: Node features [num_nodes, node_dim]\n",
    "            edge_index: Edge indices [2, num_edges]\n",
    "            edge_attr: Edge features [num_edges, edge_dim]\n",
    "            batch: Batch assignment for nodes [num_nodes]\n",
    "        \n",
    "        Returns:\n",
    "            Updated node features [num_nodes, out_dim]\n",
    "        \"\"\"\n",
    "        # Project features\n",
    "        h_node = self.node_proj(x)\n",
    "        h_edge = self.edge_proj(edge_attr)\n",
    "        \n",
    "        # First message passing\n",
    "        h_residual = self.residual1(h_node)\n",
    "        h = self.conv1(h_node, edge_index, edge_attr=h_edge)\n",
    "        h = self.norm1(h + h_residual)\n",
    "        \n",
    "        # Second message passing\n",
    "        h_residual = self.residual2(h)\n",
    "        h = self.conv2(h, edge_index, edge_attr=h_edge)\n",
    "        h = self.output_proj(h)\n",
    "        \n",
    "        # Final residual connection and normalization\n",
    "        h = self.norm2(h + h_residual)\n",
    "        \n",
    "        return h\n",
    "\n",
    "class ConditionalGraphBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph neural network block that incorporates conditioning information\n",
    "    \"\"\"\n",
    "    def __init__(self, node_dim, edge_dim, cond_dim, hidden_dim, out_dim, heads=4, dropout=0.1):\n",
    "        super(ConditionalGraphBlock, self).__init__()\n",
    "        \n",
    "        # Conditioning projection\n",
    "        self.cond_proj = nn.Sequential(\n",
    "            nn.Linear(cond_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Base graph encoder\n",
    "        self.graph_encoder = GraphEncoderBlock(\n",
    "            node_dim + hidden_dim,  # Add conditioning dim to node features\n",
    "            edge_dim, \n",
    "            hidden_dim, \n",
    "            out_dim,\n",
    "            heads=heads,\n",
    "            dropout=dropout\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, edge_index, edge_attr, condition, batch):\n",
    "        \"\"\"\n",
    "        Forward pass with conditioning\n",
    "        \n",
    "        Args:\n",
    "            x: Node features [num_nodes, node_dim]\n",
    "            edge_index: Edge indices [2, num_edges]\n",
    "            edge_attr: Edge features [num_edges, edge_dim]\n",
    "            condition: Conditioning vector [batch_size, cond_dim]\n",
    "            batch: Batch assignment for nodes [num_nodes]\n",
    "        \n",
    "        Returns:\n",
    "            Updated node features [num_nodes, out_dim]\n",
    "        \"\"\"\n",
    "        # Project condition\n",
    "        h_cond = self.cond_proj(condition)\n",
    "        \n",
    "        # Expand condition to match nodes\n",
    "        expanded_cond = h_cond[batch]  # [num_nodes, hidden_dim]\n",
    "        \n",
    "        # Concatenate node features with condition\n",
    "        h_node = torch.cat([x, expanded_cond], dim=1)\n",
    "        \n",
    "        # Forward through graph encoder\n",
    "        h = self.graph_encoder(h_node, edge_index, edge_attr, batch)\n",
    "        \n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910aa32c-b381-40bd-9e96-15ae430d2306",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Graph Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ffb773e-c2b4-4e97-9c5a-eb4e2046f3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphDiffusion(nn.Module):\n",
    "    \"\"\"\n",
    "    Base diffusion model for graph-based data\n",
    "    \"\"\"\n",
    "    def __init__(self, node_dim, edge_dim, hidden_dim=128, num_steps=1000, beta_schedule='linear', device=None):\n",
    "        super(GraphDiffusion, self).__init__()\n",
    "\n",
    "        self.device = device if device is not None else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.num_steps = num_steps\n",
    "        self.node_dim = node_dim\n",
    "        self.edge_dim = edge_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Set up noise schedule\n",
    "        if beta_schedule == 'linear':\n",
    "            beta = self._linear_beta_schedule(num_steps)\n",
    "        elif beta_schedule == 'cosine':\n",
    "            beta = self._cosine_beta_schedule(num_steps)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown beta schedule: {beta_schedule}\")\n",
    "\n",
    "        # Move to device before registering\n",
    "        beta = beta.to(self.device)\n",
    "        alpha = 1 - beta\n",
    "        alpha_bar = torch.cumprod(alpha, dim=0)\n",
    "\n",
    "        # Register buffers\n",
    "        self.register_buffer('beta', beta)\n",
    "        self.register_buffer('alpha', alpha)\n",
    "        self.register_buffer('alpha_bar', alpha_bar)\n",
    "        self.register_buffer('sqrt_alpha_bar', torch.sqrt(alpha_bar))\n",
    "        self.register_buffer('sqrt_one_minus_alpha_bar', torch.sqrt(1 - alpha_bar))\n",
    "\n",
    "    def _linear_beta_schedule(self, num_steps):\n",
    "        beta_start = 1e-4\n",
    "        beta_end = 2e-2\n",
    "        return torch.linspace(beta_start, beta_end, num_steps)\n",
    "\n",
    "    def _cosine_beta_schedule(self, num_steps):\n",
    "        steps = torch.linspace(0, num_steps, num_steps + 1)\n",
    "        x = steps / num_steps\n",
    "        alphas_cumprod = torch.cos((x + 0.008) / 1.008 * torch.pi / 2) ** 2\n",
    "        alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "        betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "        return torch.clip(betas, 0.0001, 0.9999)\n",
    "\n",
    "    def forward_diffusion(self, x_0, t, batch=None, device=None):\n",
    "        \"\"\"\n",
    "        Perform forward diffusion process\n",
    "        \"\"\"\n",
    "        if device is None:\n",
    "            device = self.device\n",
    "        \n",
    "        # Ensure all input tensors are on the correct device\n",
    "        x_0 = x_0.to(device)\n",
    "        t = t.to(device)\n",
    "        if batch is not None:\n",
    "            batch = batch.to(device)\n",
    "        \n",
    "        # Create noise on the same device\n",
    "        noise = torch.randn_like(x_0, device=device)\n",
    "        \n",
    "        # Get diffusion parameters\n",
    "        sqrt_alpha_bar_t = self.sqrt_alpha_bar[t].to(device)\n",
    "        sqrt_one_minus_alpha_bar_t = self.sqrt_one_minus_alpha_bar[t].to(device)\n",
    "        \n",
    "        if batch is not None:\n",
    "            # Map parameters to each node based on batch assignment\n",
    "            sqrt_alpha_bar_t = sqrt_alpha_bar_t[batch]\n",
    "            sqrt_one_minus_alpha_bar_t = sqrt_one_minus_alpha_bar_t[batch]\n",
    "        \n",
    "        # Reshape for broadcasting\n",
    "        sqrt_alpha_bar_t = sqrt_alpha_bar_t.view(-1, 1)\n",
    "        sqrt_one_minus_alpha_bar_t = sqrt_one_minus_alpha_bar_t.view(-1, 1)\n",
    "        \n",
    "        # Perform forward diffusion\n",
    "        x_t = sqrt_alpha_bar_t * x_0 + sqrt_one_minus_alpha_bar_t * noise\n",
    "        \n",
    "        return x_t, noise\n",
    "class GraphDenoiser(nn.Module):\n",
    "    \"\"\"\n",
    "    Denoising network for graph diffusion model\n",
    "    \"\"\"\n",
    "    def __init__(self, node_dim, edge_dim, cond_dim, hidden_dim=128, out_dim=None, \n",
    "                time_emb_dim=128, num_layers=3, heads=4, dropout=0.1, device=None):\n",
    "        super(GraphDenoiser, self).__init__()\n",
    "\n",
    "        self.device = device if device is not None else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.out_dim = out_dim if out_dim is not None else node_dim\n",
    "\n",
    "        # Timestep embedding\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPositionEmbeddings(time_emb_dim),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim),\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Combine time and conditioning\n",
    "        self.cond_with_time = nn.Sequential(\n",
    "            nn.Linear(cond_dim + time_emb_dim, cond_dim),\n",
    "            nn.LayerNorm(cond_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Stacked conditional graph blocks\n",
    "        self.layers = nn.ModuleList([\n",
    "            ConditionalGraphBlock(\n",
    "                node_dim=(node_dim if i == 0 else hidden_dim),\n",
    "                edge_dim=edge_dim,\n",
    "                cond_dim=cond_dim,\n",
    "                hidden_dim=hidden_dim,\n",
    "                out_dim=hidden_dim,\n",
    "                heads=heads,\n",
    "                dropout=dropout\n",
    "            ).to(self.device)\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # Final output projection\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, self.out_dim)\n",
    "        ).to(self.device)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, t, condition, batch):\n",
    "        \"\"\"\n",
    "        Denoise graph data\n",
    "        \"\"\"\n",
    "        # Ensure inputs are on the correct device\n",
    "        x = x.to(self.device)\n",
    "        edge_index = edge_index.to(self.device)\n",
    "        edge_attr = edge_attr.to(self.device)\n",
    "        t = t.to(self.device)\n",
    "        condition = condition.to(self.device)\n",
    "        batch = batch.to(self.device)\n",
    "        \n",
    "        # Embed timestep\n",
    "        t_emb = self.time_mlp(t)  # [batch_size, time_emb_dim]\n",
    "\n",
    "        # Combine condition with time\n",
    "        cond_t = torch.cat([condition, t_emb], dim=1)\n",
    "        cond_t = self.cond_with_time(cond_t)\n",
    "\n",
    "        # Process through layers\n",
    "        h = x\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, edge_index, edge_attr, cond_t, batch)\n",
    "\n",
    "        # Final output prediction\n",
    "        noise_pred = self.output(h)\n",
    "\n",
    "        return noise_pred\n",
    "\n",
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    \"\"\"Sinusoidal positional embeddings for timestep encoding\"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "        \"\"\"Forward pass for sinusoidal embeddings\"\"\"\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((torch.sin(embeddings), torch.cos(embeddings)), dim=-1)\n",
    "        \n",
    "        # Zero-pad if dim is odd\n",
    "        if self.dim % 2 == 1:\n",
    "            embeddings = F.pad(embeddings, (0, 1, 0, 0))\n",
    "        \n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7740f2-1808-41a0-8c1f-d339f04d8584",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7cdff2fa-26ae-40da-9171-cb4d717754a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiScaleGraphDiffusion(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-scale graph diffusion model with three connected diffusion processes:\n",
    "    1. Level 1: Scaffold generation\n",
    "    2. Level 2: Functional group attachment\n",
    "    3. Level 3: Atom-level refinement\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, node_dim=8, edge_dim=5, protein_dim=1280, hidden_dim=256, \n",
    "                cond_dim=128, time_emb_dim=64, num_steps=1000, beta_schedule='linear',\n",
    "                num_layers=3, heads=4, dropout=0.1, atom_embed_dim=32, device=None):\n",
    "        super(MultiScaleGraphDiffusion, self).__init__()\n",
    "\n",
    "        self.device = device if device is not None else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # New atom embedding layer\n",
    "        self.atom_embedding = nn.Embedding(119, atom_embed_dim).to(self.device)  # 118 elements + padding\n",
    "        \n",
    "        # Adjusted node dimension to include atom embedding\n",
    "        self.full_node_dim = node_dim + atom_embed_dim\n",
    "\n",
    "        # Protein encoder\n",
    "        self.protein_encoder = ProteinEncoder(\n",
    "            input_dim=protein_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            output_dim=cond_dim,\n",
    "            n_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            use_attention=True\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Level 1: Scaffold diffusion\n",
    "        self.level1_diffusion = GraphDiffusion(\n",
    "            node_dim=self.full_node_dim,  \n",
    "            edge_dim=edge_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_steps=num_steps,\n",
    "            beta_schedule=beta_schedule,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.level1_denoiser = GraphDenoiser(\n",
    "            node_dim=self.full_node_dim,  \n",
    "            edge_dim=edge_dim,\n",
    "            cond_dim=cond_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            out_dim=self.full_node_dim, \n",
    "            time_emb_dim=time_emb_dim,\n",
    "            num_layers=num_layers,\n",
    "            heads=heads,\n",
    "            dropout=dropout,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        # Level 2: Fragment diffusion\n",
    "        self.level2_diffusion = GraphDiffusion(\n",
    "            node_dim=self.full_node_dim,  \n",
    "            edge_dim=edge_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_steps=num_steps,\n",
    "            beta_schedule=beta_schedule,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.level2_denoiser = GraphDenoiser(\n",
    "            node_dim=self.full_node_dim + cond_dim,  # Include scaffold information\n",
    "            edge_dim=edge_dim,\n",
    "            cond_dim=cond_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            out_dim=self.full_node_dim,  \n",
    "            time_emb_dim=time_emb_dim,\n",
    "            num_layers=num_layers,\n",
    "            heads=heads,\n",
    "            dropout=dropout,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        # Level 3: Complete molecule diffusion\n",
    "        self.level3_diffusion = GraphDiffusion(\n",
    "            node_dim=self.full_node_dim,  \n",
    "            edge_dim=edge_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_steps=num_steps,\n",
    "            beta_schedule=beta_schedule,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.level3_denoiser = GraphDenoiser(\n",
    "            node_dim=self.full_node_dim + cond_dim,  # Include scaffold and fragment information\n",
    "            edge_dim=edge_dim,\n",
    "            cond_dim=cond_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            out_dim=self.full_node_dim,  \n",
    "            time_emb_dim=time_emb_dim,\n",
    "            num_layers=num_layers,\n",
    "            heads=heads,\n",
    "            dropout=dropout,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        # Scaffold encoder for conditioning level 2 and 3\n",
    "        self.scaffold_encoder = GraphEncoderBlock(\n",
    "            node_dim=self.full_node_dim,  \n",
    "            edge_dim=edge_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            out_dim=cond_dim,\n",
    "            heads=heads,\n",
    "            dropout=dropout,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        # Fragment encoder for conditioning level 3\n",
    "        self.fragment_encoder = GraphEncoderBlock(\n",
    "            node_dim=self.full_node_dim,  \n",
    "            edge_dim=edge_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            out_dim=cond_dim,\n",
    "            heads=heads,\n",
    "            dropout=dropout,\n",
    "            device=self.device\n",
    "        )\n",
    "        \n",
    "        # Molecule encoder for affinity prediction\n",
    "        self.molecule_encoder = GraphEncoderBlock(\n",
    "            node_dim=self.full_node_dim, \n",
    "            edge_dim=edge_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            out_dim=cond_dim,\n",
    "            heads=heads,\n",
    "            dropout=dropout,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.affinity_predictor = nn.Sequential(\n",
    "            nn.Linear(cond_dim * 3, hidden_dim),  \n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 1) \n",
    "        ).to(self.device)\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.num_steps = num_steps\n",
    "        self.node_dim = node_dim\n",
    "        self.full_node_dim = self.full_node_dim\n",
    "        self.edge_dim = edge_dim\n",
    "        self.cond_dim = cond_dim\n",
    "        self.atom_embed_dim = atom_embed_dim\n",
    "    \n",
    "    def combine_node_features(self, x, atom_types):\n",
    "        \"\"\"Helper method to combine continuous features with atom embeddings\"\"\"\n",
    "        atom_embeds = self.atom_embedding(atom_types)\n",
    "        return torch.cat([x, atom_embeds], dim=1)\n",
    "        \n",
    "    def extract_scaffold_features(self, level1_graph):\n",
    "        \"\"\"Extract scaffold features for conditioning with atom embeddings\"\"\"\n",
    "        # Combine node features with atom embeddings\n",
    "        combined_features = self.combine_node_features(\n",
    "            level1_graph.x,\n",
    "            level1_graph.atom_types\n",
    "        )\n",
    "        \n",
    "        scaffold_feat = self.scaffold_encoder(\n",
    "            combined_features, level1_graph.edge_index, \n",
    "            level1_graph.edge_attr, level1_graph.batch\n",
    "        )\n",
    "\n",
    "        # Global pooling to get one vector per scaffold\n",
    "        scaffold_feat = torch_scatter.scatter_mean(\n",
    "            scaffold_feat, level1_graph.batch, dim=0\n",
    "        )\n",
    "\n",
    "        return scaffold_feat\n",
    "\n",
    "    \n",
    "    def extract_fragment_features(self, level2_graphs):\n",
    "        \"\"\"Extract fragment features for conditioning with atom embeddings\"\"\"\n",
    "        frag_feats = []\n",
    "        batch_dims = set()\n",
    "    \n",
    "        for graph in level2_graphs:\n",
    "            if graph.x.size(0) > 1:  # Skip empty fragments\n",
    "                # Combine node features with atom embeddings\n",
    "                combined_features = self.combine_node_features(\n",
    "                    graph.x,\n",
    "                    graph.atom_types\n",
    "                )\n",
    "                \n",
    "                frag_feat = self.fragment_encoder(\n",
    "                    combined_features, graph.edge_index, graph.edge_attr, graph.batch\n",
    "                )\n",
    "    \n",
    "                # Global pooling to get one vector per fragment batch\n",
    "                frag_feat = torch_scatter.scatter_mean(\n",
    "                    frag_feat, graph.batch, dim=0\n",
    "                )\n",
    "    \n",
    "                # Keep track of batch dimensions\n",
    "                batch_dims.add(frag_feat.size(0))\n",
    "                frag_feats.append(frag_feat)\n",
    "    \n",
    "        if frag_feats:\n",
    "            if len(batch_dims) > 1:\n",
    "                target_batch_size = max(batch_dims)\n",
    "                normalized_feats = []\n",
    "    \n",
    "                for feat in frag_feats:\n",
    "                    if feat.size(0) == target_batch_size:\n",
    "                        normalized_feats.append(feat)\n",
    "                    else:\n",
    "                        new_feat = torch.zeros(target_batch_size, feat.size(1), device=feat.device)\n",
    "                        new_feat[:feat.size(0)] = feat\n",
    "                        normalized_feats.append(new_feat)\n",
    "    \n",
    "                frag_feats = torch.stack(normalized_feats, dim=0)\n",
    "            else:\n",
    "                frag_feats = torch.stack(frag_feats, dim=0)\n",
    "    \n",
    "            frag_feats = torch.mean(frag_feats, dim=0)\n",
    "        else:\n",
    "            if level2_graphs and hasattr(level2_graphs[0], 'batch'):\n",
    "                batch_size = level2_graphs[0].batch.max().item() + 1\n",
    "            else:\n",
    "                batch_size = 1\n",
    "            device = level2_graphs[0].x.device if level2_graphs else self.device\n",
    "            frag_feats = torch.zeros(batch_size, self.cond_dim, device=device)\n",
    "    \n",
    "        return frag_feats\n",
    "\n",
    "    def forward(self, batch, mode='all', t_level1=None, t_level2=None, t_level3=None):\n",
    "        \"\"\"Forward pass through the multi-scale diffusion model with affinity guidance\"\"\"\n",
    "        device = next(self.parameters()).device\n",
    "        batch = deep_to_device(batch, device)\n",
    "\n",
    "        batch_size = batch['protein_embedding'].size(0)\n",
    "        actual_affinity = batch['affinity'].to(device)\n",
    "        if t_level1 is None:\n",
    "            t_level1 = torch.randint(0, self.num_steps, (batch_size,), device=device)\n",
    "        elif t_level1.device != device:\n",
    "            t_level1 = t_level1.to(device)\n",
    "\n",
    "        if t_level2 is None:\n",
    "            t_level2 = torch.randint(0, self.num_steps, (batch_size,), device=device)\n",
    "        elif t_level2.device != device:\n",
    "            t_level2 = t_level2.to(device)\n",
    "\n",
    "        if t_level3 is None:\n",
    "            t_level3 = torch.randint(0, self.num_steps, (batch_size,), device=device)\n",
    "        elif t_level3.device != device:\n",
    "            t_level3 = t_level3.to(device)\n",
    "\n",
    "        # Encode protein sequence\n",
    "        level1_cond, level2_cond, level3_cond = self.protein_encoder(\n",
    "            batch['protein_embedding'].to(device),\n",
    "            padding_mask=None\n",
    "        )\n",
    "\n",
    "        results = {}\n",
    "\n",
    "        # Process each level based on mode\n",
    "        if mode in ['all', 'level1', 'hierarchical']:\n",
    "            # Level 1: Scaffold\n",
    "            level1_graph = batch['level1_graph']\n",
    "            \n",
    "            # Separate features and atom types\n",
    "            level1_x_0 = level1_graph.x.to(device)\n",
    "            level1_atom_types = level1_graph.atom_types.to(device)\n",
    "            \n",
    "            # Combine with atom embeddings\n",
    "            level1_full_x_0 = self.combine_node_features(level1_x_0, level1_atom_types)\n",
    "            \n",
    "            level1_graph.batch = level1_graph.batch.to(device)\n",
    "            level1_graph.edge_index = level1_graph.edge_index.to(device)\n",
    "            level1_graph.edge_attr = level1_graph.edge_attr.to(device)\n",
    "\n",
    "            # Forward diffusion\n",
    "            level1_x_t, level1_noise = self.level1_diffusion.forward_diffusion(\n",
    "                level1_full_x_0, t_level1, batch=level1_graph.batch, device=device\n",
    "            )\n",
    "\n",
    "            # Predict noise\n",
    "            level1_noise_pred = self.level1_denoiser(\n",
    "                level1_x_t, level1_graph.edge_index, level1_graph.edge_attr,\n",
    "                t_level1, level1_cond, level1_graph.batch\n",
    "            )\n",
    "\n",
    "            # Calculate loss\n",
    "            level1_loss = F.mse_loss(level1_noise_pred, level1_noise)\n",
    "\n",
    "            results['level1_loss'] = level1_loss\n",
    "            results['level1_noise_pred'] = level1_noise_pred\n",
    "\n",
    "            # Extract scaffold features for conditioning level 2 and 3\n",
    "            if mode in ['all', 'hierarchical']:\n",
    "                level1_features = self.extract_scaffold_features(level1_graph)\n",
    "\n",
    "        if mode in ['all', 'level2', 'hierarchical']:\n",
    "            # Level 2: Fragments\n",
    "            level2_graphs = batch['level2_graphs']\n",
    "            level2_loss = torch.tensor(0.0, device=device, requires_grad=True)\n",
    "            results['level2_loss'] = level2_loss\n",
    "\n",
    "            if level2_graphs and len(level2_graphs) > 0:\n",
    "                # Get conditioning from level 1 (scaffold)\n",
    "                if mode == 'level2':\n",
    "                    # If only training level 2, extract scaffold features directly\n",
    "                    level1_features = self.extract_scaffold_features(batch['level1_graph'])\n",
    "\n",
    "                # Process each fragment graph\n",
    "                level2_losses = []\n",
    "                level2_noise_preds = []\n",
    "\n",
    "                for i, graph in enumerate(level2_graphs):\n",
    "                    graph = graph.to(device)\n",
    "                    if graph.x.size(0) > 1:  \n",
    "                        level2_x_0 = graph.x.to(device)\n",
    "                        level2_atom_types = graph.atom_types.to(device)\n",
    "                        \n",
    "                        # Combine with atom embeddings\n",
    "                        level2_full_x_0 = self.combine_node_features(level2_x_0, level2_atom_types)\n",
    "\n",
    "                        # Add scaffold conditioning to fragment features\n",
    "                        expanded_scaffold = level1_features[graph.batch]\n",
    "\n",
    "                        # Forward diffusion\n",
    "                        level2_x_t, level2_noise = self.level2_diffusion.forward_diffusion(\n",
    "                            level2_full_x_0, t_level2, batch=graph.batch, device=device\n",
    "                        )\n",
    "\n",
    "                        # Combine scaffold features with fragment features\n",
    "                        combined_x_t = torch.cat([level2_x_t, expanded_scaffold], dim=1)\n",
    "\n",
    "                        # Predict noise\n",
    "                        level2_noise_pred = self.level2_denoiser(\n",
    "                            combined_x_t, graph.edge_index, graph.edge_attr,\n",
    "                            t_level2, level2_cond, graph.batch\n",
    "                        )\n",
    "\n",
    "                        # Calculate loss\n",
    "                        level2_loss = F.mse_loss(level2_noise_pred, level2_noise)\n",
    "\n",
    "                        level2_losses.append(level2_loss)\n",
    "                        level2_noise_preds.append(level2_noise_pred)\n",
    "\n",
    "                # Combine losses only if we have valid fragments\n",
    "                if level2_losses:\n",
    "                    level2_loss = torch.stack(level2_losses).mean()\n",
    "                    results['level2_loss'] = level2_loss\n",
    "                    results['level2_noise_preds'] = level2_noise_preds\n",
    "                else:\n",
    "                    print(\"Warning: No valid fragments in batch for level2\")\n",
    "\n",
    "                # Extract fragment features for conditioning level 3\n",
    "                if mode in ['all', 'hierarchical']:\n",
    "                    level2_features = self.extract_fragment_features(level2_graphs)\n",
    "\n",
    "        if mode in ['all', 'level3', 'hierarchical']:\n",
    "            # Level 3: Complete molecule\n",
    "            level3_graph = batch['level3_graph']\n",
    "            \n",
    "            # Separate features and atom types\n",
    "            level3_x_0 = level3_graph.x.to(device)\n",
    "            level3_atom_types = level3_graph.atom_types.to(device)\n",
    "            \n",
    "            # Combine with atom embeddings\n",
    "            level3_full_x_0 = self.combine_node_features(level3_x_0, level3_atom_types)\n",
    "            \n",
    "            # Rest of graph to device\n",
    "            level3_graph.batch = level3_graph.batch.to(device)\n",
    "            level3_graph.edge_index = level3_graph.edge_index.to(device)\n",
    "            level3_graph.edge_attr = level3_graph.edge_attr.to(device)\n",
    "\n",
    "            # Initialize with default tensor for safety\n",
    "            level3_loss = torch.tensor(0.0, device=device, requires_grad=True)\n",
    "            results['level3_loss'] = level3_loss\n",
    "\n",
    "            # Get conditioning from level 1 and 2\n",
    "            if mode == 'level3':\n",
    "                # If only training level 3, extract features directly\n",
    "                level1_features = self.extract_scaffold_features(batch['level1_graph'])\n",
    "                level2_features = self.extract_fragment_features(batch['level2_graphs'])\n",
    "\n",
    "            # Forward diffusion\n",
    "            level3_x_t, level3_noise = self.level3_diffusion.forward_diffusion(\n",
    "                level3_full_x_0, t_level3, batch=level3_graph.batch, device=device\n",
    "            )\n",
    "\n",
    "            # Combine scaffold and fragment conditioning\n",
    "            combined_cond = level1_features + level2_features  # Simple additive conditioning may update in future with more complex feature aggregation\n",
    "\n",
    "            # Expand conditioning to match node dimension\n",
    "            expanded_cond = combined_cond[level3_graph.batch]\n",
    "\n",
    "            # Combine conditioning with node features\n",
    "            combined_x_t = torch.cat([level3_x_t, expanded_cond], dim=1)\n",
    "\n",
    "            # Predict noise\n",
    "            level3_noise_pred = self.level3_denoiser(\n",
    "                combined_x_t, level3_graph.edge_index, level3_graph.edge_attr,\n",
    "                t_level3, level3_cond, level3_graph.batch\n",
    "            )\n",
    "\n",
    "            # Calculate loss\n",
    "            level3_loss = F.mse_loss(level3_noise_pred, level3_noise)\n",
    "\n",
    "            results['level3_loss'] = level3_loss\n",
    "            results['level3_noise_pred'] = level3_noise_pred\n",
    "\n",
    "        # Calculate predicted affinity if needed\n",
    "        if mode in ['all', 'hierarchical']:\n",
    "            # Extract features for affinity prediction from all levels\n",
    "            scaffold_features = self.extract_scaffold_features(batch['level1_graph'])\n",
    "            fragment_features = self.extract_fragment_features(batch['level2_graphs'])\n",
    "            \n",
    "            # Get molecule features using molecule encoder\n",
    "            molecule_features = torch_scatter.scatter_mean(\n",
    "                self.molecule_encoder(\n",
    "                    self.combine_node_features(level3_graph.x, level3_graph.atom_types), \n",
    "                    level3_graph.edge_index,\n",
    "                    level3_graph.edge_attr, \n",
    "                    level3_graph.batch\n",
    "                ),\n",
    "                level3_graph.batch, dim=0\n",
    "            )\n",
    "            \n",
    "            # Concatenate all features\n",
    "            combined_features = torch.cat([scaffold_features, fragment_features, molecule_features], dim=1)\n",
    "            \n",
    "            # Predict affinity\n",
    "            predicted_affinity = self.affinity_predictor(combined_features)\n",
    "            \n",
    "            # Affinity loss\n",
    "            affinity_loss = F.mse_loss(predicted_affinity, actual_affinity)\n",
    "            \n",
    "            # Add to results\n",
    "            results['affinity_loss'] = affinity_loss\n",
    "            results['predicted_affinity'] = predicted_affinity\n",
    "\n",
    "        # Calculate overall loss based on mode - ensure all losses are proper tensors\n",
    "        if mode == 'all':\n",
    "            # Weigh the losses for each level\n",
    "            level1_loss_value = results.get('level1_loss', torch.tensor(0.0, device=device, requires_grad=True))\n",
    "            level2_loss_value = results.get('level2_loss', torch.tensor(0.0, device=device, requires_grad=True))\n",
    "            level3_loss_value = results.get('level3_loss', torch.tensor(0.0, device=device, requires_grad=True))\n",
    "            affinity_loss_value = results.get('affinity_loss', torch.tensor(0.0, device=device, requires_grad=True))\n",
    "\n",
    "            # Include affinity loss in total loss\n",
    "            results['loss'] = level1_loss_value * 0.4 + level2_loss_value * 0.3 + level3_loss_value * 0.2 + affinity_loss_value * 0.1\n",
    "            \n",
    "        elif mode == 'hierarchical':\n",
    "            # All losses with emphasis on the current total loss \n",
    "            level1_loss_value = results.get('level1_loss', torch.tensor(0.0, device=device, requires_grad=True))\n",
    "            level2_loss_value = results.get('level2_loss', torch.tensor(0.0, device=device, requires_grad=True))\n",
    "            level3_loss_value = results.get('level3_loss', torch.tensor(0.0, device=device, requires_grad=True))\n",
    "            affinity_loss_value = results.get('affinity_loss', torch.tensor(0.0, device=device, requires_grad=True))\n",
    "\n",
    "            # Include affinity loss in total loss\n",
    "            results['loss'] = level1_loss_value * 0.2 + level2_loss_value * 0.2 + level3_loss_value * 0.4 + affinity_loss_value * 0.2\n",
    "            \n",
    "        elif mode == 'level1':\n",
    "            results['loss'] = results.get('level1_loss', torch.tensor(0.0, device=device, requires_grad=True))\n",
    "        elif mode == 'level2':\n",
    "            results['loss'] = results.get('level2_loss', torch.tensor(0.0, device=device, requires_grad=True))\n",
    "        elif mode == 'level3':\n",
    "            results['loss'] = results.get('level3_loss', torch.tensor(0.0, device=device, requires_grad=True))\n",
    "\n",
    "        return results\n",
    "\n",
    "    \n",
    "    def sample(self, protein_embedding, num_samples=1, device=None, \n",
    "          scaffold=None, fragments=None, target_affinity=None):\n",
    "        \"\"\"\n",
    "        Generate samples by reversing the diffusion process with stochastic initialization\n",
    "        \n",
    "        Args:\n",
    "            protein_embedding: Protein embedding tensor\n",
    "            num_samples: Number of molecules to generate per protein\n",
    "            device: Compute device\n",
    "            scaffold: Predefined scaffold (optional)\n",
    "            fragments: Predefined fragments (optional)\n",
    "            target_affinity: Target affinity to aim for (optional)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with generated molecules, fragments, scaffolds and predicted affinities\n",
    "        \"\"\"\n",
    "        if device is None:\n",
    "            device = next(self.parameters()).device\n",
    "    \n",
    "        # Move protein embedding to device\n",
    "        protein_embedding = protein_embedding.to(device)\n",
    "    \n",
    "        # Batch dimension handling\n",
    "        if protein_embedding.dim() == 2 and protein_embedding.size(0) == 1:\n",
    "            batch_size = num_samples\n",
    "            protein_embedding = protein_embedding.repeat(num_samples, 1)\n",
    "        elif protein_embedding.dim() == 3 and protein_embedding.size(0) == 1:\n",
    "            batch_size = num_samples\n",
    "            protein_embedding = protein_embedding.repeat(num_samples, 1, 1)\n",
    "        else:\n",
    "            batch_size = protein_embedding.size(0)\n",
    "    \n",
    "        # Encode protein\n",
    "        level1_cond, level2_cond, level3_cond = self.protein_encoder(\n",
    "            protein_embedding, padding_mask=None\n",
    "        )\n",
    "    \n",
    "        # Sample scaffold if not provided\n",
    "        if scaffold is None:\n",
    "            # Stochastic scaffold generation\n",
    "            # Step 1: Choose scaffold type with probabilities\n",
    "            scaffold_types = ['simple_ring', 'fused_rings', 'bridged_system', 'macrocycle']\n",
    "            scaffold_probs = [0.5, 0.3, 0.1, 0.1]\n",
    "            \n",
    "            scaffold_choices = [random.choices(scaffold_types, weights=scaffold_probs)[0] \n",
    "                              for _ in range(batch_size)]\n",
    "            \n",
    "            # Step 2: Generate size parameters\n",
    "            min_atoms = 5\n",
    "            max_atoms = 25 \n",
    "            \n",
    "            # More stochastic size distribution\n",
    "            num_atoms = []\n",
    "            for scaffold_type in scaffold_choices:\n",
    "                if scaffold_type == 'simple_ring':\n",
    "                    # Common ring sizes: 5 and 6 more likely\n",
    "                    ring_sizes = [3, 4, 5, 6, 7, 8]\n",
    "                    ring_weights = [0.05, 0.1, 0.4, 0.3, 0.1, 0.05]\n",
    "                    size = random.choices(ring_sizes, weights=ring_weights)[0]\n",
    "                elif scaffold_type == 'fused_rings':\n",
    "                    num_rings = random.choices([2, 3], weights=[0.7, 0.3])[0]\n",
    "                    size = random.randint(8, 14)\n",
    "                elif scaffold_type == 'bridged_system':\n",
    "                    size = random.randint(8, 15)\n",
    "                else:  # macrocycle\n",
    "                    size = random.randint(10, 20)\n",
    "                \n",
    "                num_atoms.append(size)\n",
    "            \n",
    "            num_atoms = torch.tensor(num_atoms, device=device)\n",
    "            max_atoms = num_atoms.max().item()\n",
    "    \n",
    "            # Create initial random features\n",
    "            x = torch.randn(batch_size * max_atoms, self.node_dim, device=device)\n",
    "            \n",
    "            atom_types = []\n",
    "            for i, scaffold_type in enumerate(scaffold_choices):\n",
    "                n_atoms = num_atoms[i].item()\n",
    "                \n",
    "                # Different probabilities based on scaffold type\n",
    "                if scaffold_type in ['simple_ring', 'fused_rings']:\n",
    "                    elements = [6, 7, 8, 16]  # C, N, O, S\n",
    "                    weights = [0.7, 0.15, 0.1, 0.05]\n",
    "                elif scaffold_type == 'bridged_system':\n",
    "                    # More nitrogen-rich\n",
    "                    elements = [6, 7, 8, 16]\n",
    "                    weights = [0.6, 0.25, 0.1, 0.05]\n",
    "                else:  # macrocycle\n",
    "                    # Varied heteroatoms\n",
    "                    elements = [6, 7, 8, 16, 15]  # C, N, O, S, P\n",
    "                    weights = [0.6, 0.2, 0.1, 0.05, 0.05]\n",
    "                    \n",
    "                scaffold_atoms = random.choices(elements, weights=weights, k=n_atoms)\n",
    "                atom_types.extend(scaffold_atoms)\n",
    "                \n",
    "            atom_types = torch.tensor(atom_types, device=device)\n",
    "            \n",
    "            # Combine for full node features\n",
    "            full_x = self.combine_node_features(x, atom_types)\n",
    "    \n",
    "            # Create structure - more diverse connectivity\n",
    "            edge_index = []\n",
    "            edge_attr = []\n",
    "            batch_indices = []\n",
    "    \n",
    "            node_offset = 0\n",
    "            for i, (n_atoms, scaffold_type) in enumerate(zip(num_atoms, scaffold_choices)):\n",
    "                n_atoms = n_atoms.item()\n",
    "                # Add batch indices\n",
    "                for j in range(n_atoms):\n",
    "                    batch_indices.append(i)\n",
    "                \n",
    "                # Create connectivity based on scaffold type\n",
    "                if scaffold_type == 'simple_ring':\n",
    "                    # Regular ring\n",
    "                    for j in range(n_atoms):\n",
    "                        if j < n_atoms - 1:\n",
    "                            edge_index.append([node_offset + j, node_offset + j + 1])\n",
    "                            edge_index.append([node_offset + j + 1, node_offset + j])\n",
    "                        else:\n",
    "                            # Connect last node to first\n",
    "                            edge_index.append([node_offset + j, node_offset])\n",
    "                            edge_index.append([node_offset, node_offset + j])\n",
    "                            \n",
    "                        # Randomly decide on bond types\n",
    "                        if random.random() < 0.85:  \n",
    "                            bond_type = [1, 0, 0, 0, 0]  # Single bond\n",
    "                        elif random.random() < 0.5:  \n",
    "                            bond_type = [0, 1, 0, 0, 0]  # Double bond\n",
    "                        else:  # Few aromatic bonds\n",
    "                            bond_type = [0, 0, 0, 1, 0]  # Aromatic bond\n",
    "                            \n",
    "                        edge_attr.append(bond_type)\n",
    "                        edge_attr.append(bond_type)\n",
    "                    \n",
    "                    # Probability for additional cross-ring connections\n",
    "                    if n_atoms > 5 and random.random() < 0.3:\n",
    "                        # Add a bridging connection\n",
    "                        start = random.randint(0, n_atoms//2 - 1)\n",
    "                        end = random.randint(n_atoms//2 + 1, n_atoms - 1)\n",
    "                        \n",
    "                        edge_index.append([node_offset + start, node_offset + end])\n",
    "                        edge_index.append([node_offset + end, node_offset + start])\n",
    "                        \n",
    "                        edge_attr.append([1, 0, 0, 0, 0])  # Single bond\n",
    "                        edge_attr.append([1, 0, 0, 0, 0])\n",
    "                    \n",
    "                elif scaffold_type == 'fused_rings':\n",
    "                    # Create first ring\n",
    "                    ring1_size = n_atoms // 2\n",
    "                    for j in range(ring1_size):\n",
    "                        if j < ring1_size - 1:\n",
    "                            edge_index.append([node_offset + j, node_offset + j + 1])\n",
    "                            edge_index.append([node_offset + j + 1, node_offset + j])\n",
    "                        else:\n",
    "                            edge_index.append([node_offset + j, node_offset])\n",
    "                            edge_index.append([node_offset, node_offset + j])\n",
    "                            \n",
    "                        # Add edge features - aromatic rings more common\n",
    "                        if random.random() < 0.5:\n",
    "                            bond_type = [0, 0, 0, 1, 0]  # Aromatic\n",
    "                        else:\n",
    "                            bond_type = [1, 0, 0, 0, 0]  # Single bond\n",
    "                            \n",
    "                        edge_attr.append(bond_type)\n",
    "                        edge_attr.append(bond_type)\n",
    "                    \n",
    "                    # Create second ring - fused\n",
    "                    for j in range(ring1_size, n_atoms):\n",
    "                        if j < n_atoms - 1:\n",
    "                            edge_index.append([node_offset + j, node_offset + j + 1])\n",
    "                            edge_index.append([node_offset + j + 1, node_offset + j])\n",
    "                        else:\n",
    "                            edge_index.append([node_offset + j, node_offset + ring1_size])\n",
    "                            edge_index.append([node_offset + ring1_size, node_offset + j])\n",
    "                            \n",
    "                        edge_attr.append([1, 0, 0, 0, 0])  # Single bond\n",
    "                        edge_attr.append([1, 0, 0, 0, 0])\n",
    "                    \n",
    "                    # Add fusion bonds\n",
    "                    fusion_point = random.randint(1, ring1_size - 2)\n",
    "                    edge_index.append([node_offset + fusion_point, node_offset + ring1_size])\n",
    "                    edge_index.append([node_offset + ring1_size, node_offset + fusion_point])\n",
    "                    edge_attr.append([1, 0, 0, 0, 0])\n",
    "                    edge_attr.append([1, 0, 0, 0, 0])\n",
    "                    \n",
    "                elif scaffold_type == 'bridged_system':\n",
    "                    # Create base ring\n",
    "                    base_size = n_atoms - 2\n",
    "                    for j in range(base_size):\n",
    "                        if j < base_size - 1:\n",
    "                            edge_index.append([node_offset + j, node_offset + j + 1])\n",
    "                            edge_index.append([node_offset + j + 1, node_offset + j])\n",
    "                        else:\n",
    "                            edge_index.append([node_offset + j, node_offset])\n",
    "                            edge_index.append([node_offset, node_offset + j])\n",
    "                            \n",
    "                        edge_attr.append([1, 0, 0, 0, 0])\n",
    "                        edge_attr.append([1, 0, 0, 0, 0])\n",
    "                    \n",
    "                    # Add bridge\n",
    "                    bridge_start = random.randint(1, base_size//3)\n",
    "                    bridge_end = random.randint(2*base_size//3, base_size-1)\n",
    "                    \n",
    "                    # Bridge atoms\n",
    "                    bridge1 = node_offset + base_size\n",
    "                    bridge2 = node_offset + base_size + 1\n",
    "                    \n",
    "                    # Connect bridge\n",
    "                    edge_index.append([node_offset + bridge_start, bridge1])\n",
    "                    edge_index.append([bridge1, node_offset + bridge_start])\n",
    "                    edge_index.append([bridge1, bridge2])\n",
    "                    edge_index.append([bridge2, bridge1])\n",
    "                    edge_index.append([bridge2, node_offset + bridge_end])\n",
    "                    edge_index.append([node_offset + bridge_end, bridge2])\n",
    "                    \n",
    "                    for _ in range(6):  # Add 6 edges (3 bonds)\n",
    "                        edge_attr.append([1, 0, 0, 0, 0])\n",
    "                    \n",
    "                else:  # macrocycle\n",
    "                    # Create large ring\n",
    "                    for j in range(n_atoms):\n",
    "                        if j < n_atoms - 1:\n",
    "                            edge_index.append([node_offset + j, node_offset + j + 1])\n",
    "                            edge_index.append([node_offset + j + 1, node_offset + j])\n",
    "                        else:\n",
    "                            edge_index.append([node_offset + j, node_offset])\n",
    "                            edge_index.append([node_offset, node_offset + j])\n",
    "                            \n",
    "                        # Add single and double bonds\n",
    "                        if j % 3 == 0:  # Every third bond is double\n",
    "                            bond_type = [0, 1, 0, 0, 0]  # Double bond\n",
    "                        else:\n",
    "                            bond_type = [1, 0, 0, 0, 0]  # Single bond\n",
    "                            \n",
    "                        edge_attr.append(bond_type)\n",
    "                        edge_attr.append(bond_type)\n",
    "                    \n",
    "                    # Add cross-links with low probability\n",
    "                    if random.random() < 0.3:\n",
    "                        cross1 = random.randint(0, n_atoms//3)\n",
    "                        cross2 = random.randint(2*n_atoms//3, n_atoms-1)\n",
    "                        \n",
    "                        edge_index.append([node_offset + cross1, node_offset + cross2])\n",
    "                        edge_index.append([node_offset + cross2, node_offset + cross1])\n",
    "                        \n",
    "                        edge_attr.append([1, 0, 0, 0, 0])\n",
    "                        edge_attr.append([1, 0, 0, 0, 0])\n",
    "                \n",
    "                # Update offset for next molecule\n",
    "                node_offset += n_atoms\n",
    "    \n",
    "            # Convert to tensors\n",
    "            edge_index = torch.tensor(edge_index, device=device).t().contiguous()\n",
    "            edge_attr = torch.tensor(edge_attr, device=device, dtype=torch.float)\n",
    "            batch_indices = torch.tensor(batch_indices, device=device)\n",
    "    \n",
    "            # Create initial graph\n",
    "            scaffold_graph = Data(\n",
    "                x=x[:node_offset],  # Original node features \n",
    "                atom_types=atom_types,  # Atom type indices\n",
    "                edge_index=edge_index,\n",
    "                edge_attr=edge_attr,\n",
    "                batch=batch_indices\n",
    "            )\n",
    "            \n",
    "            # Combine node features with embeddings for sampling\n",
    "            scaffold_graph.x_full = self.combine_node_features(scaffold_graph.x, scaffold_graph.atom_types)\n",
    "    \n",
    "            # Reverse diffusion for scaffold\n",
    "            for t in tqdm(range(self.num_steps - 1, -1, -1), desc=\"Generating scaffolds\"):\n",
    "                # Get timesteps\n",
    "                timesteps = torch.full((batch_size,), t, device=device, dtype=torch.long)\n",
    "    \n",
    "                # Add noise if t > 0\n",
    "                if t > 0:\n",
    "                    noise = torch.randn_like(scaffold_graph.x_full) * 0.5  # Reduced noise\n",
    "                else:\n",
    "                    noise = 0\n",
    "    \n",
    "                # Predict noise and denoise\n",
    "                with torch.no_grad():\n",
    "                    noise_pred = self.level1_denoiser(\n",
    "                        scaffold_graph.x_full, scaffold_graph.edge_index, scaffold_graph.edge_attr,\n",
    "                        timesteps, level1_cond, scaffold_graph.batch\n",
    "                    )\n",
    "    \n",
    "                # Get alpha and beta values for this timestep\n",
    "                alpha = self.level1_diffusion.alpha[t].to(device)\n",
    "                alpha_bar = self.level1_diffusion.alpha_bar[t].to(device)\n",
    "                beta = self.level1_diffusion.beta[t].to(device)\n",
    "    \n",
    "                # Update x\n",
    "                if t > 0:\n",
    "                    alpha_next = self.level1_diffusion.alpha[t-1].to(device)\n",
    "                    alpha_bar_next = self.level1_diffusion.alpha_bar[t-1].to(device)\n",
    "    \n",
    "                    # Calculate posterior mean and variance\n",
    "                    posterior_mean = (\n",
    "                        torch.sqrt(alpha_bar_next) * beta / (1 - alpha_bar) * scaffold_graph.x_full +\n",
    "                        torch.sqrt(alpha) * (1 - alpha_bar_next) / (1 - alpha_bar) * \n",
    "                        (scaffold_graph.x_full - noise_pred * torch.sqrt(1 - alpha_bar) / torch.sqrt(alpha_bar))\n",
    "                    )\n",
    "    \n",
    "                    posterior_var = beta * (1 - alpha_bar_next) / (1 - alpha_bar)\n",
    "    \n",
    "                    # Update x\n",
    "                    scaffold_graph.x_full = posterior_mean + torch.sqrt(posterior_var) * noise\n",
    "                else:\n",
    "                    # Final prediction\n",
    "                    scaffold_graph.x_full = (scaffold_graph.x_full - noise_pred * torch.sqrt(1 - alpha_bar)) / torch.sqrt(alpha_bar)\n",
    "    \n",
    "                \n",
    "                scaffold_graph.x_full = torch.clamp(scaffold_graph.x_full, -5, 5)\n",
    "                \n",
    "                # Every 200 steps, update atom types based on the embedding part\n",
    "                if t % 200 == 0 or t == 0:\n",
    "                    # Extract updated atom embeddings\n",
    "                    node_features = scaffold_graph.x_full[:, :self.node_dim]\n",
    "                    atom_embeddings = scaffold_graph.x_full[:, self.node_dim:]\n",
    "                    \n",
    "                    # Update x and separate atom embeddings\n",
    "                    scaffold_graph.x = node_features\n",
    "                \n",
    "            generated_scaffold = scaffold_graph\n",
    "        else:\n",
    "            generated_scaffold = scaffold\n",
    "    \n",
    "        # Extract scaffold features for conditioning\n",
    "        scaffold_features = self.extract_scaffold_features(generated_scaffold)\n",
    "        if fragments is None and generated_scaffold.x.size(0) > 0:\n",
    "            # Create initial random fragments\n",
    "\n",
    "            num_scaffold_atoms = torch.bincount(generated_scaffold.batch)\n",
    "            \n",
    "            # Decide on number of fragments per molecule\n",
    "            fragments_per_mol = []\n",
    "            for i in range(batch_size):\n",
    "                n_scaff_atoms = num_scaffold_atoms[i].item() if i < len(num_scaffold_atoms) else 0\n",
    "                if n_scaff_atoms > 0:\n",
    "                    min_frags = max(1, n_scaff_atoms // 5)  # At least 1 fragment\n",
    "                    max_frags = max(2, n_scaff_atoms // 3)  # More fragments for larger scaffolds\n",
    "                    n_frags = random.randint(min_frags, max_frags)\n",
    "                    fragments_per_mol.append(n_frags)\n",
    "                else:\n",
    "                    fragments_per_mol.append(0)\n",
    "    \n",
    "            # Generate fragments\n",
    "            generated_fragments = []\n",
    "            \n",
    "            for mol_idx, n_frags in enumerate(fragments_per_mol):\n",
    "                for frag_idx in range(n_frags):\n",
    "                    # Randomly choose fragment type\n",
    "                    frag_types = ['chain', 'ring', 'branch', 'functional_group']\n",
    "                    frag_probs = [0.4, 0.3, 0.2, 0.1]  # Chains most common\n",
    "                    frag_type = random.choices(frag_types, weights=frag_probs)[0]\n",
    "                    \n",
    "                    # Determine fragment size based on type\n",
    "                    if frag_type == 'chain':\n",
    "                        frag_size = random.randint(2, 6)  # Linear chains\n",
    "                    elif frag_type == 'ring':\n",
    "                        frag_size = random.choices([3, 4, 5, 6], weights=[0.1, 0.2, 0.4, 0.3])[0]  # Rings\n",
    "                    elif frag_type == 'branch':\n",
    "                        frag_size = random.randint(3, 5)  # Branched structures\n",
    "                    else:  # functional_group\n",
    "                        frag_size = random.randint(1, 3)  # Small functional groups\n",
    "                    \n",
    "                    # Create fragment features\n",
    "                    frag_x = torch.randn(frag_size, self.node_dim, device=device)\n",
    "                    frag_batch = torch.full((frag_size,), mol_idx, device=device)\n",
    "                    \n",
    "                    # Choose atom types for this fragment\n",
    "                    atom_types = []\n",
    "                    \n",
    "                    if frag_type == 'chain':\n",
    "                        # Linear chains - mostly carbon with some heteroatoms\n",
    "                        elements = [6, 7, 8, 16]  # C, N, O, S\n",
    "                        weights = [0.7, 0.15, 0.1, 0.05]\n",
    "                    elif frag_type == 'ring':\n",
    "                        # Rings - aromatic carbons and nitrogens\n",
    "                        elements = [6, 7, 8]  # C, N, O\n",
    "                        weights = [0.6, 0.3, 0.1]\n",
    "                    elif frag_type == 'branch':\n",
    "                        # Branches - mixed elements\n",
    "                        elements = [6, 7, 8, 16, 15]  # C, N, O, S, P\n",
    "                        weights = [0.5, 0.2, 0.15, 0.1, 0.05]\n",
    "                    else:  # functional_group\n",
    "                        # Functional groups - heteroatom-rich\n",
    "                        elements = [7, 8, 9, 16, 17]  # N, O, F, S, Cl\n",
    "                        weights = [0.3, 0.3, 0.2, 0.1, 0.1]\n",
    "                    \n",
    "                    atom_types = torch.tensor(\n",
    "                        random.choices(elements, weights=weights, k=frag_size),\n",
    "                        device=device, dtype=torch.long\n",
    "                    )\n",
    "                    \n",
    "                    # Create edges based on fragment type\n",
    "                    frag_edge_index = []\n",
    "                    frag_edge_attr = []\n",
    "                    \n",
    "                    if frag_type == 'chain':\n",
    "                        # Linear chain\n",
    "                        for j in range(frag_size - 1):\n",
    "                            frag_edge_index.append([j, j+1])\n",
    "                            frag_edge_index.append([j+1, j])\n",
    "                            \n",
    "                            # Random bond types, mostly single\n",
    "                            if random.random() < 0.8:  # 80% single bonds\n",
    "                                bond_type = [1, 0, 0, 0, 0]  # Single\n",
    "                            else:\n",
    "                                bond_type = [0, 1, 0, 0, 0]  # Double\n",
    "                                \n",
    "                            frag_edge_attr.append(bond_type)\n",
    "                            frag_edge_attr.append(bond_type)\n",
    "                    \n",
    "                    elif frag_type == 'ring':\n",
    "                        # Ring structure\n",
    "                        for j in range(frag_size):\n",
    "                            next_j = (j + 1) % frag_size\n",
    "                            frag_edge_index.append([j, next_j])\n",
    "                            frag_edge_index.append([next_j, j])\n",
    "                            \n",
    "                            # More aromatic bonds in rings\n",
    "                            if random.random() < 0.6:  # 60% aromatic\n",
    "                                bond_type = [0, 0, 0, 1, 0]  # Aromatic\n",
    "                            else:\n",
    "                                bond_type = [1, 0, 0, 0, 0]  # Single\n",
    "                                \n",
    "                            frag_edge_attr.append(bond_type)\n",
    "                            frag_edge_attr.append(bond_type)\n",
    "                    \n",
    "                    elif frag_type == 'branch':\n",
    "                        # Central atom with branches\n",
    "                        center = 0\n",
    "                        for j in range(1, frag_size):\n",
    "                            frag_edge_index.append([center, j])\n",
    "                            frag_edge_index.append([j, center])\n",
    "                            \n",
    "                            # Mostly single bonds\n",
    "                            bond_type = [1, 0, 0, 0, 0]  # Single\n",
    "                            frag_edge_attr.append(bond_type)\n",
    "                            frag_edge_attr.append(bond_type)\n",
    "                    \n",
    "                    else:  # functional_group\n",
    "                        # Simple chain for functional groups\n",
    "                        for j in range(frag_size - 1):\n",
    "                            frag_edge_index.append([j, j+1])\n",
    "                            frag_edge_index.append([j+1, j])\n",
    "                            \n",
    "                            # Mix of bond types\n",
    "                            if j == 0 and frag_size > 2:\n",
    "                                bond_type = [0, 1, 0, 0, 0]  # Double (e.g. C=O)\n",
    "                            else:\n",
    "                                bond_type = [1, 0, 0, 0, 0]  # Single\n",
    "                                \n",
    "                            frag_edge_attr.append(bond_type)\n",
    "                            frag_edge_attr.append(bond_type)\n",
    "                    \n",
    "                    # Convert to tensors\n",
    "                    if len(frag_edge_index) > 0:\n",
    "                        frag_edge_index = torch.tensor(frag_edge_index, device=device).t().contiguous()\n",
    "                        frag_edge_attr = torch.tensor(frag_edge_attr, device=device, dtype=torch.float)\n",
    "                    else:\n",
    "                        # Empty tensors if no edges\n",
    "                        frag_edge_index = torch.zeros((2, 0), device=device, dtype=torch.long)\n",
    "                        frag_edge_attr = torch.zeros((0, 5), device=device, dtype=torch.float)\n",
    "                    \n",
    "                    # Create fragment graph\n",
    "                    frag_graph = Data(\n",
    "                        x=frag_x,\n",
    "                        atom_types=atom_types,\n",
    "                        edge_index=frag_edge_index,\n",
    "                        edge_attr=frag_edge_attr,\n",
    "                        batch=frag_batch\n",
    "                    )\n",
    "                    \n",
    "                    # Combine node features with embeddings for sampling\n",
    "                    frag_graph.x_full = self.combine_node_features(frag_graph.x, frag_graph.atom_types)\n",
    "                    \n",
    "                    # Expand scaffold features to fragment dimensions\n",
    "                    expanded_scaffold = scaffold_features[frag_graph.batch]\n",
    "                    \n",
    "                    # Reverse diffusion for fragment\n",
    "                    for t in tqdm(range(self.num_steps - 1, -1, -1), \n",
    "                                 desc=f\"Generating fragment {frag_idx+1}/{n_frags} for molecule {mol_idx+1}/{batch_size}\"):\n",
    "                        # Get timesteps\n",
    "                        timesteps = torch.full((batch_size,), t, device=device, dtype=torch.long)\n",
    "                        \n",
    "                        # Add noise if t > 0\n",
    "                        if t > 0:\n",
    "                            noise = torch.randn_like(frag_graph.x_full) * 0.5\n",
    "                        else:\n",
    "                            noise = 0\n",
    "                        \n",
    "                        # Combine scaffold features with fragment features\n",
    "                        combined_x = torch.cat([frag_graph.x_full, expanded_scaffold], dim=1)\n",
    "                        \n",
    "                        # Predict noise and denoise\n",
    "                        with torch.no_grad():\n",
    "                            noise_pred = self.level2_denoiser(\n",
    "                                combined_x, frag_graph.edge_index, frag_graph.edge_attr,\n",
    "                                timesteps, level2_cond, frag_graph.batch\n",
    "                            )\n",
    "                        \n",
    "                        # Get alpha and beta values\n",
    "                        alpha = self.level2_diffusion.alpha[t].to(device)\n",
    "                        alpha_bar = self.level2_diffusion.alpha_bar[t].to(device)\n",
    "                        beta = self.level2_diffusion.beta[t].to(device)\n",
    "                        \n",
    "                        # Update x\n",
    "                        if t > 0:\n",
    "                            alpha_next = self.level2_diffusion.alpha[t-1].to(device)\n",
    "                            alpha_bar_next = self.level2_diffusion.alpha_bar[t-1].to(device)\n",
    "                            \n",
    "                            # Calculate posterior mean and variance\n",
    "                            posterior_mean = (\n",
    "                                torch.sqrt(alpha_bar_next) * beta / (1 - alpha_bar) * frag_graph.x_full +\n",
    "                                torch.sqrt(alpha) * (1 - alpha_bar_next) / (1 - alpha_bar) * \n",
    "                                (frag_graph.x_full - noise_pred * torch.sqrt(1 - alpha_bar) / torch.sqrt(alpha_bar))\n",
    "                            )\n",
    "                            \n",
    "                            posterior_var = beta * (1 - alpha_bar_next) / (1 - alpha_bar)\n",
    "                            \n",
    "                            # Update x\n",
    "                            frag_graph.x_full = posterior_mean + torch.sqrt(posterior_var) * noise\n",
    "                        else:\n",
    "                            # Final prediction\n",
    "                            frag_graph.x_full = (frag_graph.x_full - noise_pred * torch.sqrt(1 - alpha_bar)) / torch.sqrt(alpha_bar)\n",
    "                        \n",
    "                        # Apply validity constraints\n",
    "                        frag_graph.x_full = torch.clamp(frag_graph.x_full, -5, 5)\n",
    "                        \n",
    "                        # Update node features and atom types occasionally\n",
    "                        if t % 200 == 0 or t == 0:\n",
    "                            # Extract updated node features\n",
    "                            node_features = frag_graph.x_full[:, :self.node_dim]\n",
    "                            frag_graph.x = node_features\n",
    "                    \n",
    "                    # Add to generated fragments\n",
    "                    generated_fragments.append(frag_graph)\n",
    "        else:\n",
    "            # Use provided fragments\n",
    "            generated_fragments = fragments\n",
    "    \n",
    "        # Extract fragment features for conditioning level 3\n",
    "        if generated_fragments:\n",
    "            fragment_features = self.extract_fragment_features(generated_fragments)\n",
    "        else:\n",
    "            # Create empty tensor if no fragments\n",
    "            fragment_features = torch.zeros(batch_size, self.cond_dim, device=device)\n",
    "    \n",
    "        # Generate complete molecules by combining scaffold and fragments\n",
    "        if generated_scaffold.x.size(0) > 0:\n",
    "            # Calculate total atoms needed for full molecule\n",
    "            num_scaffold_atoms = torch.bincount(generated_scaffold.batch)\n",
    "            \n",
    "            # Count fragment atoms per molecule\n",
    "            fragment_atoms_per_mol = defaultdict(int)\n",
    "            attachment_points_per_mol = {}\n",
    "            \n",
    "            for frag in generated_fragments:\n",
    "                batch_counts = torch.bincount(frag.batch, minlength=batch_size)\n",
    "                for i in range(batch_size):\n",
    "                    fragment_atoms_per_mol[i] += batch_counts[i].item()\n",
    "            \n",
    "            # Determine attachment points on scaffold\n",
    "            for i in range(batch_size):\n",
    "                n_scaff = num_scaffold_atoms[i].item() if i < len(num_scaffold_atoms) else 0\n",
    "                n_frags = 0\n",
    "                for frag in generated_fragments:\n",
    "                    batch_idx = frag.batch == i\n",
    "                    if torch.any(batch_idx):\n",
    "                        n_frags += 1\n",
    "                \n",
    "                if n_scaff > 0 and n_frags > 0:\n",
    "                    # Sample random attachment points for each fragment\n",
    "                    scaffold_mask = generated_scaffold.batch == i\n",
    "                    attachment_points = []\n",
    "                    \n",
    "                    # Identify all scaffold atoms for this molecule\n",
    "                    scaffold_atoms = torch.where(scaffold_mask)[0].tolist()\n",
    "                    \n",
    "                    if scaffold_atoms:\n",
    "                        # Sample attachment points without replacement if possible\n",
    "                        if len(scaffold_atoms) >= n_frags:\n",
    "                            attachment_indices = random.sample(range(len(scaffold_atoms)), n_frags)\n",
    "                            attachment_points = [scaffold_atoms[idx] for idx in attachment_indices]\n",
    "                        else:\n",
    "                            # If we have more fragments than scaffold atoms, some can share attachment points\n",
    "                            attachment_points = [random.choice(scaffold_atoms) for _ in range(n_frags)]\n",
    "                    \n",
    "                    attachment_points_per_mol[i] = attachment_points\n",
    "            \n",
    "            # Calculate total atoms for full molecule\n",
    "            num_total_atoms = []\n",
    "            for i in range(batch_size):\n",
    "                n_scaff = num_scaffold_atoms[i].item() if i < len(num_scaffold_atoms) else 0\n",
    "                n_frag = fragment_atoms_per_mol.get(i, 0)\n",
    "                num_total_atoms.append(n_scaff + n_frag)\n",
    "            \n",
    "            max_total_atoms = max(num_total_atoms) if num_total_atoms else 0\n",
    "            \n",
    "            # Create space for the full molecules\n",
    "            full_x = torch.zeros(sum(num_total_atoms), self.node_dim, device=device)\n",
    "            full_atom_types = torch.zeros(sum(num_total_atoms), dtype=torch.long, device=device)\n",
    "            full_edge_index = []\n",
    "            full_edge_attr = []\n",
    "            full_batch = []\n",
    "            \n",
    "            # Track node offsets per molecule\n",
    "            node_offsets = [0]\n",
    "            for i in range(batch_size - 1):\n",
    "                node_offsets.append(node_offsets[-1] + num_total_atoms[i])\n",
    "            \n",
    "            # Copy scaffold nodes and edges\n",
    "            for i in range(batch_size):\n",
    "                # Identify scaffold nodes for this molecule\n",
    "                scaffold_mask = generated_scaffold.batch == i\n",
    "                scaffold_nodes = torch.where(scaffold_mask)[0].tolist()\n",
    "                \n",
    "                # Calculate offset for this molecule\n",
    "                offset = node_offsets[i]\n",
    "                \n",
    "                # Copy scaffold node features and atom types\n",
    "                for j, node_idx in enumerate(scaffold_nodes):\n",
    "                    full_x[offset + j] = generated_scaffold.x[node_idx]\n",
    "                    full_atom_types[offset + j] = generated_scaffold.atom_types[node_idx]\n",
    "                    full_batch.append(i)\n",
    "                \n",
    "                # Map scaffold node indices\n",
    "                scaffold_node_map = {old_idx: offset + new_idx for new_idx, old_idx in enumerate(scaffold_nodes)}\n",
    "                \n",
    "                # Copy scaffold edges\n",
    "                for edge_idx in range(generated_scaffold.edge_index.size(1)):\n",
    "                    src, dst = generated_scaffold.edge_index[:, edge_idx]\n",
    "                    \n",
    "                    # Check if both nodes belong to this molecule\n",
    "                    if generated_scaffold.batch[src] == i and generated_scaffold.batch[dst] == i:\n",
    "                        # Map to new indices\n",
    "                        src_new = scaffold_node_map[src.item()]\n",
    "                        dst_new = scaffold_node_map[dst.item()]\n",
    "                        \n",
    "                        full_edge_index.append([src_new, dst_new])\n",
    "                        full_edge_attr.append(generated_scaffold.edge_attr[edge_idx].tolist())\n",
    "                \n",
    "                # Track the current position after scaffold nodes\n",
    "                current_pos = offset + len(scaffold_nodes)\n",
    "                \n",
    "                # Process fragments for this molecule\n",
    "                frag_counter = 0\n",
    "                attachment_points = attachment_points_per_mol.get(i, [])\n",
    "                \n",
    "                for frag_idx, frag in enumerate(generated_fragments):\n",
    "                    # Check if this fragment belongs to this molecule\n",
    "                    frag_mask = frag.batch == i\n",
    "                    frag_nodes = torch.where(frag_mask)[0].tolist()\n",
    "                    \n",
    "                    if frag_nodes:\n",
    "                        # Get attachment point for this fragment\n",
    "                        attachment_idx = attachment_points[frag_counter] if frag_counter < len(attachment_points) else None\n",
    "                        \n",
    "                        # Map fragment node indices\n",
    "                        frag_node_map = {old_idx: current_pos + new_idx for new_idx, old_idx in enumerate(frag_nodes)}\n",
    "                        \n",
    "                        # Copy fragment node features and atom types\n",
    "                        for j, node_idx in enumerate(frag_nodes):\n",
    "                            full_x[current_pos + j] = frag.x[node_idx]\n",
    "                            full_atom_types[current_pos + j] = frag.atom_types[node_idx]\n",
    "                            full_batch.append(i)\n",
    "                        \n",
    "                        # Copy fragment edges\n",
    "                        for edge_idx in range(frag.edge_index.size(1)):\n",
    "                            src, dst = frag.edge_index[:, edge_idx]\n",
    "                            \n",
    "                            # Check if both nodes belong to this fragment\n",
    "                            if frag.batch[src] == i and frag.batch[dst] == i:\n",
    "                                # Map to new indices\n",
    "                                src_new = frag_node_map[src.item()]\n",
    "                                dst_new = frag_node_map[dst.item()]\n",
    "                                \n",
    "                                full_edge_index.append([src_new, dst_new])\n",
    "                                full_edge_attr.append(frag.edge_attr[edge_idx].tolist())\n",
    "                        \n",
    "                        # Connect fragment to scaffold at attachment point\n",
    "                        if attachment_idx is not None and frag_nodes:\n",
    "                            # Choose first fragment atom as connection point\n",
    "                            frag_connection = frag_node_map[frag_nodes[0]]\n",
    "                            \n",
    "                            # Add connecting bond\n",
    "                            full_edge_index.append([scaffold_node_map[attachment_idx], frag_connection])\n",
    "                            full_edge_index.append([frag_connection, scaffold_node_map[attachment_idx]])\n",
    "                            \n",
    "                            # Use single bond for connection\n",
    "                            full_edge_attr.append([1, 0, 0, 0, 0])\n",
    "                            full_edge_attr.append([1, 0, 0, 0, 0])\n",
    "                        \n",
    "                        # Update position and fragment counter\n",
    "                        current_pos += len(frag_nodes)\n",
    "                        frag_counter += 1\n",
    "            \n",
    "            # Convert to tensors\n",
    "            full_edge_index = torch.tensor(full_edge_index, device=device).t().contiguous()\n",
    "            full_edge_attr = torch.tensor(full_edge_attr, device=device, dtype=torch.float)\n",
    "            full_batch = torch.tensor(full_batch, device=device)\n",
    "            \n",
    "            # Create full molecule graph\n",
    "            full_graph = Data(\n",
    "                x=full_x,\n",
    "                atom_types=full_atom_types,\n",
    "                edge_index=full_edge_index,\n",
    "                edge_attr=full_edge_attr,\n",
    "                batch=full_batch\n",
    "            )\n",
    "            \n",
    "            # Combine features for diffusion\n",
    "            full_graph.x_full = self.combine_node_features(full_graph.x, full_graph.atom_types)\n",
    "            \n",
    "            # Ensure scaffold_features and fragment_features match batch dimension\n",
    "            if scaffold_features.size(0) != fragment_features.size(0):\n",
    "                target_size = max(scaffold_features.size(0), fragment_features.size(0))\n",
    "                \n",
    "                # Resize scaffold_features if needed\n",
    "                if scaffold_features.size(0) != target_size:\n",
    "                    new_scaffold = torch.zeros(target_size, scaffold_features.size(1), device=device)\n",
    "                    new_scaffold[:scaffold_features.size(0)] = scaffold_features\n",
    "                    scaffold_features = new_scaffold\n",
    "                \n",
    "                # Resize fragment_features if needed\n",
    "                if fragment_features.size(0) != target_size:\n",
    "                    new_fragments = torch.zeros(target_size, fragment_features.size(1), device=device)\n",
    "                    new_fragments[:fragment_features.size(0)] = fragment_features\n",
    "                    fragment_features = new_fragments\n",
    "            \n",
    "            # Combine conditional features\n",
    "            combined_cond = scaffold_features + fragment_features\n",
    "            \n",
    "            # Reverse diffusion for the full molecule\n",
    "            for t in tqdm(range(self.num_steps - 1, -1, -1), desc=\"Generating full molecules\"):\n",
    "                # Get timesteps\n",
    "                timesteps = torch.full((batch_size,), t, device=device, dtype=torch.long)\n",
    "                \n",
    "                # Add noise if t > 0\n",
    "                if t > 0:\n",
    "                    noise = torch.randn_like(full_graph.x_full) * 0.5\n",
    "                else:\n",
    "                    noise = 0\n",
    "                \n",
    "                # Expand conditioning to match node dimension\n",
    "                expanded_cond = combined_cond[full_graph.batch]\n",
    "                \n",
    "                # Combine conditioning with node features\n",
    "                combined_x = torch.cat([full_graph.x_full, expanded_cond], dim=1)\n",
    "                \n",
    "                # Predict noise and denoise\n",
    "                with torch.no_grad():\n",
    "                    noise_pred = self.level3_denoiser(\n",
    "                        combined_x, full_graph.edge_index, full_graph.edge_attr,\n",
    "                        timesteps, level3_cond, full_graph.batch\n",
    "                    )\n",
    "                \n",
    "                # Get alpha and beta values\n",
    "                alpha = self.level3_diffusion.alpha[t].to(device)\n",
    "                alpha_bar = self.level3_diffusion.alpha_bar[t].to(device)\n",
    "                beta = self.level3_diffusion.beta[t].to(device)\n",
    "                \n",
    "                # Update x\n",
    "                if t > 0:\n",
    "                    alpha_next = self.level3_diffusion.alpha[t-1].to(device)\n",
    "                    alpha_bar_next = self.level3_diffusion.alpha_bar[t-1].to(device)\n",
    "                    \n",
    "                    # Calculate posterior mean and variance\n",
    "                    posterior_mean = (\n",
    "                        torch.sqrt(alpha_bar_next) * beta / (1 - alpha_bar) * full_graph.x_full +\n",
    "                        torch.sqrt(alpha) * (1 - alpha_bar_next) / (1 - alpha_bar) * \n",
    "                        (full_graph.x_full - noise_pred * torch.sqrt(1 - alpha_bar) / torch.sqrt(alpha_bar))\n",
    "                    )\n",
    "                    \n",
    "                    posterior_var = beta * (1 - alpha_bar_next) / (1 - alpha_bar)\n",
    "                    \n",
    "                    # Update x\n",
    "                    full_graph.x_full = posterior_mean + torch.sqrt(posterior_var) * noise\n",
    "                else:\n",
    "                    # Final prediction\n",
    "                    full_graph.x_full = (full_graph.x_full - noise_pred * torch.sqrt(1 - alpha_bar)) / torch.sqrt(alpha_bar)\n",
    "                \n",
    "                # Apply validity constraints\n",
    "                full_graph.x_full = torch.clamp(full_graph.x_full, -5, 5)\n",
    "                \n",
    "                # Update node features and atom types occasionally\n",
    "                if t % 200 == 0 or t == 0:\n",
    "                    # Extract updated node features\n",
    "                    node_features = full_graph.x_full[:, :self.node_dim]\n",
    "                    full_graph.x = node_features\n",
    "            \n",
    "            generated_molecules = full_graph\n",
    "            \n",
    "            # Extract features for affinity prediction\n",
    "            scaffold_features = self.extract_scaffold_features(generated_scaffold)\n",
    "            fragment_features = self.extract_fragment_features(generated_fragments) if generated_fragments else \\\n",
    "                               torch.zeros(batch_size, self.cond_dim, device=device)\n",
    "            \n",
    "            # Get molecule features\n",
    "            molecule_features = torch_scatter.scatter_mean(\n",
    "                self.molecule_encoder(\n",
    "                    self.combine_node_features(generated_molecules.x, generated_molecules.atom_types),\n",
    "                    generated_molecules.edge_index,\n",
    "                    generated_molecules.edge_attr, \n",
    "                    generated_molecules.batch\n",
    "                ),\n",
    "                generated_molecules.batch, dim=0\n",
    "            )\n",
    "            \n",
    "            # Concatenate all features\n",
    "            combined_features = torch.cat([scaffold_features, fragment_features, molecule_features], dim=1)\n",
    "            \n",
    "            # Predict affinity\n",
    "            predicted_affinity = self.affinity_predictor(combined_features)\n",
    "        else:\n",
    "            # Handle case with no valid scaffold\n",
    "            generated_molecules = None\n",
    "            predicted_affinity = torch.zeros(batch_size, 1, device=device)\n",
    "    \n",
    "        return {\n",
    "            'scaffolds': generated_scaffold,\n",
    "            'fragments': generated_fragments,\n",
    "            'molecules': generated_molecules,\n",
    "            'predicted_affinity': predicted_affinity\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a142a18-fea2-4e16-844c-6c6f831246f2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4142ce4b-fe6b-4e03-8508-ac9c0eea5cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiScaleGraphDiffusionTrainer:\n",
    "    \"\"\"\n",
    "    Trainer for the multi-scale graph diffusion model with gradient accumulation, \n",
    "    mixed precision, and resume training capability\n",
    "    \"\"\"\n",
    "    def __init__(self, model, train_loader, val_loader, lr=3e-4, weight_decay=1e-6,\n",
    "                max_epochs=100, device=None, save_dir='checkpoints',\n",
    "                accumulation_steps=4, use_mixed_precision=True, checkpoint_frequency=5):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.max_epochs = max_epochs\n",
    "        self.device = device if device is not None else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.save_dir = save_dir\n",
    "        self.accumulation_steps = accumulation_steps\n",
    "        self.use_mixed_precision = use_mixed_precision and torch.cuda.is_available()\n",
    "        self.checkpoint_frequency = checkpoint_frequency\n",
    "\n",
    "        # Create save directory\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        # Initialize optimizer\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            model.parameters(), lr=lr, weight_decay=weight_decay\n",
    "        )\n",
    "\n",
    "        # Learning rate scheduler\n",
    "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    "        )\n",
    "\n",
    "        # Mixed precision scaler\n",
    "        if self.use_mixed_precision:\n",
    "            self.scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "        # Training history\n",
    "        self.history = {\n",
    "            'train_loss': [],\n",
    "            'val_loss': [],\n",
    "            'level1_loss': [],\n",
    "            'level2_loss': [],\n",
    "            'level3_loss': [],\n",
    "            'current_epoch': 0,\n",
    "            'best_val_loss': float('inf')\n",
    "        }\n",
    "\n",
    "    def train_epoch(self, mode='all'):\n",
    "        \"\"\"Train for one epoch with gradient accumulation and mixed precision\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        total_level1_loss = 0\n",
    "        total_level2_loss = 0\n",
    "        total_level3_loss = 0\n",
    "        batch_count = 0\n",
    "\n",
    "        # Progress bar\n",
    "        pbar = tqdm(self.train_loader, desc=f\"Training epoch {self.history['current_epoch']+1} ({mode} mode)\")\n",
    "\n",
    "        # Zero gradients at the beginning\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        for i, batch in enumerate(pbar):\n",
    "            # Process batch with deep to device\n",
    "            processed_batch = deep_to_device(batch, self.device)\n",
    "\n",
    "            # Forward pass with mixed precision if enabled\n",
    "            if self.use_mixed_precision:\n",
    "                with torch.amp.autocast(device_type='cuda'):\n",
    "                    results = self.model(processed_batch, mode=mode)\n",
    "                    loss = results['loss'] / self.accumulation_steps\n",
    "\n",
    "                # Backward pass with scaler\n",
    "                self.scaler.scale(loss).backward()\n",
    "\n",
    "                # Update weights every accumulation_steps batches\n",
    "                if (i + 1) % self.accumulation_steps == 0 or (i + 1) == len(self.train_loader):\n",
    "                    self.scaler.step(self.optimizer)\n",
    "                    self.scaler.update()\n",
    "                    self.optimizer.zero_grad()\n",
    "            else:\n",
    "                # Standard forward and backward pass\n",
    "                results = self.model(processed_batch, mode=mode)\n",
    "                loss = results['loss'] / self.accumulation_steps  # Scale loss\n",
    "\n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "\n",
    "                # Update weights every accumulation_steps batches\n",
    "                if (i + 1) % self.accumulation_steps == 0 or (i + 1) == len(self.train_loader):\n",
    "                    self.optimizer.step()\n",
    "                    self.optimizer.zero_grad()\n",
    "\n",
    "            # Update stats with unscaled loss\n",
    "            total_loss += loss.item() * self.accumulation_steps\n",
    "\n",
    "            if 'level1_loss' in results:\n",
    "                total_level1_loss += results['level1_loss'].item()\n",
    "\n",
    "            if 'level2_loss' in results:\n",
    "                total_level2_loss += results['level2_loss'].item()\n",
    "\n",
    "            if 'level3_loss' in results:\n",
    "                total_level3_loss += results['level3_loss'].item()\n",
    "\n",
    "            batch_count += 1\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'loss': loss.item() * self.accumulation_steps,\n",
    "                'avg_loss': total_loss / batch_count\n",
    "            })\n",
    "\n",
    "        # Calculate average losses\n",
    "        avg_loss = total_loss / batch_count\n",
    "        avg_level1_loss = total_level1_loss / batch_count if batch_count > 0 else 0\n",
    "        avg_level2_loss = total_level2_loss / batch_count if batch_count > 0 else 0\n",
    "        avg_level3_loss = total_level3_loss / batch_count if batch_count > 0 else 0\n",
    "\n",
    "        return avg_loss, avg_level1_loss, avg_level2_loss, avg_level3_loss\n",
    "\n",
    "    def validate(self, mode='all'):\n",
    "        \"\"\"Validate the model with mixed precision if enabled\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        total_level1_loss = 0\n",
    "        total_level2_loss = 0\n",
    "        total_level3_loss = 0\n",
    "        batch_count = 0\n",
    "\n",
    "        # Progress bar\n",
    "        pbar = tqdm(self.val_loader, desc=f\"Validating ({mode} mode)\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in pbar:\n",
    "                # Process batch with deep to device\n",
    "                processed_batch = deep_to_device(batch, self.device)\n",
    "\n",
    "                # Forward pass with mixed precision if enabled\n",
    "                if self.use_mixed_precision:\n",
    "                    with torch.amp.autocast(device_type='cuda'):\n",
    "                        results = self.model(processed_batch, mode=mode)\n",
    "                else:\n",
    "                    results = self.model(processed_batch, mode=mode)\n",
    "\n",
    "                # Calculate loss\n",
    "                loss = results['loss']\n",
    "\n",
    "                # Update stats\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                if 'level1_loss' in results:\n",
    "                    total_level1_loss += results['level1_loss'].item()\n",
    "\n",
    "                if 'level2_loss' in results:\n",
    "                    total_level2_loss += results['level2_loss'].item()\n",
    "\n",
    "                if 'level3_loss' in results:\n",
    "                    total_level3_loss += results['level3_loss'].item()\n",
    "\n",
    "                batch_count += 1\n",
    "\n",
    "                # Update progress bar\n",
    "                pbar.set_postfix({\n",
    "                    'val_loss': loss.item(),\n",
    "                    'avg_val_loss': total_loss / batch_count\n",
    "                })\n",
    "\n",
    "        # Calculate average losses\n",
    "        avg_loss = total_loss / batch_count\n",
    "        avg_level1_loss = total_level1_loss / batch_count if batch_count > 0 else 0\n",
    "        avg_level2_loss = total_level2_loss / batch_count if batch_count > 0 else 0\n",
    "        avg_level3_loss = total_level3_loss / batch_count if batch_count > 0 else 0\n",
    "\n",
    "        return avg_loss, avg_level1_loss, avg_level2_loss, avg_level3_loss\n",
    "\n",
    "    def save_checkpoint(self, path, is_best=False):\n",
    "        \"\"\"Save model checkpoint with all training state\"\"\"\n",
    "        checkpoint = {\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "            'history': self.history\n",
    "        }\n",
    "        \n",
    "        if self.use_mixed_precision:\n",
    "            checkpoint['scaler_state_dict'] = self.scaler.state_dict()\n",
    "            \n",
    "        torch.save(checkpoint, path)\n",
    "        print(f\"Checkpoint saved: {path}\")\n",
    "        \n",
    "        if is_best:\n",
    "            # Save a copy as the best model\n",
    "            best_path = os.path.join(os.path.dirname(path), 'best_model.pt')\n",
    "            torch.save(checkpoint, best_path)\n",
    "            print(f\"Best model saved: {best_path}\")\n",
    "\n",
    "    def load_checkpoint(self, path):\n",
    "        \"\"\"Load model checkpoint and training state\"\"\"\n",
    "        print(f\"Loading checkpoint: {path}\")\n",
    "        checkpoint = torch.load(path, map_location=self.device)\n",
    "        \n",
    "        # Load model weights\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "        # Load optimizer state\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        \n",
    "        # Load scheduler state\n",
    "        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        \n",
    "        # Load training history\n",
    "        self.history = checkpoint['history']\n",
    "        \n",
    "        # Load scaler state if available\n",
    "        if self.use_mixed_precision and 'scaler_state_dict' in checkpoint:\n",
    "            self.scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
    "            \n",
    "        print(f\"Resumed from epoch {self.history['current_epoch']}\")\n",
    "        print(f\"Best validation loss: {self.history['best_val_loss']:.6f}\")\n",
    "        \n",
    "        return self.history['current_epoch']\n",
    "\n",
    "    def train(self, mode='hierarchical', resume_path=None):\n",
    "        \"\"\"\n",
    "        Train the model with resume capability\n",
    "        \n",
    "        Args:\n",
    "            mode: Training mode ('all', 'hierarchical', 'level1', 'level2', 'level3')\n",
    "            resume_path: Path to the checkpoint to resume training from (optional)\n",
    "        \"\"\"\n",
    "        start_epoch = 0\n",
    "        best_val_loss = float('inf')\n",
    "        \n",
    "        # Resume training if checkpoint provided\n",
    "        if resume_path and os.path.exists(resume_path):\n",
    "            start_epoch = self.load_checkpoint(resume_path)\n",
    "            best_val_loss = self.history['best_val_loss']\n",
    "        else:\n",
    "            print(\"Starting training from scratch\")\n",
    "            # Reset history\n",
    "            self.history = {\n",
    "                'train_loss': [],\n",
    "                'val_loss': [],\n",
    "                'level1_loss': [],\n",
    "                'level2_loss': [],\n",
    "                'level3_loss': [],\n",
    "                'current_epoch': 0,\n",
    "                'best_val_loss': float('inf')\n",
    "            }\n",
    "\n",
    "        if mode == 'hierarchical' and start_epoch < 20:  # Only run hierarchical stages if starting early\n",
    "            # Hierarchical training (stages) - each stage is 2 epochs\n",
    "            stages = [\n",
    "                ('level1', 10),  # Level 1: 2 epochs \n",
    "                ('level2', 10),  # Level 2: 2 epochs\n",
    "                ('level3', 10),  # Level 3: 2 epochs\n",
    "            ]\n",
    "            \n",
    "            # Determine which stage to start from\n",
    "            stage_start = min(start_epoch // 2, len(stages))\n",
    "            \n",
    "            # Run remaining stages\n",
    "            for stage_idx in range(stage_start, len(stages)):\n",
    "                stage_mode, stage_epochs = stages[stage_idx]\n",
    "                print(f\"\\n===== Stage {stage_idx+1}: Training {stage_mode} =====\")\n",
    "                \n",
    "                # Train for stage_epochs\n",
    "                for _ in range(stage_epochs):\n",
    "                    self.history['current_epoch'] += 1\n",
    "                    current_epoch = self.history['current_epoch']\n",
    "                    \n",
    "                    print(f\"\\nEpoch {current_epoch}/{self.max_epochs}\")\n",
    "                    \n",
    "                    # Train and validate\n",
    "                    train_loss, train_l1, train_l2, train_l3 = self.train_epoch(mode=stage_mode)\n",
    "                    val_loss, val_l1, val_l2, val_l3 = self.validate(mode=stage_mode)\n",
    "                    \n",
    "                    # Update learning rate\n",
    "                    self.scheduler.step(val_loss)\n",
    "                    \n",
    "                    # Save history\n",
    "                    self.history['train_loss'].append(train_loss)\n",
    "                    self.history['val_loss'].append(val_loss)\n",
    "                    self.history['level1_loss'].append(train_l1)\n",
    "                    self.history['level2_loss'].append(train_l2)\n",
    "                    self.history['level3_loss'].append(train_l3)\n",
    "                    \n",
    "                    # Check if this is the best model\n",
    "                    if val_loss < best_val_loss:\n",
    "                        best_val_loss = val_loss\n",
    "                        self.history['best_val_loss'] = best_val_loss\n",
    "                        is_best = True\n",
    "                    else:\n",
    "                        is_best = False\n",
    "                        \n",
    "                    # Save regular checkpoint\n",
    "                    stage_checkpoint_path = f\"{self.save_dir}/checkpoint_stage{stage_idx+1}_epoch{current_epoch}.pt\"\n",
    "                    self.save_checkpoint(stage_checkpoint_path, is_best=is_best)\n",
    "                    \n",
    "                    # Save latest checkpoint\n",
    "                    latest_path = f\"{self.save_dir}/latest_checkpoint.pt\"\n",
    "                    self.save_checkpoint(latest_path)\n",
    "                    \n",
    "                    # Print stats\n",
    "                    print(f\"Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "                    print(f\"Level {stage_idx+1} - Train: {eval(f'train_l{stage_idx+1}'):.6f}, Val: {eval(f'val_l{stage_idx+1}'):.6f}\")\n",
    "                    \n",
    "                    # Plot learning curves\n",
    "                    self.plot_learning_curves()\n",
    "            \n",
    "            # Now proceed with full training\n",
    "            start_epoch = 6  # After 3 stages x 2 epochs each\n",
    "        \n",
    "        # Main training loop\n",
    "        for epoch in range(start_epoch, self.max_epochs):\n",
    "            self.history['current_epoch'] += 1\n",
    "            current_epoch = self.history['current_epoch']\n",
    "            \n",
    "            print(f\"\\nEpoch {current_epoch}/{self.max_epochs}\")\n",
    "            \n",
    "            # Train and validate\n",
    "            train_loss, train_l1, train_l2, train_l3 = self.train_epoch(mode=mode)\n",
    "            val_loss, val_l1, val_l2, val_l3 = self.validate(mode=mode)\n",
    "            \n",
    "            # Update learning rate\n",
    "            self.scheduler.step(val_loss)\n",
    "            \n",
    "            # Save history\n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            self.history['level1_loss'].append(train_l1)\n",
    "            self.history['level2_loss'].append(train_l2)\n",
    "            self.history['level3_loss'].append(train_l3)\n",
    "            \n",
    "            # Check if this is the best model\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                self.history['best_val_loss'] = best_val_loss\n",
    "                is_best = True\n",
    "                print(f\"New best validation loss: {best_val_loss:.6f}\")\n",
    "            else:\n",
    "                is_best = False\n",
    "            \n",
    "            # Save periodic checkpoints\n",
    "            if current_epoch % self.checkpoint_frequency == 0 or current_epoch == self.max_epochs:\n",
    "                checkpoint_path = f\"{self.save_dir}/checkpoint_epoch{current_epoch}.pt\"\n",
    "                self.save_checkpoint(checkpoint_path, is_best=is_best)\n",
    "            \n",
    "            # Always save latest checkpoint for resuming\n",
    "            latest_path = f\"{self.save_dir}/latest_checkpoint.pt\"\n",
    "            self.save_checkpoint(latest_path)\n",
    "            \n",
    "            # Print stats\n",
    "            print(f\"Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "            if mode in ['all', 'hierarchical']:\n",
    "                print(f\"Level 1 - Train: {train_l1:.6f}, Val: {val_l1:.6f}\")\n",
    "                print(f\"Level 2 - Train: {train_l2:.6f}, Val: {val_l2:.6f}\")\n",
    "                print(f\"Level 3 - Train: {train_l3:.6f}, Val: {val_l3:.6f}\")\n",
    "            \n",
    "            # Plot learning curves\n",
    "            self.plot_learning_curves()\n",
    "        \n",
    "        print(f\"\\nTraining completed. Best validation loss: {best_val_loss:.6f}\")\n",
    "        \n",
    "        # Final evaluation on validation set\n",
    "        final_val_loss, final_l1, final_l2, final_l3 = self.validate(mode=mode)\n",
    "        print(f\"Final validation loss: {final_val_loss:.6f}\")\n",
    "        \n",
    "        return best_val_loss\n",
    "\n",
    "    def plot_learning_curves(self):\n",
    "        \"\"\"Plot learning curves with epoch information\"\"\"\n",
    "        # Create figure\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        \n",
    "        # Get current epoch\n",
    "        current_epoch = self.history['current_epoch']\n",
    "        epochs = range(1, current_epoch + 1)\n",
    "        \n",
    "        # Plot overall loss\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(epochs, self.history['train_loss'], label='Train Loss')\n",
    "        plt.plot(epochs, self.history['val_loss'], label='Validation Loss')\n",
    "        plt.title(f'Overall Loss (Epoch {current_epoch})')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Plot level-specific losses\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(epochs, self.history['level1_loss'], label='Level 1 (Scaffold)')\n",
    "        plt.plot(epochs, self.history['level2_loss'], label='Level 2 (Fragment)')\n",
    "        plt.plot(epochs, self.history['level3_loss'], label='Level 3 (Complete)')\n",
    "        plt.title('Level-Specific Losses')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Plot learning rate\n",
    "        if hasattr(self.scheduler, 'param_groups'):\n",
    "            learning_rates = [pg['lr'] for pg in self.optimizer.param_groups]\n",
    "            if learning_rates:\n",
    "                plt.subplot(2, 2, 3)\n",
    "                plt.plot(epochs, learning_rates)\n",
    "                plt.title('Learning Rate')\n",
    "                plt.xlabel('Epoch')\n",
    "                plt.ylabel('Learning Rate')\n",
    "        \n",
    "        # Save figure\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{self.save_dir}/learning_curves_epoch{current_epoch}.png\")\n",
    "        plt.savefig(f\"{self.save_dir}/learning_curves_latest.png\")  # Overwrite for latest\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6c4db8-0db8-490b-a084-87f1fd1a8e3b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97fec93b-e193-46d2-87aa-68f52716d1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading dataset from processed_data/combined_multiscale_dataset.pkl\n",
      "Loaded 19989 protein-ligand pairs\n",
      "Filtered to 19970 pairs with <= 150 atoms\n",
      "Using 15975 samples for train\n",
      "Limited to 500 samples per epoch\n",
      "Loading dataset from processed_data/combined_multiscale_dataset.pkl\n",
      "Loaded 19989 protein-ligand pairs\n",
      "Filtered to 19970 pairs with <= 150 atoms\n",
      "Using 1998 samples for val\n",
      "Limited to 50 samples per epoch\n",
      "Loading dataset from processed_data/combined_multiscale_dataset.pkl\n",
      "Loaded 19989 protein-ligand pairs\n",
      "Filtered to 19970 pairs with <= 150 atoms\n",
      "Using 1997 samples for test\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 4\n",
    "max_atoms = 150\n",
    "lr = 5e-5\n",
    "weight_decay = 1e-6\n",
    "\n",
    "\n",
    "# Paths\n",
    "data_path = 'processed_data/combined_multiscale_dataset.pkl'\n",
    "save_dir = 'checkpoints/multiscale_diffusion_legacy'\n",
    "\n",
    "# Check for CUDA\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader, val_loader, test_loader = create_data_loaders(\n",
    "    data_path, batch_size=batch_size, num_workers=2, max_atoms=max_atoms, max_samples_per_epoch=500\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68640c81-ff24-418f-bc09-f7760dc68b3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/SensingandInnovationLab/boda.s/.dep_genai/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint: checkpoints/multiscale_diffusion_legacy/latest_checkpoint.pt\n",
      "Resumed from epoch 241\n",
      "Best validation loss: 0.945062\n",
      "\n",
      "Training completed. Best validation loss: 0.945062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating (hierarchical mode):   0%|          | 0/13 [00:00<?, ?it/s]/tmp/ipykernel_22915/2945805247.py:521: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  affinity_loss = F.mse_loss(predicted_affinity, actual_affinity)\n",
      "Validating (hierarchical mode):  92%|█████████▏| 12/13 [00:05<00:00,  4.46it/s, val_loss=1.49, avg_val_loss=1.46]/tmp/ipykernel_22915/2945805247.py:521: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  affinity_loss = F.mse_loss(predicted_affinity, actual_affinity)\n",
      "Validating (hierarchical mode): 100%|██████████| 13/13 [00:06<00:00,  2.10it/s, val_loss=2.22, avg_val_loss=1.52]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final validation loss: 1.516138\n",
      "Loading checkpoint: checkpoints/multiscale_diffusion_legacy/best_model.pt\n",
      "Resumed from epoch 20\n",
      "Best validation loss: 0.945062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating (all mode): 100%|██████████| 13/13 [00:04<00:00,  2.77it/s, val_loss=5.07, avg_val_loss=8.03]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 8.029586\n",
      "Test Level 1 Loss: 0.973925\n",
      "Test Level 2 Loss: 0.952152\n",
      "Test Level 3 Loss: 1.155999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 241\n",
    "\n",
    "# Create model\n",
    "model = MultiScaleGraphDiffusion(\n",
    "    node_dim=8,\n",
    "    edge_dim=5,\n",
    "    protein_dim=1280,\n",
    "    hidden_dim=256,\n",
    "    cond_dim=128,\n",
    "    time_emb_dim=64,\n",
    "    num_steps=1000,\n",
    "    beta_schedule='cosine',\n",
    "    num_layers=3,\n",
    "    heads=4,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "\n",
    "# Create trainer\n",
    "trainer = MultiScaleGraphDiffusionTrainer(\n",
    "    model, train_loader, val_loader,\n",
    "    lr=lr, weight_decay=weight_decay,\n",
    "    max_epochs=max_epochs,  # Now properly set to continue training\n",
    "    device=device,\n",
    "    save_dir=save_dir,\n",
    "    accumulation_steps=4,\n",
    "    use_mixed_precision=True,\n",
    "    checkpoint_frequency=1\n",
    ")\n",
    "\n",
    "# Or resume training from a checkpoint\n",
    "trainer.train(mode='hierarchical', resume_path=f\"{save_dir}/latest_checkpoint.pt\")\n",
    "\n",
    "# Train model\n",
    "# trainer.train(mode='hierarchical')\n",
    "\n",
    "# Load best model\n",
    "trainer.load_checkpoint(f\"{save_dir}/best_model.pt\")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_l1, test_l2, test_l3 = trainer.validate(mode='all')\n",
    "print(f\"Test Loss: {test_loss:.6f}\")\n",
    "print(f\"Test Level 1 Loss: {test_l1:.6f}\")\n",
    "print(f\"Test Level 2 Loss: {test_l2:.6f}\")\n",
    "print(f\"Test Level 3 Loss: {test_l3:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddaddf6-a06d-4582-b23e-e2a85e89f85c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d0e4c6f-eb30-465a-8ae6-458271499975",
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_convert_graph_to_mol(graph_data, batch_idx=0):\n",
    "    \"\"\"\n",
    "    Convert a graph representation to an RDKit molecule with extremely robust error handling\n",
    "    to address the calcImplicitValence issue.\n",
    "    \"\"\"\n",
    "    # Extract the molecule of interest from the batch\n",
    "    node_mask = graph_data.batch == batch_idx\n",
    "\n",
    "    # Get node features and atom types for this molecule\n",
    "    node_features = graph_data.x[node_mask]\n",
    "    atom_types = graph_data.atom_types[node_mask]\n",
    "\n",
    "    # Extract edges for this molecule\n",
    "    edge_mask = []\n",
    "    for i in range(graph_data.edge_index.size(1)):\n",
    "        src, dst = graph_data.edge_index[:, i]\n",
    "        if node_mask[src] and node_mask[dst]:\n",
    "            edge_mask.append(i)\n",
    "\n",
    "    edge_mask = torch.tensor(edge_mask, device=graph_data.edge_index.device)\n",
    "\n",
    "    # Get edges and remap indices\n",
    "    if len(edge_mask) > 0:\n",
    "        edges = graph_data.edge_index[:, edge_mask]\n",
    "        edge_features = graph_data.edge_attr[edge_mask]\n",
    "\n",
    "        # Remap node indices to start from 0\n",
    "        node_indices = torch.where(node_mask)[0]\n",
    "        node_map = {old_idx.item(): new_idx for new_idx, old_idx in enumerate(node_indices)}\n",
    "\n",
    "        edges_remapped = torch.tensor([\n",
    "            [node_map[src.item()], node_map[dst.item()]]\n",
    "            for src, dst in edges.t()\n",
    "        ], device=edges.device).t()\n",
    "    else:\n",
    "        edges_remapped = torch.zeros((2, 0), dtype=torch.long, device=node_features.device)\n",
    "        edge_features = torch.zeros((0, graph_data.edge_attr.size(1)), device=node_features.device)\n",
    "\n",
    "    # Create new RDKit molecule\n",
    "    mol = Chem.RWMol()\n",
    "\n",
    "    # Add atoms, restricting to common elements\n",
    "    for i, atomic_num in enumerate(atom_types):\n",
    "        atom_num = atomic_num.item()\n",
    "        \n",
    "        # Restrict to common elements in drug-like molecules\n",
    "        common_elements = {1, 6, 7, 8, 9, 15, 16, 17, 35, 53}\n",
    "        if atom_num not in common_elements:\n",
    "            atom_num = 6  # Default to carbon\n",
    "            \n",
    "        # Create atom\n",
    "        atom = Chem.Atom(int(atom_num))\n",
    "        \n",
    "        # Set minimal properties\n",
    "        atom.SetNoImplicit(True)  # Don't add implicit hydrogens automatically\n",
    "        atom.SetNumExplicitHs(0)  # Start with no explicit hydrogens\n",
    "        \n",
    "        # Add to molecule\n",
    "        mol.AddAtom(atom)\n",
    "\n",
    "    # Track which bonds we've added\n",
    "    added_bonds = set()\n",
    "    \n",
    "    # Add bonds with conservative valence checking\n",
    "    max_valence = {1: 1, 6: 4, 7: 3, 8: 2, 9: 1, 15: 5, 16: 6, 17: 1, 35: 1, 53: 1}\n",
    "    atom_valences = [0] * mol.GetNumAtoms()\n",
    "    \n",
    "    # First pass: add only single bonds to create a conservative molecule\n",
    "    for i in range(edges_remapped.size(1)):\n",
    "        src, dst = edges_remapped[:, i]\n",
    "        src_idx, dst_idx = src.item(), dst.item()\n",
    "        \n",
    "        # Skip if we've already added this bond\n",
    "        if (src_idx, dst_idx) in added_bonds or (dst_idx, src_idx) in added_bonds:\n",
    "            continue\n",
    "            \n",
    "        # Get atoms\n",
    "        src_atom = mol.GetAtomWithIdx(src_idx)\n",
    "        dst_atom = mol.GetAtomWithIdx(dst_idx)\n",
    "        src_elem = src_atom.GetAtomicNum()\n",
    "        dst_elem = dst_atom.GetAtomicNum()\n",
    "        \n",
    "        # Check if adding a bond would exceed conservative valence\n",
    "        if (atom_valences[src_idx] >= max_valence.get(src_elem, 4) or \n",
    "            atom_valences[dst_idx] >= max_valence.get(dst_elem, 4)):\n",
    "            continue\n",
    "            \n",
    "        # Add single bond\n",
    "        mol.AddBond(src_idx, dst_idx, Chem.BondType.SINGLE)\n",
    "        added_bonds.add((src_idx, dst_idx))\n",
    "        \n",
    "        # Update valence tracking\n",
    "        atom_valences[src_idx] += 1\n",
    "        atom_valences[dst_idx] += 1\n",
    "        \n",
    "    # Try to create a valid molecule\n",
    "    try:\n",
    "        # Finalize the molecule\n",
    "        mol = mol.GetMol()\n",
    "        \n",
    "        # Force calculation of implicit valence for all atoms\n",
    "        for atom in mol.GetAtoms():\n",
    "            atom.UpdatePropertyCache(strict=False)\n",
    "            \n",
    "        # Kekulize the molecule without sanitization\n",
    "        Chem.KekulizeIfPossible(mol, clearAromaticFlags=True)\n",
    "        \n",
    "        # Try to sanitize\n",
    "        Chem.SanitizeMol(mol, sanitizeOps=Chem.SanitizeFlags.SANITIZE_ALL^Chem.SanitizeFlags.SANITIZE_KEKULIZE)\n",
    "        \n",
    "        # Calculate 2D coordinates\n",
    "        AllChem.Compute2DCoords(mol)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"First attempt failed: {str(e)}\")\n",
    "        # If that fails, try a more aggressive approach - build a minimal valid molecule\n",
    "        try:\n",
    "            # Create a new molecule with just atoms\n",
    "            new_mol = Chem.RWMol()\n",
    "            \n",
    "            # Add atoms\n",
    "            for i in range(mol.GetNumAtoms()):\n",
    "                atom = mol.GetAtomWithIdx(i)\n",
    "                new_atom = Chem.Atom(atom.GetAtomicNum())\n",
    "                new_mol.AddAtom(new_atom)\n",
    "            \n",
    "            # Add a minimal spanning set of bonds to keep it connected\n",
    "            for i, j in added_bonds:\n",
    "                if i < new_mol.GetNumAtoms() and j < new_mol.GetNumAtoms():\n",
    "                    try:\n",
    "                        new_mol.AddBond(i, j, Chem.BondType.SINGLE)\n",
    "                    except:\n",
    "                        pass\n",
    "            \n",
    "            # Try to finalize and sanitize\n",
    "            new_mol = new_mol.GetMol()\n",
    "            for atom in new_mol.GetAtoms():\n",
    "                atom.UpdatePropertyCache(strict=False)\n",
    "            \n",
    "            # Basic sanitization\n",
    "            Chem.SanitizeMol(new_mol, sanitizeOps=Chem.SanitizeFlags.SANITIZE_FINDRADICALS | \n",
    "                           Chem.SanitizeFlags.SANITIZE_SETAROMATICITY | \n",
    "                           Chem.SanitizeFlags.SANITIZE_SETCONJUGATION |\n",
    "                           Chem.SanitizeFlags.SANITIZE_SETHYBRIDIZATION)\n",
    "            \n",
    "            # Add coordinates\n",
    "            AllChem.Compute2DCoords(new_mol)\n",
    "            \n",
    "            return new_mol\n",
    "        except Exception as e2:\n",
    "            print(f\"Second attempt also failed: {str(e2)}\")\n",
    "            \n",
    "            # Last resort: Generate a simple valid molecule\n",
    "            try:\n",
    "                # Create a minimalist molecule (e.g., methane)\n",
    "                fallback_mol = Chem.MolFromSmiles(\"C\")\n",
    "                print(\"Using fallback molecule (methane)\")\n",
    "                return fallback_mol\n",
    "            except:\n",
    "                return None\n",
    "    \n",
    "    return mol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe708134-ba88-4dda-8691-39f82f23699c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors, Lipinski, QED, AllChem, rdMolDescriptors, Draw\n",
    "from rdkit.Chem.Draw import rdMolDraw2D\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from io import BytesIO\n",
    "\n",
    "# Function to calculate properties for a molecule\n",
    "def calculate_molecule_properties(mol):\n",
    "    \"\"\"Calculate key properties for synthesizability and drug-likeness assessment\"\"\"\n",
    "    if mol is None:\n",
    "        return {}\n",
    "    \n",
    "    properties = {}\n",
    "    \n",
    "    try:\n",
    "        # Basic molecular properties\n",
    "        properties['MW'] = round(Descriptors.MolWt(mol), 1)\n",
    "        properties['LogP'] = round(Descriptors.MolLogP(mol), 1)\n",
    "        properties['TPSA'] = round(Descriptors.TPSA(mol), 1)\n",
    "        properties['HBD'] = Descriptors.NumHDonors(mol)\n",
    "        properties['HBA'] = Descriptors.NumHAcceptors(mol)\n",
    "        properties['RotBonds'] = Descriptors.NumRotatableBonds(mol)\n",
    "        \n",
    "        # Drug-likeness\n",
    "        properties['QED'] = round(QED.qed(mol), 2)\n",
    "        \n",
    "        # Lipinski's Rule of Five\n",
    "        violations = 0\n",
    "        if properties['MW'] > 500: violations += 1\n",
    "        if properties['LogP'] > 5: violations += 1\n",
    "        if properties['HBD'] > 5: violations += 1\n",
    "        if properties['HBA'] > 10: violations += 1\n",
    "        properties['Lipinski'] = f\"{4-violations}/4\"\n",
    "        \n",
    "        # Complexity and stability indicators\n",
    "        properties['Rings'] = rdMolDescriptors.CalcNumRings(mol)  # Fixed: Use proper ring count function\n",
    "        properties['Stereocenters'] = len(Chem.FindMolChiralCenters(mol, includeUnassigned=True))\n",
    "        \n",
    "        # Synthetic accessibility approximation\n",
    "        # Use a heuristic approach based on complexity factors\n",
    "        complexity_score = (\n",
    "            properties['Rings'] * 0.5 + \n",
    "            properties['Stereocenters'] * 0.5 + \n",
    "            properties['RotBonds'] * 0.1 + \n",
    "            (properties['MW'] / 100) * 0.2\n",
    "        )\n",
    "        properties['SA_Score'] = round(min(1 + complexity_score, 10), 1)  # Scale 1-10\n",
    "        \n",
    "        # Stability estimation based on reactive groups\n",
    "        # Check for common reactive functional groups\n",
    "        unstable_groups = 0\n",
    "        \n",
    "        # Common reactive/unstable patterns\n",
    "        reactive_patterns = [\n",
    "            '[OX2H]C=O',  # carboxylic acid\n",
    "            '[CX3](=[OX1])[OX2H0][CX4]',  # ester\n",
    "            '[NX3][CX3](=[OX1])[CX4]',  # amide\n",
    "            'C=C',  # alkene\n",
    "            '[NX3+](=[OX1])[OX1-]',  # nitro\n",
    "            '[CX3]=[OX1]',  # aldehyde/ketone\n",
    "            '[SX2]',  # thiol\n",
    "            '[NX3;H2]',  # primary amine\n",
    "            '[OX2H][CX4;!$(C([OX2H])[O,N,S])]',  # alcohol (not part of acid)\n",
    "        ]\n",
    "        \n",
    "        for pattern in reactive_patterns:\n",
    "            patt = Chem.MolFromSmarts(pattern)\n",
    "            if patt and mol.HasSubstructMatch(patt):\n",
    "                unstable_groups += 1\n",
    "                \n",
    "        properties['Reactive_Groups'] = unstable_groups\n",
    "        stability_score = max(10 - unstable_groups - properties['Rings']*0.2 - properties['Stereocenters']*0.2, 1)\n",
    "        properties['Stability'] = round(stability_score, 1)  # Scale 1-10 (higher is more stable)\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating properties: {str(e)}\")\n",
    "        return {}\n",
    "    \n",
    "    return properties\n",
    "\n",
    "# Function to create enhanced molecule visualization with large legend\n",
    "def create_molecule_image_with_legend(mol, props, filename, affinity, molecule_index=1):\n",
    "    \"\"\"Create an enhanced molecule image with larger legend text\"\"\"\n",
    "    if mol is None:\n",
    "        print(\"Cannot create image: molecule is None\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Create molecule image with highlighted atoms but no legend\n",
    "        drawer = rdMolDraw2D.MolDraw2DCairo(450, 350)\n",
    "        \n",
    "        # Set atom highlighting\n",
    "        highlight_atoms = []\n",
    "        highlight_colors = {}\n",
    "        \n",
    "        # Find stereocenters (blue)\n",
    "        chiral_centers = Chem.FindMolChiralCenters(mol, includeUnassigned=True)\n",
    "        for atom_idx, _ in chiral_centers:\n",
    "            highlight_atoms.append(atom_idx)\n",
    "            highlight_colors[atom_idx] = (0.2, 0.6, 1.0)  # Light blue\n",
    "            \n",
    "        # Find potentially reactive groups (orange)\n",
    "        for pattern in [\n",
    "            '[OX2H]C=O',  # carboxylic acid\n",
    "            '[CX3](=[OX1])[OX2H0][CX4]',  # ester\n",
    "            '[NX3+](=[OX1])[OX1-]',  # nitro\n",
    "            '[CX3]=[OX1]',  # carbonyl\n",
    "        ]:\n",
    "            patt = Chem.MolFromSmarts(pattern)\n",
    "            if patt:\n",
    "                matches = mol.GetSubstructMatches(patt)\n",
    "                for match in matches:\n",
    "                    for atom_idx in match:\n",
    "                        if atom_idx not in highlight_atoms:\n",
    "                            highlight_atoms.append(atom_idx)\n",
    "                            highlight_colors[atom_idx] = (1.0, 0.7, 0.2)  # Orange\n",
    "        \n",
    "        # Draw molecule with highlights\n",
    "        drawer.DrawMolecule(mol, highlightAtoms=highlight_atoms, highlightAtomColors=highlight_colors)\n",
    "        drawer.FinishDrawing()\n",
    "        mol_img = drawer.GetDrawingText()\n",
    "        \n",
    "        # Create image from PNG data\n",
    "        mol_pil = Image.open(BytesIO(mol_img))\n",
    "        \n",
    "        # Create a larger image to accommodate molecule and legend with larger text\n",
    "        img_width = 600\n",
    "        img_height = 550\n",
    "        combined = Image.new('RGB', (img_width, img_height), (255, 255, 255))\n",
    "        combined.paste(mol_pil, (75, 0))\n",
    "        \n",
    "        # Add larger text legend\n",
    "        draw = ImageDraw.Draw(combined)\n",
    "        try:\n",
    "            # Try to use a nicer font if available\n",
    "            font = ImageFont.truetype(\"Arial\", 16)\n",
    "            small_font = ImageFont.truetype(\"Arial\", 14)\n",
    "        except:\n",
    "            # Fallback to default\n",
    "            font = ImageFont.load_default()\n",
    "            small_font = font\n",
    "        \n",
    "        # Affinity value and basic properties (larger text)\n",
    "        pIC50 = affinity\n",
    "        draw.text((20, 370), f\"Molecule {molecule_index} (pIC50: {pIC50:.2f})\", fill=(0, 0, 0), font=font)\n",
    "        \n",
    "        # Physical properties\n",
    "        draw.text((20, 400), f\"MW: {props.get('MW', 'N/A')} | LogP: {props.get('LogP', 'N/A')} | QED: {props.get('QED', 'N/A')}\", \n",
    "                 fill=(0, 0, 0), font=font)\n",
    "        \n",
    "        # Drug-likeness\n",
    "        draw.text((20, 430), f\"TPSA: {props.get('TPSA', 'N/A')} | Lipinski: {props.get('Lipinski', 'N/A')}\", \n",
    "                 fill=(0, 0, 0), font=font)\n",
    "        \n",
    "        # Synthesizability and stability\n",
    "        draw.text((20, 460), f\"Synthesizability: {props.get('SA_Score', 'N/A')}/10 | Stability: {props.get('Stability', 'N/A')}/10\", \n",
    "                 fill=(0, 0, 0), font=font)\n",
    "        \n",
    "        # Structural features\n",
    "        draw.text((20, 490), f\"Rings: {props.get('Rings', 'N/A')} | Stereocenters: {props.get('Stereocenters', 'N/A')} | \" + \n",
    "                  f\"Reactive groups: {props.get('Reactive_Groups', 'N/A')}\", fill=(0, 0, 0), font=font)\n",
    "        \n",
    "        # Color legend\n",
    "        draw.rectangle([(20, 520), (40, 535)], fill=(51, 153, 255))  # Blue box\n",
    "        draw.text((45, 520), \"Stereocenters\", fill=(0, 0, 0), font=small_font)\n",
    "        \n",
    "        draw.rectangle([(250, 520), (270, 535)], fill=(255, 179, 51))  # Orange box\n",
    "        draw.text((275, 520), \"Reactive groups\", fill=(0, 0, 0), font=small_font)\n",
    "        \n",
    "        # Save the combined image\n",
    "        combined.save(filename)\n",
    "        print(f\"Saved enhanced molecule image with larger legend to {filename}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating molecule image: {str(e)}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e466e845-71fb-4d01-bbf8-26433a719ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating molecules for protein 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating scaffolds: 100%|██████████| 1000/1000 [00:07<00:00, 126.57it/s]\n",
      "Generating fragment 1/2 for molecule 1/1: 100%|██████████| 1000/1000 [00:07<00:00, 130.47it/s]\n",
      "Generating fragment 2/2 for molecule 1/1: 100%|██████████| 1000/1000 [00:10<00:00, 99.89it/s]\n",
      "Generating full molecules: 100%|██████████| 1000/1000 [00:07<00:00, 128.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted binding affinity: 0.4242\n",
      "Successfully created molecule 1 with 21 atoms\n",
      "  MW: 274.2 | LogP: -0.5 | QED: 0.64 | SA Score: 3.2\n",
      "  Rings: 3 | Stereocenters: 0 | Stability: 9.4\n",
      "Saved enhanced molecule image with larger legend to checkpoints/multiscale_diffusion_legacy/generated_protein_1_mol_1.png\n",
      "\n",
      "Generating molecules for protein 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating scaffolds: 100%|██████████| 1000/1000 [00:07<00:00, 126.51it/s]\n",
      "Generating fragment 1/3 for molecule 1/1: 100%|██████████| 1000/1000 [00:07<00:00, 129.24it/s]\n",
      "Generating fragment 2/3 for molecule 1/1: 100%|██████████| 1000/1000 [00:07<00:00, 128.31it/s]\n",
      "Generating fragment 3/3 for molecule 1/1: 100%|██████████| 1000/1000 [00:07<00:00, 133.51it/s]\n",
      "Generating full molecules: 100%|██████████| 1000/1000 [00:09<00:00, 102.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted binding affinity: 0.7002\n",
      "Successfully created molecule 1 with 27 atoms\n",
      "  MW: 378.2 | LogP: -0.4 | QED: 0.48 | SA Score: 3.5\n",
      "  Rings: 3 | Stereocenters: 0 | Stability: 9.4\n",
      "Saved enhanced molecule image with larger legend to checkpoints/multiscale_diffusion_legacy/generated_protein_2_mol_1.png\n",
      "\n",
      "Generating molecules for protein 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating scaffolds: 100%|██████████| 1000/1000 [00:10<00:00, 98.45it/s]\n",
      "Generating fragment 1/1 for molecule 1/1: 100%|██████████| 1000/1000 [00:08<00:00, 122.52it/s]\n",
      "Generating full molecules: 100%|██████████| 1000/1000 [00:07<00:00, 131.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted binding affinity: 0.5593\n",
      "Successfully created molecule 1 with 12 atoms\n",
      "  MW: 154.1 | LogP: 0.1 | QED: 0.5 | SA Score: 2.3\n",
      "  Rings: 2 | Stereocenters: 0 | Stability: 9.6\n",
      "Saved enhanced molecule image with larger legend to checkpoints/multiscale_diffusion_legacy/generated_protein_3_mol_1.png\n",
      "\n",
      "Generating molecules for protein 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating scaffolds: 100%|██████████| 1000/1000 [00:07<00:00, 127.86it/s]\n",
      "Generating fragment 1/2 for molecule 1/1: 100%|██████████| 1000/1000 [00:07<00:00, 129.92it/s]\n",
      "Generating fragment 2/2 for molecule 1/1: 100%|██████████| 1000/1000 [00:07<00:00, 132.42it/s]\n",
      "Generating full molecules: 100%|██████████| 1000/1000 [00:07<00:00, 132.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted binding affinity: 0.5896\n",
      "Successfully created molecule 1 with 18 atoms\n",
      "  MW: 226.2 | LogP: -1.0 | QED: 0.43 | SA Score: 3.1\n",
      "  Rings: 2 | Stereocenters: 0 | Stability: 9.6\n",
      "Saved enhanced molecule image with larger legend to checkpoints/multiscale_diffusion_legacy/generated_protein_4_mol_1.png\n"
     ]
    }
   ],
   "source": [
    "# Generate samples for visualization\n",
    "sample_data = next(iter(test_loader))\n",
    "sample_data = {\n",
    "    k: (v.to(device) if isinstance(v, torch.Tensor) \n",
    "       else [g.to(device) if hasattr(g, 'to') else g for g in v] if isinstance(v, list) \n",
    "       else v)\n",
    "    for k, v in sample_data.items()\n",
    "}\n",
    "\n",
    "# Generate molecules for all proteins in the batch\n",
    "for i in range(min(10, len(sample_data['protein_embedding']))):\n",
    "    protein_emb = sample_data['protein_embedding'][i:i+1]\n",
    "    \n",
    "    print(f\"\\nGenerating molecules for protein {i+1}...\")\n",
    "    \n",
    "    try:\n",
    "        generated = model.sample(protein_emb, num_samples=1, device=device)\n",
    "        \n",
    "        # Display predicted affinity\n",
    "        predicted_affinity = generated['predicted_affinity']\n",
    "        print(f\"Predicted binding affinity: {predicted_affinity.item():.4f}\")\n",
    "        \n",
    "        # Visualize the generated molecules if available\n",
    "        if generated['molecules'] is not None and generated['molecules'].x.size(0) > 0:\n",
    "            # Determine how many graphs are in the batch\n",
    "            if hasattr(generated['molecules'], 'batch') and generated['molecules'].batch is not None:\n",
    "                num_graphs = generated['molecules'].batch.max().item() + 1\n",
    "            else:\n",
    "                num_graphs = 1\n",
    "                \n",
    "            # Convert to RDKit molecules using the successful robust conversion\n",
    "            mols = []\n",
    "            properties_list = []  # Store calculated properties\n",
    "            \n",
    "            for j in range(min(5, num_graphs)):\n",
    "                try:\n",
    "                    mol = robust_convert_graph_to_mol(generated['molecules'], batch_idx=j)\n",
    "                    if mol is not None and mol.GetNumAtoms() > 0:\n",
    "                        # Calculate properties\n",
    "                        props = calculate_molecule_properties(mol)\n",
    "                        \n",
    "                        mols.append(mol)\n",
    "                        properties_list.append(props)\n",
    "                        print(f\"Successfully created molecule {j+1} with {mol.GetNumAtoms()} atoms\")\n",
    "                        \n",
    "                        # Print key properties\n",
    "                        print(f\"  MW: {props.get('MW', 'N/A')} | LogP: {props.get('LogP', 'N/A')} | \" +\n",
    "                              f\"QED: {props.get('QED', 'N/A')} | SA Score: {props.get('SA_Score', 'N/A')}\")\n",
    "                        print(f\"  Rings: {props.get('Rings', 'N/A')} | Stereocenters: {props.get('Stereocenters', 'N/A')} | \" +\n",
    "                              f\"Stability: {props.get('Stability', 'N/A')}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to convert molecule {j}: {str(e)}\")\n",
    "            \n",
    "            # Draw molecules if any were successfully created\n",
    "            if mols:\n",
    "                try:\n",
    "                    # First try to create a grid of all molecules (useful for comparison)\n",
    "                    # Create enhanced legends with properties for grid image\n",
    "                    legends = []\n",
    "                    for j, (mol, props) in enumerate(zip(mols, properties_list)):\n",
    "                        # Format affinity score\n",
    "                        pIC50 = predicted_affinity.item()\n",
    "                        legend = (\n",
    "                            f\"Molecule {j+1} (pIC50: {pIC50:.2f})\\n\"\n",
    "                            f\"MW: {props.get('MW', 'N/A')} | LogP: {props.get('LogP', 'N/A')} | QED: {props.get('QED', 'N/A')}\\n\"\n",
    "                            f\"Synthesizability: {props.get('SA_Score', 'N/A')}/10 | Stability: {props.get('Stability', 'N/A')}/10\"\n",
    "                        )\n",
    "                        legends.append(legend)\n",
    "                    \n",
    "                    # Create grid image (simpler format for overview)\n",
    "                    grid_img = Draw.MolsToGridImage(\n",
    "                        mols, molsPerRow=min(3, len(mols)), subImgSize=(300, 300),\n",
    "                        legends=legends\n",
    "                    )\n",
    "                    \n",
    "                    grid_filename = f\"{save_dir}/generated_protein_{i+1}_grid.png\"\n",
    "                    if hasattr(grid_img, 'save'):\n",
    "                        grid_img.save(grid_filename)\n",
    "                        print(f\"Saved molecule grid to {grid_filename}\")\n",
    "                    \n",
    "                    # Now create detailed individual molecule images with enhanced legends\n",
    "                    for j, (mol, props) in enumerate(zip(mols, properties_list)):\n",
    "                        filename = f\"{save_dir}/generated_protein_{i+1}_mol_{j+1}.png\"\n",
    "                        create_molecule_image_with_legend(\n",
    "                            mol, props, filename, \n",
    "                            predicted_affinity.item(), j+1\n",
    "                        )\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to create/save grid image: {e}\")\n",
    "                    # Simple fallback\n",
    "                    for j, mol in enumerate(mols):\n",
    "                        try:\n",
    "                            filename = f\"{save_dir}/generated_protein_{i+1}_mol_{j+1}_simple.png\"\n",
    "                            Draw.MolToFile(mol, filename, size=(250, 250))\n",
    "                            print(f\"Saved simple molecule image to {filename}\")\n",
    "                        except Exception as e2:\n",
    "                            print(f\"Also failed to save molecule {j+1}: {str(e2)}\")\n",
    "            else:\n",
    "                print(f\"No valid molecules generated for protein {i+1}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating molecules for protein {i+1}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7350980-ea8f-4506-9c6b-39f4f5156eb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
